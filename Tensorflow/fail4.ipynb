{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eff2238b-2b35-48bf-8ccc-478e71a9da74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 22:56:58.912105: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-07 22:56:58.986696: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741363019.034805  134946 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741363019.051208  134946 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-07 22:56:59.139073: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/nhat/.local/lib/python3.10/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1741363025.615600  134946 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 54ms/step - accuracy: 0.1089 - loss: 7.8985 - val_accuracy: 0.1107 - val_loss: 5.4268\n",
      "Epoch 2/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 69ms/step - accuracy: 0.1096 - loss: 4.7187 - val_accuracy: 0.1107 - val_loss: 2.8735\n",
      "Epoch 3/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 63ms/step - accuracy: 0.1103 - loss: 2.6222 - val_accuracy: 0.1107 - val_loss: 2.3119\n",
      "Epoch 4/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 54ms/step - accuracy: 0.1086 - loss: 2.3075 - val_accuracy: 0.1047 - val_loss: 2.3025\n",
      "Epoch 5/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 50ms/step - accuracy: 0.1041 - loss: 2.3025 - val_accuracy: 0.1047 - val_loss: 2.3023\n",
      "Epoch 6/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 50ms/step - accuracy: 0.1026 - loss: 2.3026 - val_accuracy: 0.1047 - val_loss: 2.3023\n",
      "Epoch 7/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 49ms/step - accuracy: 0.1061 - loss: 2.3026 - val_accuracy: 0.1047 - val_loss: 2.3024\n",
      "Epoch 8/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 52ms/step - accuracy: 0.1036 - loss: 2.3026 - val_accuracy: 0.1047 - val_loss: 2.3024\n",
      "Epoch 9/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 48ms/step - accuracy: 0.1031 - loss: 2.3025 - val_accuracy: 0.1047 - val_loss: 2.3023\n",
      "Epoch 10/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 49ms/step - accuracy: 0.1041 - loss: 2.3025 - val_accuracy: 0.1047 - val_loss: 2.3023\n",
      "Epoch 1/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 55ms/step - accuracy: 0.1037 - loss: 2.3025 - val_accuracy: 0.1090 - val_loss: 2.3022\n",
      "Epoch 2/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 56ms/step - accuracy: 0.1152 - loss: 2.3020 - val_accuracy: 0.1140 - val_loss: 2.3017\n",
      "Epoch 3/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 57ms/step - accuracy: 0.1165 - loss: 2.3018 - val_accuracy: 0.1142 - val_loss: 2.3012\n",
      "Epoch 4/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 57ms/step - accuracy: 0.1248 - loss: 2.3005 - val_accuracy: 0.1112 - val_loss: 2.3006\n",
      "Epoch 5/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 56ms/step - accuracy: 0.1224 - loss: 2.2998 - val_accuracy: 0.1145 - val_loss: 2.2996\n",
      "Epoch 6/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 57ms/step - accuracy: 0.1243 - loss: 2.2991 - val_accuracy: 0.1187 - val_loss: 2.2983\n",
      "Epoch 7/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 56ms/step - accuracy: 0.1269 - loss: 2.2979 - val_accuracy: 0.1215 - val_loss: 2.2966\n",
      "Epoch 8/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 57ms/step - accuracy: 0.1285 - loss: 2.2963 - val_accuracy: 0.1225 - val_loss: 2.2940\n",
      "Epoch 9/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 57ms/step - accuracy: 0.1342 - loss: 2.2933 - val_accuracy: 0.1320 - val_loss: 2.2904\n",
      "Epoch 10/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - accuracy: 0.1401 - loss: 2.2892 - val_accuracy: 0.1357 - val_loss: 2.2863\n",
      "Epoch 11/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 57ms/step - accuracy: 0.1432 - loss: 2.2854 - val_accuracy: 0.1355 - val_loss: 2.2815\n",
      "Epoch 12/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 54ms/step - accuracy: 0.1423 - loss: 2.2797 - val_accuracy: 0.1358 - val_loss: 2.2774\n",
      "Epoch 13/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 53ms/step - accuracy: 0.1436 - loss: 2.2762 - val_accuracy: 0.1373 - val_loss: 2.2732\n",
      "Epoch 14/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 54ms/step - accuracy: 0.1451 - loss: 2.2748 - val_accuracy: 0.1405 - val_loss: 2.2705\n",
      "Epoch 15/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 55ms/step - accuracy: 0.1479 - loss: 2.2699 - val_accuracy: 0.1438 - val_loss: 2.2669\n",
      "Epoch 16/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 53ms/step - accuracy: 0.1491 - loss: 2.2690 - val_accuracy: 0.1433 - val_loss: 2.2636\n",
      "Epoch 17/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 58ms/step - accuracy: 0.1481 - loss: 2.2665 - val_accuracy: 0.1482 - val_loss: 2.2617\n",
      "Epoch 18/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - accuracy: 0.1504 - loss: 2.2646 - val_accuracy: 0.1472 - val_loss: 2.2587\n",
      "Epoch 19/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 53ms/step - accuracy: 0.1534 - loss: 2.2638 - val_accuracy: 0.1480 - val_loss: 2.2568\n",
      "Epoch 20/20\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 55ms/step - accuracy: 0.1520 - loss: 2.2586 - val_accuracy: 0.1512 - val_loss: 2.2552\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.1614 - loss: 2.2525\n",
      "Test accuracy: 0.1593\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, datasets\n",
    "import numpy as np\n",
    "\n",
    "# Custom layers implementing ABCNet paper components\n",
    "class BinaryActivation(layers.Layer):\n",
    "    def __init__(self, num_binary=5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_binary = num_binary\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.shift = self.add_weight(name='shift', \n",
    "                                   shape=(self.num_binary,),\n",
    "                                   initializer='random_normal',\n",
    "                                   trainable=True)\n",
    "        self.beta = self.add_weight(name='beta',\n",
    "                                  shape=(self.num_binary,),\n",
    "                                  initializer='ones',\n",
    "                                  trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Input binarization with learnable shifts\n",
    "        outputs = []\n",
    "        for i in range(self.num_binary):\n",
    "            shifted = tf.clip_by_value(inputs + self.shift[i], 0, 1)\n",
    "            binary = tf.sign(shifted - 0.5)  # STE included automatically\n",
    "            outputs.append(binary * self.beta[i])\n",
    "        return tf.reduce_sum(outputs, axis=0)\n",
    "\n",
    "class ABCConv2D(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, num_binary=5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_binary = num_binary\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Full-precision weights storage\n",
    "        self.kernel = self.add_weight(\n",
    "            name='kernel',\n",
    "            shape=(self.kernel_size, self.kernel_size, input_shape[-1], self.filters),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True)\n",
    "        \n",
    "        # Binary approximation parameters\n",
    "        self.alpha = self.add_weight(name='alpha',\n",
    "                                   shape=(self.num_binary,),\n",
    "                                   initializer='ones',\n",
    "                                   trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Calculate mean and std for weight normalization\n",
    "        mean = tf.reduce_mean(self.kernel)\n",
    "        std = tf.math.reduce_std(self.kernel)\n",
    "        \n",
    "        # Generate multiple binary bases\n",
    "        binary_bases = []\n",
    "        for i in range(self.num_binary):\n",
    "            threshold = mean + (-1 + (2*i)/(self.num_binary-1)) * std\n",
    "            binary = tf.sign(self.kernel - threshold)\n",
    "            binary_bases.append(binary)\n",
    "        \n",
    "        # Combine binary bases with learned alphas\n",
    "        weighted_outputs = []\n",
    "        for i in range(self.num_binary):\n",
    "            conv = tf.nn.conv2d(inputs, binary_bases[i], \n",
    "                               strides=1, padding='SAME')\n",
    "            weighted_outputs.append(conv * self.alpha[i])\n",
    "            \n",
    "        return tf.add_n(weighted_outputs)\n",
    "\n",
    "# Modified network architecture\n",
    "def build_abcnet():\n",
    "    model = models.Sequential([\n",
    "        layers.InputLayer(input_shape=(28, 28, 1)),\n",
    "        \n",
    "        # Block 1\n",
    "        ABCConv2D(4, 3, num_binary=5),\n",
    "        layers.BatchNormalization(),\n",
    "        BinaryActivation(num_binary=5),\n",
    "        layers.MaxPooling2D(2),\n",
    "        \n",
    "        # Block 2\n",
    "        ABCConv2D(4, 3, num_binary=5),\n",
    "        layers.BatchNormalization(),\n",
    "        BinaryActivation(num_binary=5),\n",
    "        layers.MaxPooling2D(2),\n",
    "\n",
    "        # Block 3\n",
    "        ABCConv2D(8, 3, num_binary=5),\n",
    "        layers.BatchNormalization(),\n",
    "        BinaryActivation(num_binary=5),\n",
    "        layers.MaxPooling2D(2),\n",
    "        \n",
    "        # Block 3\n",
    "        ABCConv2D(16, 3, num_binary=5),\n",
    "        layers.BatchNormalization(),\n",
    "        BinaryActivation(num_binary=5),\n",
    "        layers.GlobalMaxPooling2D(),\n",
    "        \n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Training configuration\n",
    "def train_model():\n",
    "    (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "    x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "    \n",
    "    # Paper-specific preprocessing\n",
    "    x_train = (x_train - 0.5) * 2.0\n",
    "    x_test = (x_test - 0.5) * 2.0\n",
    "    \n",
    "    model = build_abcnet()\n",
    "    \n",
    "    # Two-phase training as described in the paper\n",
    "    # Phase 1: Train binary parameters only\n",
    "    for layer in model.layers:\n",
    "        if not isinstance(layer, (ABCConv2D, BinaryActivation)):\n",
    "            layer.trainable = False\n",
    "            \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(x_train, y_train,\n",
    "             batch_size=128,\n",
    "             epochs=10,\n",
    "             validation_split=0.1)\n",
    "    \n",
    "    # Phase 2: Fine-tune all parameters\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(x_train, y_train,\n",
    "             batch_size=128,\n",
    "             epochs=20,\n",
    "             validation_split=0.1)\n",
    "    \n",
    "    # Evaluation\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80128156-1f4e-43ac-a2e9-c9a7c7786c57",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_1/nemodel.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m checkpoint_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(checkpoint_path)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave(checkpoint_dir)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "checkpoint_path = \"training_1/nemodel.h5\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "model.save(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaedc59-ccee-4c0d-9a06-f985a9036db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhat/.local/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 43ms/step - accuracy: 0.3227 - loss: 2.2542 - val_accuracy: 0.6891 - val_loss: 0.9146\n",
      "Epoch 2/30\n",
      "\u001b[1m  1/468\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 36ms/step - accuracy: 0.7344 - loss: 0.8996"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhat/.local/lib/python3.10/site-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7344 - loss: 0.8996 - val_accuracy: 0.5850 - val_loss: 1.2038\n",
      "Epoch 3/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.7902 - loss: 0.7937 - val_accuracy: 0.6343 - val_loss: 1.0297\n",
      "Epoch 4/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8750 - loss: 0.4380 - val_accuracy: 0.7859 - val_loss: 0.6645\n",
      "Epoch 5/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.8685 - loss: 0.4949 - val_accuracy: 0.8577 - val_loss: 0.4729\n",
      "Epoch 6/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8359 - loss: 0.4956 - val_accuracy: 0.8261 - val_loss: 0.5568\n",
      "Epoch 7/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.8897 - loss: 0.4008 - val_accuracy: 0.8788 - val_loss: 0.3897\n",
      "Epoch 8/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8594 - loss: 0.4171 - val_accuracy: 0.8398 - val_loss: 0.5173\n",
      "Epoch 9/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.9076 - loss: 0.3311 - val_accuracy: 0.8188 - val_loss: 0.5242\n",
      "Epoch 10/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8906 - loss: 0.3601 - val_accuracy: 0.8613 - val_loss: 0.4326\n",
      "Epoch 11/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.9174 - loss: 0.2866 - val_accuracy: 0.8962 - val_loss: 0.3233\n",
      "Epoch 12/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8906 - loss: 0.3001 - val_accuracy: 0.9059 - val_loss: 0.3068\n",
      "Epoch 13/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.9220 - loss: 0.2707 - val_accuracy: 0.8375 - val_loss: 0.5099\n",
      "Epoch 14/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9219 - loss: 0.2910 - val_accuracy: 0.8613 - val_loss: 0.4756\n",
      "Epoch 15/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 44ms/step - accuracy: 0.9240 - loss: 0.2607 - val_accuracy: 0.6908 - val_loss: 1.0429\n",
      "Epoch 16/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9453 - loss: 0.2565 - val_accuracy: 0.7588 - val_loss: 0.7744\n",
      "Epoch 17/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 58ms/step - accuracy: 0.9229 - loss: 0.2606 - val_accuracy: 0.7587 - val_loss: 0.6526\n",
      "Epoch 18/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9531 - loss: 0.1852 - val_accuracy: 0.8299 - val_loss: 0.4842\n",
      "Epoch 19/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 54ms/step - accuracy: 0.9263 - loss: 0.2477 - val_accuracy: 0.8965 - val_loss: 0.3121\n",
      "Epoch 20/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9531 - loss: 0.1888 - val_accuracy: 0.9175 - val_loss: 0.2691\n",
      "Epoch 21/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 51ms/step - accuracy: 0.9331 - loss: 0.2215 - val_accuracy: 0.9075 - val_loss: 0.3103\n",
      "Epoch 22/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9375 - loss: 0.2323 - val_accuracy: 0.8845 - val_loss: 0.3777\n",
      "Epoch 23/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 59ms/step - accuracy: 0.9323 - loss: 0.2238 - val_accuracy: 0.9136 - val_loss: 0.2597\n",
      "Epoch 24/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9453 - loss: 0.1636 - val_accuracy: 0.8446 - val_loss: 0.4723\n",
      "Epoch 25/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 52ms/step - accuracy: 0.9371 - loss: 0.2075 - val_accuracy: 0.8767 - val_loss: 0.3673\n",
      "Epoch 26/30\n",
      "\u001b[1m468/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9375 - loss: 0.2177 - val_accuracy: 0.8545 - val_loss: 0.4281\n",
      "Epoch 27/30\n",
      "\u001b[1m457/468\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9404 - loss: 0.2034"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, datasets\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Custom Binary Convolution Layer with STE\n",
    "class BinaryConv2D(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, strides=(1,1), padding='same', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Full-precision weights storage\n",
    "        self.kernel = self.add_weight(\n",
    "            name='kernel',\n",
    "            shape=(self.kernel_size, self.kernel_size, input_shape[-1], self.filters),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "    @tf.custom_gradient\n",
    "    def binarize(self, weights):\n",
    "        def grad(dy):\n",
    "            return dy  # Straight-Through Estimator\n",
    "        return tf.sign(weights), grad\n",
    "\n",
    "    def call(self, inputs):\n",
    "        binary_kernel = self.binarize(self.kernel)\n",
    "        return tf.nn.conv2d(\n",
    "            inputs,\n",
    "            binary_kernel,\n",
    "            strides=[1, self.strides[0], self.strides[1], 1],\n",
    "            padding=self.padding.upper()\n",
    "        )\n",
    "\n",
    "# Build the model with ReLU activations\n",
    "def create_binary_relu_model():\n",
    "    model = models.Sequential([\n",
    "        layers.InputLayer(input_shape=(28, 28, 1)),\n",
    "        \n",
    "        # Block 1\n",
    "        BinaryConv2D(4, 3, padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        BinaryConv2D(4, 3, padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.MaxPooling2D(2),\n",
    "        \n",
    "        # Block 2\n",
    "        BinaryConv2D(8, 3, padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        BinaryConv2D(8, 3, padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.MaxPooling2D(2),\n",
    "        \n",
    "        # Block 3\n",
    "        BinaryConv2D(16, 3, padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.ReLU(),\n",
    "        layers.GlobalMaxPooling2D(),\n",
    "        \n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Data preparation and training\n",
    "def train_model():\n",
    "    (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "    \n",
    "    # Preprocessing\n",
    "    x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "    x_train = (x_train - 0.5) * 2.0  # Scale to [-1, 1]\n",
    "    x_test = (x_test - 0.5) * 2.0\n",
    "\n",
    "    # Data augmentation\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.1\n",
    "    )\n",
    "\n",
    "    # Learning rate schedule\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-3,\n",
    "        decay_steps=600,\n",
    "        decay_rate=0.9,\n",
    "        staircase=True\n",
    "    )\n",
    "\n",
    "    # Create and compile model\n",
    "    model = create_binary_relu_model()\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    history = model.fit(\n",
    "        train_datagen.flow(x_train, y_train, batch_size=128),\n",
    "        steps_per_epoch=len(x_train) // 128,\n",
    "        epochs=30,\n",
    "        validation_data=(x_test, y_test),\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"\\nTest accuracy: {test_acc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60076126-938e-435d-a7d9-ce90467fe7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
