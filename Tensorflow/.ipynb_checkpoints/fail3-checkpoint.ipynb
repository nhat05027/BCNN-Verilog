{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa03016-6d3b-4299-bb98-bd6d046803a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar  7 16:17:38 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.77.01              Driver Version: 566.36         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...    On  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   52C    P8              8W /   75W |     359MiB /   4096MiB |      3%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "327de96a-04f1-4e89-ae13-ce7d0eba90b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0415fc37-706a-4bde-a6eb-e160fbde23dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom autograd functions for binary operations\n",
    "class SignSTE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return torch.sign(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.clone()\n",
    "\n",
    "class BinaryWeightSTE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return torch.sign(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.clone()\n",
    "\n",
    "def Binarize(tensor,quant_mode='det'):\n",
    "    if quant_mode=='det':\n",
    "        return tensor.sign()\n",
    "    else:\n",
    "        return tensor.add_(1).div_(2).add_(torch.rand(tensor.size()).add(-0.5)).clamp_(0,1).round().mul_(2).add_(-1)\n",
    "# Binary convolutional layer\n",
    "class BinarizeLinear(nn.Linear):\n",
    "\n",
    "    def __init__(self, *kargs, **kwargs):\n",
    "        super(BinarizeLinear, self).__init__(*kargs, **kwargs)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        if input.size(1) != 784:\n",
    "            input.data=Binarize(input.data)\n",
    "        if not hasattr(self.weight,'org'):\n",
    "            self.weight.org=self.weight.data.clone()\n",
    "        self.weight.data=Binarize(self.weight.org)\n",
    "        out = F.linear(input, self.weight)\n",
    "        if not self.bias is None:\n",
    "            self.bias.org=self.bias.data.clone()\n",
    "            out += self.bias.view(1, -1).expand_as(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class BinaryConv2d(nn.Conv2d):\n",
    "\n",
    "    def __init__(self, *kargs, **kwargs):\n",
    "        super(BinaryConv2d, self).__init__(*kargs, **kwargs)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.size(1) != 3:\n",
    "            input.data = Binarize(input.data)\n",
    "        if not hasattr(self.weight,'org'):\n",
    "            self.weight.org=self.weight.data.clone()\n",
    "        self.weight.data=Binarize(self.weight.org)\n",
    "\n",
    "        out = F.conv2d(input, self.weight, None, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "        if not self.bias is None:\n",
    "            self.bias.org=self.bias.data.clone()\n",
    "            out += self.bias.view(1, -1, 1, 1).expand_as(out)\n",
    "        return out\n",
    "\n",
    "# Binary CNN architecture\n",
    "class BinaryCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryCNN, self).__init__()\n",
    "        self.ratioInfl=0.0625\n",
    "        self.features = nn.Sequential(\n",
    "            BinaryConv2d(1, int(64*self.ratioInfl), kernel_size=3, stride=1, padding=1),\n",
    "\n",
    "            nn.BatchNorm2d(int(64*self.ratioInfl)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            BinaryConv2d(int(64*self.ratioInfl), int(64*self.ratioInfl), kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(int(64*self.ratioInfl)),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            BinaryConv2d(int(64*self.ratioInfl), int(128*self.ratioInfl), kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(int(128*self.ratioInfl)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            BinaryConv2d(int(128*self.ratioInfl), int(128*self.ratioInfl), kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(int(128*self.ratioInfl)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            BinaryConv2d(int(128*self.ratioInfl), 16, kernel_size=3, padding=1),\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            BinarizeLinear(16 , 10),\n",
    "            #nn.ReLU(inplace=True),\n",
    "            #nn.Dropout(0.5),\n",
    "            #BinarizeLinear(16, num_classes),\n",
    "            #nn.BatchNorm1d(10),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        #x = x.view(-1, 256*3*3)\n",
    "        x = torch.squeeze(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5bbc12e7-c39f-49a2-851b-d63a06f9e10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5],std=[0.5])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "af62e5f5-189f-4b91-845c-4e2fbeb622e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n",
      "tensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3412,\n",
      "           0.4510,  0.2471,  0.1843, -0.5294, -0.7176, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.7412,\n",
      "           0.9922,  0.9922,  0.9922,  0.9922,  0.8902,  0.5529,  0.5529,\n",
      "           0.5529,  0.5529,  0.5529,  0.5529,  0.5529,  0.5529,  0.3333,\n",
      "          -0.5922, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4745,\n",
      "          -0.1059, -0.4353, -0.1059,  0.2784,  0.7804,  0.9922,  0.7647,\n",
      "           0.9922,  0.9922,  0.9922,  0.9608,  0.7961,  0.9922,  0.9922,\n",
      "           0.0980, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -0.8667, -0.4824, -0.8902,\n",
      "          -0.4745, -0.4745, -0.4745, -0.5373, -0.8353,  0.8510,  0.9922,\n",
      "          -0.1686, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -0.3490,  0.9843,  0.6392,\n",
      "          -0.8588, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -0.8275,  0.8275,  1.0000, -0.3490,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000,  0.0118,  0.9922,  0.8667, -0.6549,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -0.5373,  0.9529,  0.9922, -0.5137, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000,  0.0431,  0.9922,  0.4667, -0.9608, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -0.9294,  0.6078,  0.9451, -0.5451, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -0.0118,  0.9922,  0.4275, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -0.4118,  0.9686,  0.8824, -0.5529, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8510,\n",
      "           0.7333,  0.9922,  0.3020, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9765,  0.5922,\n",
      "           0.9922,  0.7176, -0.7255, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7020,  0.9922,\n",
      "           0.9922, -0.3961, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -0.7569,  0.7569,  0.9922,\n",
      "          -0.0980, -0.9922, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000,  0.0431,  0.9922,  0.9922,\n",
      "          -0.5922, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -0.5216,  0.8980,  0.9922,  0.9922,\n",
      "          -0.5922, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -0.0510,  0.9922,  0.9922,  0.7176,\n",
      "          -0.6863, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -0.0510,  0.9922,  0.6235, -0.8588,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(test_dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c5afdda3-0ed6-40b1-8192-6f212909b00d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (1, 28, 28) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m mnist_og \u001b[38;5;241m=\u001b[39m test_dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.imshow(mnist_og, cmap=cm.Greys_r)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmnist_og\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/matplotlib/pyplot.py:3476\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   3455\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[1;32m   3456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mimshow\u001b[39m(\n\u001b[1;32m   3457\u001b[0m     X: ArrayLike \u001b[38;5;241m|\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3474\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3475\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AxesImage:\n\u001b[0;32m-> 3476\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3480\u001b[0m \u001b[43m        \u001b[49m\u001b[43maspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3481\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3482\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3485\u001b[0m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3487\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3491\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3492\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3493\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3494\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3495\u001b[0m     sci(__ret)\n\u001b[1;32m   3496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/matplotlib/__init__.py:1473\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1473\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1474\u001b[0m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1478\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1479\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1480\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/matplotlib/axes/_axes.py:5895\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5893\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m-> 5895\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5896\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5898\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/matplotlib/image.py:729\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    728\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/matplotlib/image.py:697\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    695\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[0;32m--> 697\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (1, 28, 28) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "mnist_og = test_dataset[0][0]\n",
    "# plt.imshow(mnist_og, cmap=cm.Greys_r)\n",
    "# plt.show()\n",
    "plt.imshow(mnist_og, interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f8ba9c99-9bec-418b-af5e-c9305e00d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BinaryCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "10b4ea36-234c-4e90-abbd-2246196a908e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.198937\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.187845\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.267798\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.290514\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.231546\n",
      "\n",
      "Test set: Average loss: 2.3462, Accuracy: 1183/10000 (11.83%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.214943\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.239225\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.259748\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.169056\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.228522\n",
      "\n",
      "Test set: Average loss: 2.2099, Accuracy: 1650/10000 (16.50%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.193044\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 2.206994\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.323282\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 2.398580\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 2.282883\n",
      "\n",
      "Test set: Average loss: 2.6631, Accuracy: 763/10000 (7.63%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.198756\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 2.320337\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 2.307584\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 2.354470\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 2.250252\n",
      "\n",
      "Test set: Average loss: 2.2409, Accuracy: 1774/10000 (17.74%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.304982\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 2.255109\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 2.304946\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 2.348133\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 2.235723\n",
      "\n",
      "Test set: Average loss: 2.2410, Accuracy: 1932/10000 (19.32%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.321658\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 2.155562\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 2.247697\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 2.289620\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 2.221854\n",
      "\n",
      "Test set: Average loss: 2.6572, Accuracy: 974/10000 (9.74%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.224840\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 2.263509\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 2.213759\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 2.156017\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 2.229696\n",
      "\n",
      "Test set: Average loss: 2.6301, Accuracy: 1055/10000 (10.55%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.274154\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 2.226108\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 2.209928\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 2.243467\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 2.262484\n",
      "\n",
      "Test set: Average loss: 2.3022, Accuracy: 1297/10000 (12.97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.262383\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 2.248173\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 2.177047\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 2.280645\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 2.241801\n",
      "\n",
      "Test set: Average loss: 2.4272, Accuracy: 1155/10000 (11.55%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}'\n",
    "                  f' ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "# Testing loop\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() * data.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
    "# Run training and testing\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "98a8c21d-2c23-4364-886d-fd3e5911b4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING - Epoch: [1][0/469]\tTime 0.090 (0.090)\tData 0.072 (0.072)\tLoss 2.3080 (2.3080)\tPrec@1 8.594 (8.594)\tPrec@5 41.406 (41.406)\n",
      "TRAINING - Epoch: [1][10/469]\tTime 0.027 (0.031)\tData 0.002 (0.008)\tLoss 2.2950 (2.3014)\tPrec@1 12.500 (10.369)\tPrec@5 53.906 (52.415)\n",
      "TRAINING - Epoch: [1][20/469]\tTime 0.010 (0.021)\tData 0.001 (0.005)\tLoss 2.3106 (2.3021)\tPrec@1 9.375 (10.268)\tPrec@5 43.750 (51.339)\n",
      "TRAINING - Epoch: [1][30/469]\tTime 0.011 (0.017)\tData 0.002 (0.004)\tLoss 2.2926 (2.3010)\tPrec@1 14.844 (10.887)\tPrec@5 57.031 (51.714)\n",
      "TRAINING - Epoch: [1][40/469]\tTime 0.010 (0.015)\tData 0.002 (0.004)\tLoss 2.2996 (2.3008)\tPrec@1 12.500 (11.242)\tPrec@5 49.219 (51.982)\n",
      "TRAINING - Epoch: [1][50/469]\tTime 0.008 (0.013)\tData 0.002 (0.003)\tLoss 2.3077 (2.3008)\tPrec@1 4.688 (11.045)\tPrec@5 56.250 (52.114)\n",
      "TRAINING - Epoch: [1][60/469]\tTime 0.010 (0.013)\tData 0.005 (0.003)\tLoss 2.3051 (2.3012)\tPrec@1 7.812 (11.155)\tPrec@5 53.906 (51.742)\n",
      "TRAINING - Epoch: [1][70/469]\tTime 0.011 (0.012)\tData 0.005 (0.003)\tLoss 2.2973 (2.3007)\tPrec@1 17.188 (11.466)\tPrec@5 50.781 (51.926)\n",
      "TRAINING - Epoch: [1][80/469]\tTime 0.016 (0.012)\tData 0.003 (0.003)\tLoss 2.3100 (2.3009)\tPrec@1 7.031 (11.497)\tPrec@5 48.438 (51.842)\n",
      "TRAINING - Epoch: [1][90/469]\tTime 0.019 (0.012)\tData 0.005 (0.003)\tLoss 2.3045 (2.3007)\tPrec@1 8.594 (11.513)\tPrec@5 46.875 (51.949)\n",
      "TRAINING - Epoch: [1][100/469]\tTime 0.012 (0.012)\tData 0.004 (0.003)\tLoss 2.3020 (2.3010)\tPrec@1 13.281 (11.433)\tPrec@5 46.094 (51.849)\n",
      "TRAINING - Epoch: [1][110/469]\tTime 0.016 (0.012)\tData 0.003 (0.003)\tLoss 2.3008 (2.3010)\tPrec@1 8.594 (11.416)\tPrec@5 50.000 (51.717)\n",
      "TRAINING - Epoch: [1][120/469]\tTime 0.012 (0.012)\tData 0.002 (0.003)\tLoss 2.3005 (2.3010)\tPrec@1 13.281 (11.415)\tPrec@5 49.219 (51.692)\n",
      "TRAINING - Epoch: [1][130/469]\tTime 0.014 (0.012)\tData 0.003 (0.003)\tLoss 2.3040 (2.3011)\tPrec@1 9.375 (11.307)\tPrec@5 53.906 (51.735)\n",
      "TRAINING - Epoch: [1][140/469]\tTime 0.013 (0.012)\tData 0.002 (0.003)\tLoss 2.2966 (2.3009)\tPrec@1 17.969 (11.431)\tPrec@5 53.906 (51.934)\n",
      "TRAINING - Epoch: [1][150/469]\tTime 0.016 (0.013)\tData 0.002 (0.003)\tLoss 2.3023 (2.3010)\tPrec@1 14.844 (11.460)\tPrec@5 47.656 (51.914)\n",
      "TRAINING - Epoch: [1][160/469]\tTime 0.010 (0.013)\tData 0.002 (0.003)\tLoss 2.2833 (2.3009)\tPrec@1 16.406 (11.462)\tPrec@5 60.156 (52.033)\n",
      "TRAINING - Epoch: [1][170/469]\tTime 0.011 (0.013)\tData 0.002 (0.003)\tLoss 2.3033 (2.3009)\tPrec@1 10.938 (11.472)\tPrec@5 47.656 (51.873)\n",
      "TRAINING - Epoch: [1][180/469]\tTime 0.012 (0.013)\tData 0.002 (0.003)\tLoss 2.3018 (2.3008)\tPrec@1 9.375 (11.425)\tPrec@5 49.219 (51.882)\n",
      "TRAINING - Epoch: [1][190/469]\tTime 0.030 (0.013)\tData 0.003 (0.003)\tLoss 2.3110 (2.3009)\tPrec@1 7.031 (11.412)\tPrec@5 43.750 (51.804)\n",
      "TRAINING - Epoch: [1][200/469]\tTime 0.007 (0.013)\tData 0.002 (0.003)\tLoss 2.2997 (2.3010)\tPrec@1 10.938 (11.315)\tPrec@5 54.688 (51.710)\n",
      "TRAINING - Epoch: [1][210/469]\tTime 0.013 (0.013)\tData 0.001 (0.003)\tLoss 2.3022 (2.3010)\tPrec@1 10.156 (11.386)\tPrec@5 55.469 (51.848)\n",
      "TRAINING - Epoch: [1][220/469]\tTime 0.011 (0.013)\tData 0.001 (0.003)\tLoss 2.3025 (2.3010)\tPrec@1 14.844 (11.411)\tPrec@5 49.219 (51.806)\n",
      "TRAINING - Epoch: [1][230/469]\tTime 0.011 (0.013)\tData 0.002 (0.002)\tLoss 2.2996 (2.3010)\tPrec@1 14.062 (11.418)\tPrec@5 52.344 (51.803)\n",
      "TRAINING - Epoch: [1][240/469]\tTime 0.012 (0.013)\tData 0.002 (0.002)\tLoss 2.3000 (2.3010)\tPrec@1 11.719 (11.430)\tPrec@5 53.125 (51.776)\n",
      "TRAINING - Epoch: [1][250/469]\tTime 0.009 (0.013)\tData 0.002 (0.002)\tLoss 2.3065 (2.3011)\tPrec@1 7.812 (11.379)\tPrec@5 53.125 (51.709)\n",
      "TRAINING - Epoch: [1][260/469]\tTime 0.008 (0.012)\tData 0.001 (0.002)\tLoss 2.3035 (2.3011)\tPrec@1 9.375 (11.324)\tPrec@5 50.000 (51.661)\n",
      "TRAINING - Epoch: [1][270/469]\tTime 0.010 (0.012)\tData 0.002 (0.002)\tLoss 2.2900 (2.3009)\tPrec@1 14.844 (11.344)\tPrec@5 54.688 (51.698)\n",
      "TRAINING - Epoch: [1][280/469]\tTime 0.008 (0.012)\tData 0.002 (0.002)\tLoss 2.2984 (2.3009)\tPrec@1 8.594 (11.313)\tPrec@5 52.344 (51.668)\n",
      "TRAINING - Epoch: [1][290/469]\tTime 0.008 (0.012)\tData 0.002 (0.002)\tLoss 2.2983 (2.3009)\tPrec@1 8.594 (11.313)\tPrec@5 51.562 (51.697)\n",
      "TRAINING - Epoch: [1][300/469]\tTime 0.008 (0.012)\tData 0.001 (0.002)\tLoss 2.3161 (2.3009)\tPrec@1 10.156 (11.296)\tPrec@5 48.438 (51.716)\n",
      "TRAINING - Epoch: [1][310/469]\tTime 0.008 (0.012)\tData 0.001 (0.002)\tLoss 2.2976 (2.3008)\tPrec@1 14.062 (11.284)\tPrec@5 52.344 (51.733)\n",
      "TRAINING - Epoch: [1][320/469]\tTime 0.009 (0.012)\tData 0.001 (0.002)\tLoss 2.3018 (2.3007)\tPrec@1 4.688 (11.264)\tPrec@5 53.125 (51.784)\n",
      "TRAINING - Epoch: [1][330/469]\tTime 0.008 (0.012)\tData 0.001 (0.002)\tLoss 2.3070 (2.3006)\tPrec@1 7.031 (11.268)\tPrec@5 49.219 (51.803)\n",
      "TRAINING - Epoch: [1][340/469]\tTime 0.010 (0.012)\tData 0.001 (0.002)\tLoss 2.2806 (2.3004)\tPrec@1 10.156 (11.288)\tPrec@5 51.562 (51.750)\n",
      "TRAINING - Epoch: [1][350/469]\tTime 0.014 (0.012)\tData 0.003 (0.002)\tLoss 2.2906 (2.3002)\tPrec@1 13.281 (11.298)\tPrec@5 57.031 (51.812)\n",
      "TRAINING - Epoch: [1][360/469]\tTime 0.017 (0.012)\tData 0.003 (0.002)\tLoss 2.2884 (2.3000)\tPrec@1 10.938 (11.290)\tPrec@5 55.469 (51.742)\n",
      "TRAINING - Epoch: [1][370/469]\tTime 0.012 (0.012)\tData 0.002 (0.002)\tLoss 2.2618 (2.2996)\tPrec@1 14.844 (11.304)\tPrec@5 55.469 (51.777)\n",
      "TRAINING - Epoch: [1][380/469]\tTime 0.012 (0.012)\tData 0.001 (0.002)\tLoss 2.3199 (2.2992)\tPrec@1 10.156 (11.290)\tPrec@5 53.125 (51.763)\n",
      "TRAINING - Epoch: [1][390/469]\tTime 0.012 (0.012)\tData 0.001 (0.002)\tLoss 2.3008 (2.2992)\tPrec@1 5.469 (11.261)\tPrec@5 46.875 (51.688)\n",
      "TRAINING - Epoch: [1][400/469]\tTime 0.008 (0.012)\tData 0.002 (0.002)\tLoss 2.2609 (2.2987)\tPrec@1 18.750 (11.271)\tPrec@5 60.156 (51.742)\n",
      "TRAINING - Epoch: [1][410/469]\tTime 0.013 (0.012)\tData 0.001 (0.002)\tLoss 2.2918 (2.2978)\tPrec@1 7.031 (11.272)\tPrec@5 46.875 (51.785)\n",
      "TRAINING - Epoch: [1][420/469]\tTime 0.013 (0.012)\tData 0.001 (0.002)\tLoss 2.2328 (2.2962)\tPrec@1 13.281 (11.286)\tPrec@5 53.125 (51.763)\n",
      "TRAINING - Epoch: [1][430/469]\tTime 0.012 (0.012)\tData 0.001 (0.002)\tLoss 2.3181 (2.2950)\tPrec@1 6.250 (11.231)\tPrec@5 50.000 (51.708)\n",
      "TRAINING - Epoch: [1][440/469]\tTime 0.013 (0.012)\tData 0.002 (0.002)\tLoss 2.2356 (2.2934)\tPrec@1 7.812 (11.203)\tPrec@5 46.875 (51.704)\n",
      "TRAINING - Epoch: [1][450/469]\tTime 0.011 (0.012)\tData 0.001 (0.002)\tLoss 2.3116 (2.2912)\tPrec@1 4.688 (11.209)\tPrec@5 53.906 (51.713)\n",
      "TRAINING - Epoch: [1][460/469]\tTime 0.013 (0.012)\tData 0.001 (0.002)\tLoss 2.2236 (2.2892)\tPrec@1 8.594 (11.219)\tPrec@5 56.250 (51.722)\n",
      "EVALUATING - Epoch: [1][0/79]\tTime 0.061 (0.061)\tData 0.056 (0.056)\tLoss 2.3521 (2.3521)\tPrec@1 16.406 (16.406)\tPrec@5 57.812 (57.812)\n",
      "EVALUATING - Epoch: [1][10/79]\tTime 0.009 (0.013)\tData 0.006 (0.010)\tLoss 2.3577 (2.3431)\tPrec@1 13.281 (14.631)\tPrec@5 48.438 (50.639)\n",
      "EVALUATING - Epoch: [1][20/79]\tTime 0.011 (0.011)\tData 0.009 (0.008)\tLoss 2.4445 (2.3662)\tPrec@1 7.812 (13.132)\tPrec@5 53.125 (50.818)\n",
      "EVALUATING - Epoch: [1][30/79]\tTime 0.010 (0.010)\tData 0.008 (0.007)\tLoss 2.3500 (2.3671)\tPrec@1 10.938 (13.105)\tPrec@5 49.219 (50.806)\n",
      "EVALUATING - Epoch: [1][40/79]\tTime 0.012 (0.010)\tData 0.009 (0.007)\tLoss 2.3914 (2.3619)\tPrec@1 9.375 (13.224)\tPrec@5 49.219 (51.410)\n",
      "EVALUATING - Epoch: [1][50/79]\tTime 0.003 (0.010)\tData 0.000 (0.007)\tLoss 2.3415 (2.3545)\tPrec@1 11.719 (13.281)\tPrec@5 54.688 (51.440)\n",
      "EVALUATING - Epoch: [1][60/79]\tTime 0.003 (0.010)\tData 0.000 (0.007)\tLoss 2.2706 (2.3550)\tPrec@1 15.625 (13.166)\tPrec@5 49.219 (51.498)\n",
      "EVALUATING - Epoch: [1][70/79]\tTime 0.002 (0.010)\tData 0.000 (0.007)\tLoss 2.1695 (2.3444)\tPrec@1 25.000 (13.545)\tPrec@5 54.688 (51.607)\n",
      "\n",
      " Epoch: 2\tTraining Loss 2.2874 \tTraining Prec@1 11.318 \tTraining Prec@5 51.705 \tValidation Loss 2.3423 \tValidation Prec@1 13.740 \tValidation Prec@5 51.730 \n",
      "\n",
      "TRAINING - Epoch: [2][0/469]\tTime 0.100 (0.100)\tData 0.079 (0.079)\tLoss 2.2376 (2.2376)\tPrec@1 14.062 (14.062)\tPrec@5 50.781 (50.781)\n",
      "TRAINING - Epoch: [2][10/469]\tTime 0.017 (0.027)\tData 0.002 (0.010)\tLoss 2.2051 (2.2089)\tPrec@1 10.938 (13.068)\tPrec@5 47.656 (51.136)\n",
      "TRAINING - Epoch: [2][20/469]\tTime 0.013 (0.021)\tData 0.001 (0.006)\tLoss 2.2253 (2.2063)\tPrec@1 21.094 (15.365)\tPrec@5 50.781 (51.823)\n",
      "TRAINING - Epoch: [2][30/469]\tTime 0.014 (0.019)\tData 0.002 (0.005)\tLoss 2.2107 (2.2048)\tPrec@1 10.156 (15.348)\tPrec@5 53.125 (53.881)\n",
      "TRAINING - Epoch: [2][40/469]\tTime 0.018 (0.017)\tData 0.004 (0.004)\tLoss 2.2976 (2.2105)\tPrec@1 14.062 (15.454)\tPrec@5 64.062 (55.716)\n",
      "TRAINING - Epoch: [2][50/469]\tTime 0.015 (0.017)\tData 0.002 (0.004)\tLoss 2.2451 (2.2107)\tPrec@1 19.531 (15.043)\tPrec@5 61.719 (56.464)\n",
      "TRAINING - Epoch: [2][60/469]\tTime 0.011 (0.017)\tData 0.001 (0.004)\tLoss 2.1914 (2.2129)\tPrec@1 14.844 (15.369)\tPrec@5 62.500 (57.082)\n",
      "TRAINING - Epoch: [2][70/469]\tTime 0.013 (0.016)\tData 0.002 (0.003)\tLoss 2.2016 (2.2169)\tPrec@1 12.500 (14.679)\tPrec@5 64.062 (57.383)\n",
      "TRAINING - Epoch: [2][80/469]\tTime 0.009 (0.015)\tData 0.002 (0.003)\tLoss 2.1672 (2.2154)\tPrec@1 11.719 (14.969)\tPrec@5 64.844 (57.986)\n",
      "TRAINING - Epoch: [2][90/469]\tTime 0.010 (0.014)\tData 0.002 (0.003)\tLoss 2.2583 (2.2161)\tPrec@1 7.812 (15.007)\tPrec@5 61.719 (58.336)\n",
      "TRAINING - Epoch: [2][100/469]\tTime 0.008 (0.014)\tData 0.002 (0.003)\tLoss 2.1998 (2.2162)\tPrec@1 10.938 (14.689)\tPrec@5 53.906 (58.222)\n",
      "TRAINING - Epoch: [2][110/469]\tTime 0.009 (0.014)\tData 0.002 (0.003)\tLoss 2.2421 (2.2148)\tPrec@1 14.844 (14.802)\tPrec@5 60.938 (58.305)\n",
      "TRAINING - Epoch: [2][120/469]\tTime 0.008 (0.013)\tData 0.002 (0.003)\tLoss 2.2721 (2.2139)\tPrec@1 9.375 (14.592)\tPrec@5 56.250 (58.381)\n",
      "TRAINING - Epoch: [2][130/469]\tTime 0.007 (0.013)\tData 0.002 (0.003)\tLoss 2.2429 (2.2139)\tPrec@1 14.844 (14.701)\tPrec@5 62.500 (58.582)\n",
      "TRAINING - Epoch: [2][140/469]\tTime 0.007 (0.013)\tData 0.002 (0.003)\tLoss 2.2813 (2.2140)\tPrec@1 12.500 (14.871)\tPrec@5 54.688 (58.572)\n",
      "TRAINING - Epoch: [2][150/469]\tTime 0.008 (0.013)\tData 0.002 (0.003)\tLoss 2.2081 (2.2120)\tPrec@1 14.062 (14.994)\tPrec@5 63.281 (58.656)\n",
      "TRAINING - Epoch: [2][160/469]\tTime 0.012 (0.012)\tData 0.002 (0.003)\tLoss 2.3025 (2.2121)\tPrec@1 9.375 (14.980)\tPrec@5 56.250 (58.890)\n",
      "TRAINING - Epoch: [2][170/469]\tTime 0.014 (0.012)\tData 0.002 (0.003)\tLoss 2.2564 (2.2108)\tPrec@1 9.375 (15.122)\tPrec@5 62.500 (59.064)\n",
      "TRAINING - Epoch: [2][180/469]\tTime 0.008 (0.012)\tData 0.002 (0.003)\tLoss 2.1954 (2.2105)\tPrec@1 8.594 (15.068)\tPrec@5 53.906 (59.125)\n",
      "TRAINING - Epoch: [2][190/469]\tTime 0.008 (0.012)\tData 0.002 (0.003)\tLoss 2.2466 (2.2103)\tPrec@1 13.281 (15.110)\tPrec@5 53.906 (58.995)\n",
      "TRAINING - Epoch: [2][200/469]\tTime 0.015 (0.012)\tData 0.004 (0.003)\tLoss 2.2485 (2.2112)\tPrec@1 13.281 (15.244)\tPrec@5 56.250 (58.979)\n",
      "TRAINING - Epoch: [2][210/469]\tTime 0.015 (0.012)\tData 0.002 (0.002)\tLoss 2.2060 (2.2114)\tPrec@1 13.281 (15.144)\tPrec@5 62.500 (59.075)\n",
      "TRAINING - Epoch: [2][220/469]\tTime 0.010 (0.012)\tData 0.002 (0.002)\tLoss 2.2294 (2.2113)\tPrec@1 17.969 (15.077)\tPrec@5 56.250 (59.195)\n",
      "TRAINING - Epoch: [2][230/469]\tTime 0.013 (0.012)\tData 0.002 (0.002)\tLoss 2.2861 (2.2128)\tPrec@1 11.719 (15.016)\tPrec@5 60.156 (59.253)\n",
      "TRAINING - Epoch: [2][240/469]\tTime 0.014 (0.012)\tData 0.002 (0.002)\tLoss 2.1607 (2.2112)\tPrec@1 12.500 (15.126)\tPrec@5 57.812 (59.304)\n",
      "TRAINING - Epoch: [2][250/469]\tTime 0.011 (0.012)\tData 0.002 (0.002)\tLoss 2.1699 (2.2101)\tPrec@1 12.500 (15.233)\tPrec@5 64.062 (59.397)\n",
      "TRAINING - Epoch: [2][260/469]\tTime 0.008 (0.012)\tData 0.001 (0.002)\tLoss 2.1715 (2.2099)\tPrec@1 13.281 (15.251)\tPrec@5 59.375 (59.465)\n",
      "TRAINING - Epoch: [2][270/469]\tTime 0.009 (0.011)\tData 0.002 (0.002)\tLoss 2.1866 (2.2104)\tPrec@1 18.750 (15.236)\tPrec@5 60.156 (59.502)\n",
      "TRAINING - Epoch: [2][280/469]\tTime 0.012 (0.011)\tData 0.002 (0.002)\tLoss 2.0513 (2.2091)\tPrec@1 32.812 (15.316)\tPrec@5 68.750 (59.639)\n",
      "TRAINING - Epoch: [2][290/469]\tTime 0.009 (0.011)\tData 0.002 (0.002)\tLoss 2.2687 (2.2092)\tPrec@1 14.844 (15.359)\tPrec@5 59.375 (59.601)\n",
      "TRAINING - Epoch: [2][300/469]\tTime 0.008 (0.011)\tData 0.002 (0.002)\tLoss 2.1518 (2.2102)\tPrec@1 24.219 (15.402)\tPrec@5 63.281 (59.554)\n",
      "TRAINING - Epoch: [2][310/469]\tTime 0.008 (0.011)\tData 0.002 (0.002)\tLoss 2.1726 (2.2104)\tPrec@1 24.219 (15.447)\tPrec@5 64.062 (59.651)\n",
      "TRAINING - Epoch: [2][320/469]\tTime 0.007 (0.011)\tData 0.000 (0.002)\tLoss 2.2236 (2.2109)\tPrec@1 7.031 (15.369)\tPrec@5 57.812 (59.667)\n",
      "TRAINING - Epoch: [2][330/469]\tTime 0.009 (0.011)\tData 0.002 (0.002)\tLoss 2.2613 (2.2103)\tPrec@1 11.719 (15.439)\tPrec@5 58.594 (59.696)\n",
      "TRAINING - Epoch: [2][340/469]\tTime 0.008 (0.011)\tData 0.002 (0.002)\tLoss 2.1986 (2.2097)\tPrec@1 26.562 (15.565)\tPrec@5 57.812 (59.675)\n",
      "TRAINING - Epoch: [2][350/469]\tTime 0.008 (0.011)\tData 0.002 (0.002)\tLoss 2.2782 (2.2108)\tPrec@1 10.938 (15.538)\tPrec@5 55.469 (59.613)\n",
      "TRAINING - Epoch: [2][360/469]\tTime 0.008 (0.011)\tData 0.002 (0.002)\tLoss 2.2212 (2.2108)\tPrec@1 14.062 (15.534)\tPrec@5 62.500 (59.533)\n",
      "TRAINING - Epoch: [2][370/469]\tTime 0.007 (0.011)\tData 0.002 (0.002)\tLoss 2.2588 (2.2117)\tPrec@1 15.625 (15.463)\tPrec@5 53.906 (59.535)\n",
      "TRAINING - Epoch: [2][380/469]\tTime 0.010 (0.011)\tData 0.004 (0.002)\tLoss 2.2155 (2.2113)\tPrec@1 19.531 (15.510)\tPrec@5 53.906 (59.488)\n",
      "TRAINING - Epoch: [2][390/469]\tTime 0.008 (0.011)\tData 0.001 (0.002)\tLoss 2.2472 (2.2111)\tPrec@1 10.938 (15.601)\tPrec@5 55.469 (59.477)\n",
      "TRAINING - Epoch: [2][400/469]\tTime 0.011 (0.011)\tData 0.002 (0.002)\tLoss 2.1934 (2.2120)\tPrec@1 18.750 (15.551)\tPrec@5 52.344 (59.416)\n",
      "TRAINING - Epoch: [2][410/469]\tTime 0.008 (0.011)\tData 0.002 (0.002)\tLoss 2.2289 (2.2115)\tPrec@1 14.844 (15.612)\tPrec@5 60.938 (59.449)\n",
      "TRAINING - Epoch: [2][420/469]\tTime 0.008 (0.011)\tData 0.002 (0.002)\tLoss 2.2096 (2.2112)\tPrec@1 10.156 (15.686)\tPrec@5 66.406 (59.481)\n",
      "TRAINING - Epoch: [2][430/469]\tTime 0.006 (0.011)\tData 0.000 (0.002)\tLoss 2.1980 (2.2117)\tPrec@1 17.969 (15.672)\tPrec@5 68.750 (59.486)\n",
      "TRAINING - Epoch: [2][440/469]\tTime 0.013 (0.011)\tData 0.002 (0.002)\tLoss 2.2214 (2.2125)\tPrec@1 20.312 (15.719)\tPrec@5 57.031 (59.380)\n",
      "TRAINING - Epoch: [2][450/469]\tTime 0.007 (0.010)\tData 0.001 (0.002)\tLoss 2.2380 (2.2127)\tPrec@1 14.062 (15.639)\tPrec@5 64.062 (59.404)\n",
      "TRAINING - Epoch: [2][460/469]\tTime 0.011 (0.010)\tData 0.004 (0.002)\tLoss 2.2217 (2.2125)\tPrec@1 11.719 (15.674)\tPrec@5 58.594 (59.394)\n",
      "EVALUATING - Epoch: [2][0/79]\tTime 0.059 (0.059)\tData 0.054 (0.054)\tLoss 2.2288 (2.2288)\tPrec@1 11.719 (11.719)\tPrec@5 52.344 (52.344)\n",
      "EVALUATING - Epoch: [2][10/79]\tTime 0.010 (0.012)\tData 0.007 (0.009)\tLoss 2.3336 (2.3260)\tPrec@1 10.156 (10.795)\tPrec@5 57.812 (59.730)\n",
      "EVALUATING - Epoch: [2][20/79]\tTime 0.005 (0.010)\tData 0.002 (0.007)\tLoss 2.3951 (2.3449)\tPrec@1 14.062 (10.863)\tPrec@5 59.375 (59.449)\n",
      "EVALUATING - Epoch: [2][30/79]\tTime 0.019 (0.010)\tData 0.015 (0.007)\tLoss 2.3003 (2.3424)\tPrec@1 9.375 (10.534)\tPrec@5 58.594 (59.677)\n",
      "EVALUATING - Epoch: [2][40/79]\tTime 0.012 (0.010)\tData 0.010 (0.007)\tLoss 2.4240 (2.3320)\tPrec@1 10.938 (10.423)\tPrec@5 53.125 (59.623)\n",
      "EVALUATING - Epoch: [2][50/79]\tTime 0.013 (0.009)\tData 0.011 (0.006)\tLoss 2.2956 (2.3268)\tPrec@1 11.719 (10.371)\tPrec@5 60.938 (59.789)\n",
      "EVALUATING - Epoch: [2][60/79]\tTime 0.018 (0.009)\tData 0.011 (0.006)\tLoss 2.3468 (2.3239)\tPrec@1 9.375 (10.502)\tPrec@5 61.719 (60.015)\n",
      "EVALUATING - Epoch: [2][70/79]\tTime 0.010 (0.009)\tData 0.004 (0.006)\tLoss 2.2233 (2.3197)\tPrec@1 10.938 (10.563)\tPrec@5 62.500 (59.815)\n",
      "\n",
      " Epoch: 3\tTraining Loss 2.2122 \tTraining Prec@1 15.738 \tTraining Prec@5 59.455 \tValidation Loss 2.3178 \tValidation Prec@1 10.560 \tValidation Prec@5 60.020 \n",
      "\n",
      "TRAINING - Epoch: [3][0/469]\tTime 0.073 (0.073)\tData 0.056 (0.056)\tLoss 2.2300 (2.2300)\tPrec@1 10.156 (10.156)\tPrec@5 57.031 (57.031)\n",
      "TRAINING - Epoch: [3][10/469]\tTime 0.010 (0.018)\tData 0.001 (0.007)\tLoss 2.2095 (2.1989)\tPrec@1 21.094 (14.773)\tPrec@5 58.594 (58.665)\n",
      "TRAINING - Epoch: [3][20/469]\tTime 0.012 (0.015)\tData 0.002 (0.004)\tLoss 2.1137 (2.2048)\tPrec@1 29.688 (16.518)\tPrec@5 62.500 (58.482)\n",
      "TRAINING - Epoch: [3][30/469]\tTime 0.014 (0.015)\tData 0.002 (0.004)\tLoss 2.2836 (2.2145)\tPrec@1 13.281 (15.953)\tPrec@5 57.812 (58.846)\n",
      "TRAINING - Epoch: [3][40/469]\tTime 0.013 (0.015)\tData 0.003 (0.003)\tLoss 2.2352 (2.2173)\tPrec@1 15.625 (15.663)\tPrec@5 60.156 (59.070)\n",
      "TRAINING - Epoch: [3][50/469]\tTime 0.013 (0.014)\tData 0.001 (0.003)\tLoss 2.2604 (2.2205)\tPrec@1 14.844 (15.594)\tPrec@5 54.688 (58.318)\n",
      "TRAINING - Epoch: [3][60/469]\tTime 0.013 (0.014)\tData 0.002 (0.003)\tLoss 2.2182 (2.2257)\tPrec@1 23.438 (15.638)\tPrec@5 59.375 (57.915)\n",
      "TRAINING - Epoch: [3][70/469]\tTime 0.013 (0.014)\tData 0.002 (0.003)\tLoss 2.2391 (2.2277)\tPrec@1 8.594 (15.207)\tPrec@5 62.500 (58.451)\n",
      "TRAINING - Epoch: [3][80/469]\tTime 0.010 (0.014)\tData 0.003 (0.003)\tLoss 2.1941 (2.2253)\tPrec@1 10.938 (15.046)\tPrec@5 62.500 (58.382)\n",
      "TRAINING - Epoch: [3][90/469]\tTime 0.009 (0.013)\tData 0.001 (0.003)\tLoss 2.2658 (2.2260)\tPrec@1 12.500 (15.153)\tPrec@5 57.031 (58.379)\n",
      "TRAINING - Epoch: [3][100/469]\tTime 0.008 (0.013)\tData 0.001 (0.002)\tLoss 2.1798 (2.2251)\tPrec@1 16.406 (15.099)\tPrec@5 56.250 (58.447)\n",
      "TRAINING - Epoch: [3][110/469]\tTime 0.008 (0.012)\tData 0.001 (0.002)\tLoss 2.1723 (2.2202)\tPrec@1 21.875 (15.407)\tPrec@5 66.406 (58.727)\n",
      "TRAINING - Epoch: [3][120/469]\tTime 0.008 (0.012)\tData 0.001 (0.002)\tLoss 2.2936 (2.2216)\tPrec@1 9.375 (15.560)\tPrec@5 59.375 (58.775)\n",
      "TRAINING - Epoch: [3][130/469]\tTime 0.009 (0.012)\tData 0.002 (0.002)\tLoss 2.2050 (2.2211)\tPrec@1 13.281 (15.530)\tPrec@5 63.281 (58.898)\n",
      "TRAINING - Epoch: [3][140/469]\tTime 0.008 (0.012)\tData 0.001 (0.002)\tLoss 2.1410 (2.2197)\tPrec@1 25.781 (15.780)\tPrec@5 64.062 (58.954)\n",
      "TRAINING - Epoch: [3][150/469]\tTime 0.008 (0.012)\tData 0.001 (0.002)\tLoss 2.2000 (2.2204)\tPrec@1 12.500 (15.656)\tPrec@5 67.969 (58.977)\n",
      "TRAINING - Epoch: [3][160/469]\tTime 0.009 (0.011)\tData 0.002 (0.002)\tLoss 2.0988 (2.2172)\tPrec@1 13.281 (15.911)\tPrec@5 69.531 (59.157)\n",
      "TRAINING - Epoch: [3][170/469]\tTime 0.008 (0.011)\tData 0.002 (0.002)\tLoss 2.1288 (2.2162)\tPrec@1 28.125 (16.100)\tPrec@5 64.062 (59.265)\n",
      "TRAINING - Epoch: [3][180/469]\tTime 0.009 (0.011)\tData 0.002 (0.002)\tLoss 2.1491 (2.2158)\tPrec@1 18.750 (16.143)\tPrec@5 60.938 (59.306)\n",
      "TRAINING - Epoch: [3][190/469]\tTime 0.011 (0.011)\tData 0.005 (0.002)\tLoss 2.2871 (2.2156)\tPrec@1 12.500 (16.226)\tPrec@5 54.688 (59.375)\n",
      "TRAINING - Epoch: [3][200/469]\tTime 0.011 (0.011)\tData 0.001 (0.002)\tLoss 2.1524 (2.2174)\tPrec@1 22.656 (16.088)\tPrec@5 61.719 (59.414)\n",
      "TRAINING - Epoch: [3][210/469]\tTime 0.009 (0.011)\tData 0.002 (0.002)\tLoss 2.3081 (2.2176)\tPrec@1 15.625 (16.166)\tPrec@5 58.594 (59.316)\n",
      "TRAINING - Epoch: [3][220/469]\tTime 0.008 (0.011)\tData 0.002 (0.002)\tLoss 2.3298 (2.2185)\tPrec@1 7.812 (16.046)\tPrec@5 52.344 (59.280)\n",
      "TRAINING - Epoch: [3][230/469]\tTime 0.008 (0.011)\tData 0.002 (0.002)\tLoss 2.1870 (2.2186)\tPrec@1 15.625 (16.058)\tPrec@5 56.250 (59.223)\n",
      "TRAINING - Epoch: [3][240/469]\tTime 0.013 (0.011)\tData 0.002 (0.002)\tLoss 2.1658 (2.2192)\tPrec@1 21.094 (16.085)\tPrec@5 63.281 (59.343)\n",
      "TRAINING - Epoch: [3][250/469]\tTime 0.012 (0.011)\tData 0.002 (0.002)\tLoss 2.2792 (2.2194)\tPrec@1 11.719 (16.008)\tPrec@5 59.375 (59.288)\n",
      "TRAINING - Epoch: [3][260/469]\tTime 0.013 (0.011)\tData 0.003 (0.002)\tLoss 2.1641 (2.2187)\tPrec@1 25.000 (16.092)\tPrec@5 67.969 (59.291)\n",
      "TRAINING - Epoch: [3][270/469]\tTime 0.011 (0.011)\tData 0.003 (0.002)\tLoss 2.2577 (2.2196)\tPrec@1 15.625 (16.080)\tPrec@5 58.594 (59.234)\n",
      "TRAINING - Epoch: [3][280/469]\tTime 0.014 (0.011)\tData 0.003 (0.002)\tLoss 2.2656 (2.2202)\tPrec@1 14.844 (16.073)\tPrec@5 56.250 (59.247)\n",
      "TRAINING - Epoch: [3][290/469]\tTime 0.014 (0.011)\tData 0.005 (0.002)\tLoss 2.2086 (2.2201)\tPrec@1 10.938 (16.060)\tPrec@5 57.031 (59.227)\n",
      "TRAINING - Epoch: [3][300/469]\tTime 0.008 (0.011)\tData 0.002 (0.002)\tLoss 2.3128 (2.2201)\tPrec@1 7.812 (16.020)\tPrec@5 50.781 (59.217)\n",
      "TRAINING - Epoch: [3][310/469]\tTime 0.011 (0.011)\tData 0.002 (0.002)\tLoss 2.3015 (2.2206)\tPrec@1 14.844 (16.045)\tPrec@5 62.500 (59.194)\n",
      "TRAINING - Epoch: [3][320/469]\tTime 0.007 (0.011)\tData 0.000 (0.002)\tLoss 2.3024 (2.2204)\tPrec@1 14.062 (16.080)\tPrec@5 56.250 (59.212)\n",
      "TRAINING - Epoch: [3][330/469]\tTime 0.010 (0.011)\tData 0.002 (0.002)\tLoss 2.1908 (2.2208)\tPrec@1 18.750 (16.090)\tPrec@5 67.188 (59.276)\n",
      "TRAINING - Epoch: [3][340/469]\tTime 0.010 (0.011)\tData 0.004 (0.002)\tLoss 2.2210 (2.2212)\tPrec@1 14.062 (16.031)\tPrec@5 57.031 (59.233)\n",
      "TRAINING - Epoch: [3][350/469]\tTime 0.016 (0.011)\tData 0.005 (0.002)\tLoss 2.1304 (2.2205)\tPrec@1 23.438 (16.079)\tPrec@5 65.625 (59.217)\n",
      "TRAINING - Epoch: [3][360/469]\tTime 0.010 (0.011)\tData 0.003 (0.002)\tLoss 2.2662 (2.2208)\tPrec@1 7.031 (16.028)\tPrec@5 59.375 (59.243)\n",
      "TRAINING - Epoch: [3][370/469]\tTime 0.013 (0.011)\tData 0.005 (0.002)\tLoss 2.2536 (2.2211)\tPrec@1 13.281 (15.998)\tPrec@5 56.250 (59.215)\n",
      "TRAINING - Epoch: [3][380/469]\tTime 0.013 (0.011)\tData 0.005 (0.002)\tLoss 2.1660 (2.2207)\tPrec@1 18.750 (16.049)\tPrec@5 60.938 (59.205)\n",
      "TRAINING - Epoch: [3][390/469]\tTime 0.011 (0.011)\tData 0.003 (0.002)\tLoss 2.3385 (2.2202)\tPrec@1 15.625 (16.119)\tPrec@5 52.344 (59.195)\n",
      "TRAINING - Epoch: [3][400/469]\tTime 0.007 (0.011)\tData 0.000 (0.002)\tLoss 2.1559 (2.2202)\tPrec@1 22.656 (16.067)\tPrec@5 51.562 (59.182)\n",
      "TRAINING - Epoch: [3][410/469]\tTime 0.012 (0.011)\tData 0.003 (0.002)\tLoss 2.2646 (2.2211)\tPrec@1 17.188 (16.119)\tPrec@5 55.469 (59.128)\n",
      "TRAINING - Epoch: [3][420/469]\tTime 0.016 (0.011)\tData 0.002 (0.002)\tLoss 2.2466 (2.2216)\tPrec@1 17.188 (16.094)\tPrec@5 51.562 (59.091)\n",
      "TRAINING - Epoch: [3][430/469]\tTime 0.012 (0.011)\tData 0.002 (0.002)\tLoss 2.2166 (2.2223)\tPrec@1 15.625 (16.082)\tPrec@5 47.656 (59.060)\n",
      "TRAINING - Epoch: [3][440/469]\tTime 0.010 (0.011)\tData 0.002 (0.002)\tLoss 2.2893 (2.2229)\tPrec@1 8.594 (16.013)\tPrec@5 50.781 (59.030)\n",
      "TRAINING - Epoch: [3][450/469]\tTime 0.010 (0.011)\tData 0.002 (0.002)\tLoss 2.2212 (2.2224)\tPrec@1 14.844 (16.084)\tPrec@5 58.594 (59.053)\n",
      "TRAINING - Epoch: [3][460/469]\tTime 0.009 (0.011)\tData 0.002 (0.002)\tLoss 2.1892 (2.2226)\tPrec@1 21.094 (16.084)\tPrec@5 61.719 (59.031)\n",
      "EVALUATING - Epoch: [3][0/79]\tTime 0.056 (0.056)\tData 0.050 (0.050)\tLoss 2.6328 (2.6328)\tPrec@1 14.844 (14.844)\tPrec@5 57.031 (57.031)\n",
      "EVALUATING - Epoch: [3][10/79]\tTime 0.022 (0.015)\tData 0.015 (0.010)\tLoss 2.8171 (2.8251)\tPrec@1 12.500 (12.997)\tPrec@5 50.000 (51.065)\n",
      "EVALUATING - Epoch: [3][20/79]\tTime 0.018 (0.013)\tData 0.013 (0.008)\tLoss 2.6331 (2.7958)\tPrec@1 15.625 (13.430)\tPrec@5 56.250 (51.414)\n",
      "EVALUATING - Epoch: [3][30/79]\tTime 0.008 (0.012)\tData 0.006 (0.007)\tLoss 2.8103 (2.7992)\tPrec@1 6.250 (13.080)\tPrec@5 44.531 (50.756)\n",
      "EVALUATING - Epoch: [3][40/79]\tTime 0.020 (0.011)\tData 0.017 (0.007)\tLoss 2.6899 (2.7800)\tPrec@1 14.844 (13.034)\tPrec@5 52.344 (51.620)\n",
      "EVALUATING - Epoch: [3][50/79]\tTime 0.018 (0.011)\tData 0.015 (0.007)\tLoss 2.6853 (2.7644)\tPrec@1 12.500 (13.526)\tPrec@5 53.906 (51.930)\n",
      "EVALUATING - Epoch: [3][60/79]\tTime 0.020 (0.011)\tData 0.016 (0.007)\tLoss 2.6499 (2.7577)\tPrec@1 11.719 (13.704)\tPrec@5 55.469 (52.062)\n",
      "EVALUATING - Epoch: [3][70/79]\tTime 0.020 (0.011)\tData 0.014 (0.007)\tLoss 2.6658 (2.7372)\tPrec@1 12.500 (13.930)\tPrec@5 58.594 (52.674)\n",
      "\n",
      " Epoch: 4\tTraining Loss 2.2228 \tTraining Prec@1 16.090 \tTraining Prec@5 59.033 \tValidation Loss 2.7348 \tValidation Prec@1 13.990 \tValidation Prec@5 52.880 \n",
      "\n",
      "TRAINING - Epoch: [4][0/469]\tTime 0.070 (0.070)\tData 0.058 (0.058)\tLoss 2.2361 (2.2361)\tPrec@1 10.938 (10.938)\tPrec@5 60.938 (60.938)\n",
      "TRAINING - Epoch: [4][10/469]\tTime 0.009 (0.015)\tData 0.001 (0.007)\tLoss 2.1683 (2.2202)\tPrec@1 23.438 (16.619)\tPrec@5 61.719 (60.227)\n",
      "TRAINING - Epoch: [4][20/469]\tTime 0.006 (0.012)\tData 0.000 (0.005)\tLoss 2.2731 (2.2438)\tPrec@1 7.812 (15.588)\tPrec@5 59.375 (59.598)\n",
      "TRAINING - Epoch: [4][30/469]\tTime 0.014 (0.012)\tData 0.002 (0.004)\tLoss 2.1841 (2.2383)\tPrec@1 17.969 (14.693)\tPrec@5 60.156 (59.829)\n",
      "TRAINING - Epoch: [4][40/469]\tTime 0.016 (0.012)\tData 0.005 (0.004)\tLoss 2.2779 (2.2306)\tPrec@1 10.938 (15.530)\tPrec@5 61.719 (59.566)\n",
      "TRAINING - Epoch: [4][50/469]\tTime 0.008 (0.012)\tData 0.002 (0.003)\tLoss 2.1269 (2.2279)\tPrec@1 21.875 (15.472)\tPrec@5 67.969 (59.804)\n",
      "TRAINING - Epoch: [4][60/469]\tTime 0.010 (0.011)\tData 0.002 (0.003)\tLoss 2.2373 (2.2287)\tPrec@1 9.375 (15.215)\tPrec@5 57.031 (59.580)\n",
      "TRAINING - Epoch: [4][70/469]\tTime 0.013 (0.011)\tData 0.005 (0.003)\tLoss 2.2772 (2.2323)\tPrec@1 13.281 (14.987)\tPrec@5 57.812 (59.716)\n",
      "TRAINING - Epoch: [4][80/469]\tTime 0.007 (0.011)\tData 0.002 (0.003)\tLoss 2.1803 (2.2289)\tPrec@1 12.500 (15.355)\tPrec@5 62.500 (59.751)\n",
      "TRAINING - Epoch: [4][90/469]\tTime 0.010 (0.010)\tData 0.004 (0.003)\tLoss 2.2770 (2.2311)\tPrec@1 14.844 (15.204)\tPrec@5 57.031 (59.744)\n",
      "TRAINING - Epoch: [4][100/469]\tTime 0.011 (0.010)\tData 0.004 (0.003)\tLoss 2.1988 (2.2305)\tPrec@1 14.062 (15.231)\tPrec@5 63.281 (59.638)\n",
      "TRAINING - Epoch: [4][110/469]\tTime 0.012 (0.010)\tData 0.003 (0.003)\tLoss 2.2787 (2.2269)\tPrec@1 9.375 (15.400)\tPrec@5 55.469 (59.825)\n",
      "TRAINING - Epoch: [4][120/469]\tTime 0.010 (0.010)\tData 0.003 (0.003)\tLoss 2.1847 (2.2248)\tPrec@1 9.375 (15.412)\tPrec@5 63.281 (59.892)\n",
      "TRAINING - Epoch: [4][130/469]\tTime 0.009 (0.010)\tData 0.004 (0.003)\tLoss 2.3438 (2.2242)\tPrec@1 17.188 (15.500)\tPrec@5 54.688 (59.751)\n",
      "TRAINING - Epoch: [4][140/469]\tTime 0.022 (0.010)\tData 0.005 (0.003)\tLoss 2.1830 (2.2238)\tPrec@1 19.531 (15.653)\tPrec@5 60.938 (59.835)\n",
      "TRAINING - Epoch: [4][150/469]\tTime 0.006 (0.010)\tData 0.000 (0.003)\tLoss 2.2521 (2.2232)\tPrec@1 8.594 (15.770)\tPrec@5 57.031 (59.887)\n",
      "TRAINING - Epoch: [4][160/469]\tTime 0.009 (0.010)\tData 0.003 (0.003)\tLoss 2.3302 (2.2242)\tPrec@1 9.375 (15.805)\tPrec@5 55.469 (59.860)\n",
      "TRAINING - Epoch: [4][170/469]\tTime 0.007 (0.010)\tData 0.002 (0.003)\tLoss 2.2156 (2.2252)\tPrec@1 14.844 (15.698)\tPrec@5 65.625 (59.896)\n",
      "TRAINING - Epoch: [4][180/469]\tTime 0.010 (0.010)\tData 0.004 (0.003)\tLoss 2.1300 (2.2232)\tPrec@1 25.781 (15.893)\tPrec@5 62.500 (59.889)\n",
      "TRAINING - Epoch: [4][190/469]\tTime 0.009 (0.010)\tData 0.004 (0.003)\tLoss 2.1699 (2.2249)\tPrec@1 17.188 (15.809)\tPrec@5 55.469 (59.678)\n",
      "TRAINING - Epoch: [4][200/469]\tTime 0.008 (0.010)\tData 0.002 (0.003)\tLoss 2.2004 (2.2250)\tPrec@1 18.750 (15.893)\tPrec@5 68.750 (59.639)\n",
      "TRAINING - Epoch: [4][210/469]\tTime 0.008 (0.010)\tData 0.002 (0.003)\tLoss 2.2473 (2.2244)\tPrec@1 16.406 (15.958)\tPrec@5 57.031 (59.645)\n",
      "TRAINING - Epoch: [4][220/469]\tTime 0.008 (0.010)\tData 0.002 (0.003)\tLoss 2.2149 (2.2245)\tPrec@1 9.375 (15.887)\tPrec@5 55.469 (59.743)\n",
      "TRAINING - Epoch: [4][230/469]\tTime 0.014 (0.010)\tData 0.002 (0.003)\tLoss 2.2859 (2.2248)\tPrec@1 14.062 (15.828)\tPrec@5 53.906 (59.801)\n",
      "TRAINING - Epoch: [4][240/469]\tTime 0.011 (0.010)\tData 0.003 (0.003)\tLoss 2.1950 (2.2237)\tPrec@1 25.781 (15.907)\tPrec@5 56.250 (59.822)\n",
      "TRAINING - Epoch: [4][250/469]\tTime 0.009 (0.010)\tData 0.002 (0.003)\tLoss 2.1797 (2.2228)\tPrec@1 24.219 (15.933)\tPrec@5 64.062 (59.873)\n",
      "TRAINING - Epoch: [4][260/469]\tTime 0.009 (0.010)\tData 0.001 (0.003)\tLoss 2.1774 (2.2219)\tPrec@1 12.500 (15.987)\tPrec@5 67.969 (60.013)\n",
      "TRAINING - Epoch: [4][270/469]\tTime 0.013 (0.010)\tData 0.002 (0.003)\tLoss 2.1914 (2.2221)\tPrec@1 16.406 (16.043)\tPrec@5 55.469 (60.015)\n",
      "TRAINING - Epoch: [4][280/469]\tTime 0.007 (0.010)\tData 0.002 (0.003)\tLoss 2.2572 (2.2234)\tPrec@1 14.062 (15.917)\tPrec@5 64.844 (59.995)\n",
      "TRAINING - Epoch: [4][290/469]\tTime 0.010 (0.010)\tData 0.002 (0.003)\tLoss 2.1946 (2.2231)\tPrec@1 22.656 (15.971)\tPrec@5 56.250 (60.011)\n",
      "TRAINING - Epoch: [4][300/469]\tTime 0.017 (0.010)\tData 0.006 (0.003)\tLoss 2.2818 (2.2243)\tPrec@1 12.500 (15.827)\tPrec@5 56.250 (60.117)\n",
      "TRAINING - Epoch: [4][310/469]\tTime 0.011 (0.010)\tData 0.001 (0.003)\tLoss 2.2164 (2.2241)\tPrec@1 15.625 (15.844)\tPrec@5 54.688 (60.131)\n",
      "TRAINING - Epoch: [4][320/469]\tTime 0.010 (0.010)\tData 0.002 (0.002)\tLoss 2.1082 (2.2230)\tPrec@1 6.250 (15.927)\tPrec@5 76.562 (60.144)\n",
      "TRAINING - Epoch: [4][330/469]\tTime 0.009 (0.010)\tData 0.002 (0.002)\tLoss 2.2203 (2.2227)\tPrec@1 14.844 (15.868)\tPrec@5 61.719 (60.260)\n",
      "TRAINING - Epoch: [4][340/469]\tTime 0.010 (0.010)\tData 0.001 (0.002)\tLoss 2.2217 (2.2227)\tPrec@1 16.406 (15.886)\tPrec@5 62.500 (60.188)\n",
      "TRAINING - Epoch: [4][350/469]\tTime 0.009 (0.010)\tData 0.002 (0.002)\tLoss 2.1911 (2.2237)\tPrec@1 17.969 (15.808)\tPrec@5 64.844 (60.132)\n",
      "TRAINING - Epoch: [4][360/469]\tTime 0.012 (0.010)\tData 0.003 (0.002)\tLoss 2.2200 (2.2230)\tPrec@1 18.750 (15.941)\tPrec@5 64.062 (60.096)\n",
      "TRAINING - Epoch: [4][370/469]\tTime 0.008 (0.010)\tData 0.002 (0.002)\tLoss 2.2518 (2.2232)\tPrec@1 14.844 (15.920)\tPrec@5 53.906 (60.064)\n",
      "TRAINING - Epoch: [4][380/469]\tTime 0.010 (0.010)\tData 0.004 (0.002)\tLoss 2.2112 (2.2230)\tPrec@1 8.594 (15.879)\tPrec@5 61.719 (60.007)\n",
      "TRAINING - Epoch: [4][390/469]\tTime 0.008 (0.010)\tData 0.002 (0.002)\tLoss 2.1875 (2.2238)\tPrec@1 19.531 (15.801)\tPrec@5 68.750 (59.956)\n",
      "TRAINING - Epoch: [4][400/469]\tTime 0.010 (0.010)\tData 0.002 (0.002)\tLoss 2.2695 (2.2241)\tPrec@1 8.594 (15.777)\tPrec@5 51.562 (59.954)\n",
      "TRAINING - Epoch: [4][410/469]\tTime 0.007 (0.010)\tData 0.002 (0.002)\tLoss 2.2462 (2.2241)\tPrec@1 11.719 (15.802)\tPrec@5 64.062 (59.949)\n",
      "TRAINING - Epoch: [4][420/469]\tTime 0.008 (0.010)\tData 0.002 (0.002)\tLoss 2.2202 (2.2241)\tPrec@1 17.188 (15.805)\tPrec@5 55.469 (59.924)\n",
      "TRAINING - Epoch: [4][430/469]\tTime 0.008 (0.010)\tData 0.002 (0.002)\tLoss 2.1976 (2.2245)\tPrec@1 16.406 (15.826)\tPrec@5 58.594 (59.930)\n",
      "TRAINING - Epoch: [4][440/469]\tTime 0.010 (0.010)\tData 0.003 (0.002)\tLoss 2.2039 (2.2249)\tPrec@1 24.219 (15.772)\tPrec@5 53.906 (59.894)\n",
      "TRAINING - Epoch: [4][450/469]\tTime 0.008 (0.010)\tData 0.002 (0.002)\tLoss 2.1411 (2.2240)\tPrec@1 22.656 (15.791)\tPrec@5 59.375 (59.898)\n",
      "TRAINING - Epoch: [4][460/469]\tTime 0.007 (0.010)\tData 0.002 (0.002)\tLoss 2.2926 (2.2261)\tPrec@1 7.812 (15.728)\tPrec@5 63.281 (59.785)\n",
      "EVALUATING - Epoch: [4][0/79]\tTime 0.061 (0.061)\tData 0.052 (0.052)\tLoss 2.2082 (2.2082)\tPrec@1 19.531 (19.531)\tPrec@5 50.000 (50.000)\n",
      "EVALUATING - Epoch: [4][10/79]\tTime 0.016 (0.013)\tData 0.010 (0.009)\tLoss 2.1923 (2.1939)\tPrec@1 19.531 (21.307)\tPrec@5 58.594 (58.807)\n",
      "EVALUATING - Epoch: [4][20/79]\tTime 0.009 (0.010)\tData 0.004 (0.006)\tLoss 2.2511 (2.2099)\tPrec@1 18.750 (20.275)\tPrec@5 57.031 (57.961)\n",
      "EVALUATING - Epoch: [4][30/79]\tTime 0.011 (0.010)\tData 0.008 (0.006)\tLoss 2.1928 (2.2109)\tPrec@1 17.188 (19.859)\tPrec@5 53.906 (57.812)\n",
      "EVALUATING - Epoch: [4][40/79]\tTime 0.012 (0.009)\tData 0.006 (0.005)\tLoss 2.2117 (2.2113)\tPrec@1 21.094 (19.855)\tPrec@5 57.031 (57.450)\n",
      "EVALUATING - Epoch: [4][50/79]\tTime 0.015 (0.009)\tData 0.010 (0.005)\tLoss 2.1810 (2.2100)\tPrec@1 22.656 (19.975)\tPrec@5 63.281 (57.782)\n",
      "EVALUATING - Epoch: [4][60/79]\tTime 0.005 (0.009)\tData 0.002 (0.005)\tLoss 2.2146 (2.2144)\tPrec@1 17.969 (19.800)\tPrec@5 59.375 (57.595)\n",
      "EVALUATING - Epoch: [4][70/79]\tTime 0.011 (0.009)\tData 0.009 (0.005)\tLoss 2.1428 (2.2116)\tPrec@1 24.219 (19.971)\tPrec@5 57.031 (57.669)\n",
      "\n",
      " Epoch: 5\tTraining Loss 2.2261 \tTraining Prec@1 15.767 \tTraining Prec@5 59.767 \tValidation Loss 2.2117 \tValidation Prec@1 20.000 \tValidation Prec@5 57.680 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "global best_prec1\n",
    "best_prec1 = 0\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.float().topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.contiguous().view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].contiguous().view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "    \n",
    "def save_checkpoint(state, is_best, path='.', filename='checkpoint.pth.tar', save_all=False):\n",
    "    filename = os.path.join(path, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, os.path.join(path, 'model_best.pth.tar'))\n",
    "    if save_all:\n",
    "        shutil.copyfile(filename, os.path.join(\n",
    "            path, 'checkpoint_epoch_%s.pth.tar' % state['epoch']))\n",
    "        \n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def forward(data_loader, model, criterion, epoch=0, training=True, optimizer=None):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (inputs, target) in enumerate(data_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        target = target.cuda()\n",
    "\n",
    "        if not training:\n",
    "            with torch.no_grad():\n",
    "                input_var = Variable(inputs.type(torch.cuda.FloatTensor))\n",
    "                target_var = Variable(target)\n",
    "                # compute output\n",
    "                output = model(input_var)\n",
    "        else:\n",
    "                input_var = Variable(inputs.type(torch.cuda.FloatTensor))\n",
    "                target_var = Variable(target)\n",
    "                # compute output\n",
    "                output = model(input_var)\n",
    "\n",
    "\n",
    "        loss = criterion(output, target_var)\n",
    "        if type(output) is list:\n",
    "            output = output[0]\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "        if training:\n",
    "            # compute gradient and do SGD step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for p in list(model.parameters()):\n",
    "                if hasattr(p,'org'):\n",
    "                    p.data.copy_(p.org)\n",
    "            optimizer.step()\n",
    "            for p in list(model.parameters()):\n",
    "                if hasattr(p,'org'):\n",
    "                    p.org.copy_(p.data.clamp_(-1,1))\n",
    "\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('{phase} - Epoch: [{0}][{1}/{2}]\\t'\n",
    "                         'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                         'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                         'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                         'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                         'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                             epoch, i, len(data_loader),\n",
    "                             phase='TRAINING' if training else 'EVALUATING',\n",
    "                             batch_time=batch_time,\n",
    "                             data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "\n",
    "    return losses.avg, top1.avg, top5.avg\n",
    "    \n",
    "def train(data_loader, model, criterion, epoch, optimizer):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    return forward(data_loader, model, criterion, epoch, training=True, optimizer=optimizer)\n",
    "\n",
    "\n",
    "def validate(data_loader, model, criterion, epoch):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    return forward(data_loader, model, criterion, epoch, training=False, optimizer=None)\n",
    "    \n",
    "for epoch in range(1, 5):\n",
    "\n",
    "        # train for one epoch\n",
    "        train_loss, train_prec1, train_prec5 = train(train_loader, model, criterion, epoch, optimizer)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        val_loss, val_prec1, val_prec5 = validate(test_loader, model, criterion, epoch)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = val_prec1 > best_prec1\n",
    "        best_prec1 = max(val_prec1, best_prec1)\n",
    "\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'model': 'BCNN',\n",
    "            'config': 'None',\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "        }, is_best, path='weight/')\n",
    "        print('\\n Epoch: {0}\\t'\n",
    "                     'Training Loss {train_loss:.4f} \\t'\n",
    "                     'Training Prec@1 {train_prec1:.3f} \\t'\n",
    "                     'Training Prec@5 {train_prec5:.3f} \\t'\n",
    "                     'Validation Loss {val_loss:.4f} \\t'\n",
    "                     'Validation Prec@1 {val_prec1:.3f} \\t'\n",
    "                     'Validation Prec@5 {val_prec5:.3f} \\n'\n",
    "                     .format(epoch + 1, train_loss=train_loss, val_loss=val_loss,\n",
    "                             train_prec1=train_prec1, val_prec1=val_prec1,\n",
    "                             train_prec5=train_prec5, val_prec5=val_prec5))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df51a72b-c52c-4bcf-8449-cd0017a739cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mnist_og = test_dataset[0]\n",
    "plt.imshow(mnist_og, cmap=cm.Greys_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0d062263-b449-46fb-941c-1f80abc9f526",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BinaryCNN' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m()\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1928\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1930\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BinaryCNN' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075178b4-684e-4162-a8b3-e67b8edfb958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
