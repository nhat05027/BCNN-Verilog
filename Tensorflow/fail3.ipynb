{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa03016-6d3b-4299-bb98-bd6d046803a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar  7 16:17:38 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.77.01              Driver Version: 566.36         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...    On  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   52C    P8              8W /   75W |     359MiB /   4096MiB |      3%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "327de96a-04f1-4e89-ae13-ce7d0eba90b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0415fc37-706a-4bde-a6eb-e160fbde23dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom autograd functions for binary operations\n",
    "class SignSTE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return torch.sign(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.clone()\n",
    "\n",
    "class BinaryWeightSTE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return torch.sign(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.clone()\n",
    "\n",
    "def Binarize(tensor,quant_mode='det'):\n",
    "    if quant_mode=='det':\n",
    "        return tensor.sign()\n",
    "    else:\n",
    "        return tensor.add_(1).div_(2).add_(torch.rand(tensor.size()).add(-0.5)).clamp_(0,1).round().mul_(2).add_(-1)\n",
    "# Binary convolutional layer\n",
    "class BinarizeLinear(nn.Linear):\n",
    "\n",
    "    def __init__(self, *kargs, **kwargs):\n",
    "        super(BinarizeLinear, self).__init__(*kargs, **kwargs)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        if input.size(1) != 784:\n",
    "            input.data=Binarize(input.data)\n",
    "        if not hasattr(self.weight,'org'):\n",
    "            self.weight.org=self.weight.data.clone()\n",
    "        self.weight.data=Binarize(self.weight.org)\n",
    "        out = F.linear(input, self.weight)\n",
    "        if not self.bias is None:\n",
    "            self.bias.org=self.bias.data.clone()\n",
    "            out += self.bias.view(1, -1).expand_as(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class BinaryConv2d(nn.Conv2d):\n",
    "\n",
    "    def __init__(self, *kargs, **kwargs):\n",
    "        super(BinaryConv2d, self).__init__(*kargs, **kwargs)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.size(1) != 3:\n",
    "            input.data = Binarize(input.data)\n",
    "        if not hasattr(self.weight,'org'):\n",
    "            self.weight.org=self.weight.data.clone()\n",
    "        self.weight.data=Binarize(self.weight.org)\n",
    "\n",
    "        out = F.conv2d(input, self.weight, None, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "        if not self.bias is None:\n",
    "            self.bias.org=self.bias.data.clone()\n",
    "            out += self.bias.view(1, -1, 1, 1).expand_as(out)\n",
    "        return out\n",
    "\n",
    "# Binary CNN architecture\n",
    "class BinaryCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryCNN, self).__init__()\n",
    "        self.ratioInfl=0.0625\n",
    "        self.features = nn.Sequential(\n",
    "            BinaryConv2d(1, int(64*self.ratioInfl), kernel_size=3, stride=1, padding=1),\n",
    "\n",
    "            nn.BatchNorm2d(int(64*self.ratioInfl)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            BinaryConv2d(int(64*self.ratioInfl), int(64*self.ratioInfl), kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(int(64*self.ratioInfl)),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            BinaryConv2d(int(64*self.ratioInfl), int(128*self.ratioInfl), kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(int(128*self.ratioInfl)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            BinaryConv2d(int(128*self.ratioInfl), int(128*self.ratioInfl), kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(int(128*self.ratioInfl)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            BinaryConv2d(int(128*self.ratioInfl), 16, kernel_size=3, padding=1),\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            BinarizeLinear(16 , 10),\n",
    "            #nn.ReLU(inplace=True),\n",
    "            #nn.Dropout(0.5),\n",
    "            #BinarizeLinear(16, num_classes),\n",
    "            #nn.BatchNorm1d(10),\n",
    "            nn.LogSoftmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        #x = x.view(-1, 256*3*3)\n",
    "        x = torch.squeeze(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5bbc12e7-c39f-49a2-851b-d63a06f9e10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5],std=[0.5])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "af62e5f5-189f-4b91-845c-4e2fbeb622e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n",
      "tensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3412,\n",
      "           0.4510,  0.2471,  0.1843, -0.5294, -0.7176, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.7412,\n",
      "           0.9922,  0.9922,  0.9922,  0.9922,  0.8902,  0.5529,  0.5529,\n",
      "           0.5529,  0.5529,  0.5529,  0.5529,  0.5529,  0.5529,  0.3333,\n",
      "          -0.5922, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4745,\n",
      "          -0.1059, -0.4353, -0.1059,  0.2784,  0.7804,  0.9922,  0.7647,\n",
      "           0.9922,  0.9922,  0.9922,  0.9608,  0.7961,  0.9922,  0.9922,\n",
      "           0.0980, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -0.8667, -0.4824, -0.8902,\n",
      "          -0.4745, -0.4745, -0.4745, -0.5373, -0.8353,  0.8510,  0.9922,\n",
      "          -0.1686, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -0.3490,  0.9843,  0.6392,\n",
      "          -0.8588, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -0.8275,  0.8275,  1.0000, -0.3490,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000,  0.0118,  0.9922,  0.8667, -0.6549,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -0.5373,  0.9529,  0.9922, -0.5137, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000,  0.0431,  0.9922,  0.4667, -0.9608, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -0.9294,  0.6078,  0.9451, -0.5451, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -0.0118,  0.9922,  0.4275, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -0.4118,  0.9686,  0.8824, -0.5529, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8510,\n",
      "           0.7333,  0.9922,  0.3020, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9765,  0.5922,\n",
      "           0.9922,  0.7176, -0.7255, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7020,  0.9922,\n",
      "           0.9922, -0.3961, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -0.7569,  0.7569,  0.9922,\n",
      "          -0.0980, -0.9922, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000,  0.0431,  0.9922,  0.9922,\n",
      "          -0.5922, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -0.5216,  0.8980,  0.9922,  0.9922,\n",
      "          -0.5922, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -0.0510,  0.9922,  0.9922,  0.7176,\n",
      "          -0.6863, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -0.0510,  0.9922,  0.6235, -0.8588,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(test_dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5afdda3-0ed6-40b1-8192-6f212909b00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "mnist_og = test_dataset[0][0]\n",
    "# plt.imshow(mnist_og, cmap=cm.Greys_r)\n",
    "# plt.show()\n",
    "plt.imshow(mnist_og, interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f8ba9c99-9bec-418b-af5e-c9305e00d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BinaryCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "10b4ea36-234c-4e90-abbd-2246196a908e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.198937\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.187845\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.267798\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.290514\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.231546\n",
      "\n",
      "Test set: Average loss: 2.3462, Accuracy: 1183/10000 (11.83%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.214943\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.239225\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.259748\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.169056\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.228522\n",
      "\n",
      "Test set: Average loss: 2.2099, Accuracy: 1650/10000 (16.50%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.193044\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 2.206994\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.323282\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 2.398580\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 2.282883\n",
      "\n",
      "Test set: Average loss: 2.6631, Accuracy: 763/10000 (7.63%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.198756\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 2.320337\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 2.307584\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 2.354470\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 2.250252\n",
      "\n",
      "Test set: Average loss: 2.2409, Accuracy: 1774/10000 (17.74%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.304982\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 2.255109\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 2.304946\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 2.348133\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 2.235723\n",
      "\n",
      "Test set: Average loss: 2.2410, Accuracy: 1932/10000 (19.32%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.321658\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 2.155562\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 2.247697\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 2.289620\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 2.221854\n",
      "\n",
      "Test set: Average loss: 2.6572, Accuracy: 974/10000 (9.74%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.224840\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 2.263509\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 2.213759\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 2.156017\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 2.229696\n",
      "\n",
      "Test set: Average loss: 2.6301, Accuracy: 1055/10000 (10.55%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.274154\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 2.226108\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 2.209928\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 2.243467\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 2.262484\n",
      "\n",
      "Test set: Average loss: 2.3022, Accuracy: 1297/10000 (12.97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.262383\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 2.248173\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 2.177047\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 2.280645\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 2.241801\n",
      "\n",
      "Test set: Average loss: 2.4272, Accuracy: 1155/10000 (11.55%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}'\n",
    "                  f' ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "# Testing loop\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() * data.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
    "# Run training and testing\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "98a8c21d-2c23-4364-886d-fd3e5911b4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING - Epoch: [1][0/469]\tTime 0.227 (0.227)\tData 0.201 (0.201)\tLoss 2.2093 (2.2093)\tPrec@1 21.875 (21.875)\tPrec@5 57.812 (57.812)\n",
      "TRAINING - Epoch: [1][100/469]\tTime 0.007 (0.013)\tData 0.000 (0.004)\tLoss 2.2623 (2.2522)\tPrec@1 14.062 (15.153)\tPrec@5 59.375 (58.617)\n",
      "TRAINING - Epoch: [1][200/469]\tTime 0.008 (0.011)\tData 0.001 (0.003)\tLoss 2.2777 (2.2507)\tPrec@1 14.062 (15.520)\tPrec@5 60.156 (58.244)\n",
      "TRAINING - Epoch: [1][300/469]\tTime 0.019 (0.011)\tData 0.005 (0.003)\tLoss 2.2412 (2.2477)\tPrec@1 21.094 (15.812)\tPrec@5 62.500 (58.493)\n",
      "TRAINING - Epoch: [1][400/469]\tTime 0.017 (0.011)\tData 0.007 (0.003)\tLoss 2.2119 (2.2478)\tPrec@1 14.844 (15.775)\tPrec@5 60.156 (58.598)\n",
      "EVALUATING - Epoch: [1][0/79]\tTime 0.059 (0.059)\tData 0.054 (0.054)\tLoss 2.1893 (2.1893)\tPrec@1 19.531 (19.531)\tPrec@5 53.125 (53.125)\n",
      "\n",
      " Epoch: 2\tTraining Loss 2.2453 \tTraining Prec@1 15.852 \tTraining Prec@5 58.802 \tValidation Loss 2.2059 \tValidation Prec@1 18.490 \tValidation Prec@5 58.130 \n",
      "\n",
      "TRAINING - Epoch: [2][0/469]\tTime 0.079 (0.079)\tData 0.061 (0.061)\tLoss 2.2689 (2.2689)\tPrec@1 16.406 (16.406)\tPrec@5 53.906 (53.906)\n",
      "TRAINING - Epoch: [2][100/469]\tTime 0.008 (0.011)\tData 0.001 (0.002)\tLoss 2.2578 (2.2492)\tPrec@1 16.406 (15.455)\tPrec@5 61.719 (59.236)\n",
      "TRAINING - Epoch: [2][200/469]\tTime 0.008 (0.010)\tData 0.002 (0.002)\tLoss 2.1841 (2.2469)\tPrec@1 22.656 (15.466)\tPrec@5 63.281 (59.900)\n",
      "TRAINING - Epoch: [2][300/469]\tTime 0.008 (0.010)\tData 0.002 (0.002)\tLoss 2.2059 (2.2459)\tPrec@1 19.531 (15.669)\tPrec@5 55.469 (59.668)\n",
      "TRAINING - Epoch: [2][400/469]\tTime 0.009 (0.009)\tData 0.002 (0.002)\tLoss 2.2529 (2.2462)\tPrec@1 17.188 (15.381)\tPrec@5 58.594 (59.626)\n",
      "EVALUATING - Epoch: [2][0/79]\tTime 0.052 (0.052)\tData 0.047 (0.047)\tLoss 2.9452 (2.9452)\tPrec@1 14.062 (14.062)\tPrec@5 49.219 (49.219)\n",
      "\n",
      " Epoch: 3\tTraining Loss 2.2438 \tTraining Prec@1 15.318 \tTraining Prec@5 59.608 \tValidation Loss 3.0518 \tValidation Prec@1 10.760 \tValidation Prec@5 49.770 \n",
      "\n",
      "TRAINING - Epoch: [3][0/469]\tTime 0.071 (0.071)\tData 0.059 (0.059)\tLoss 2.2541 (2.2541)\tPrec@1 9.375 (9.375)\tPrec@5 64.062 (64.062)\n",
      "TRAINING - Epoch: [3][100/469]\tTime 0.008 (0.010)\tData 0.002 (0.003)\tLoss 2.2512 (2.2287)\tPrec@1 16.406 (15.617)\tPrec@5 54.688 (60.907)\n",
      "TRAINING - Epoch: [3][200/469]\tTime 0.013 (0.010)\tData 0.001 (0.002)\tLoss 2.2754 (2.2323)\tPrec@1 17.188 (15.683)\tPrec@5 57.812 (61.419)\n",
      "TRAINING - Epoch: [3][300/469]\tTime 0.010 (0.010)\tData 0.002 (0.002)\tLoss 2.1733 (2.2410)\tPrec@1 8.594 (14.909)\tPrec@5 71.875 (61.072)\n",
      "TRAINING - Epoch: [3][400/469]\tTime 0.006 (0.010)\tData 0.000 (0.002)\tLoss 2.1809 (2.2418)\tPrec@1 16.406 (14.846)\tPrec@5 61.719 (61.197)\n",
      "EVALUATING - Epoch: [3][0/79]\tTime 0.051 (0.051)\tData 0.045 (0.045)\tLoss 3.3813 (3.3813)\tPrec@1 15.625 (15.625)\tPrec@5 42.969 (42.969)\n",
      "\n",
      " Epoch: 4\tTraining Loss 2.2420 \tTraining Prec@1 14.787 \tTraining Prec@5 61.198 \tValidation Loss 3.4798 \tValidation Prec@1 10.190 \tValidation Prec@5 49.640 \n",
      "\n",
      "TRAINING - Epoch: [4][0/469]\tTime 0.077 (0.077)\tData 0.063 (0.063)\tLoss 2.1819 (2.1819)\tPrec@1 10.938 (10.938)\tPrec@5 64.844 (64.844)\n",
      "TRAINING - Epoch: [4][100/469]\tTime 0.008 (0.010)\tData 0.002 (0.003)\tLoss 2.2144 (2.2409)\tPrec@1 19.531 (14.643)\tPrec@5 57.812 (60.558)\n",
      "TRAINING - Epoch: [4][200/469]\tTime 0.013 (0.010)\tData 0.001 (0.002)\tLoss 2.1431 (2.2338)\tPrec@1 21.875 (14.933)\tPrec@5 66.406 (61.206)\n",
      "TRAINING - Epoch: [4][300/469]\tTime 0.012 (0.010)\tData 0.004 (0.002)\tLoss 2.1577 (2.2335)\tPrec@1 8.594 (14.802)\tPrec@5 67.969 (61.153)\n",
      "TRAINING - Epoch: [4][400/469]\tTime 0.013 (0.010)\tData 0.002 (0.002)\tLoss 2.2052 (2.2350)\tPrec@1 12.500 (14.774)\tPrec@5 71.094 (61.216)\n",
      "EVALUATING - Epoch: [4][0/79]\tTime 0.051 (0.051)\tData 0.046 (0.046)\tLoss 2.3183 (2.3183)\tPrec@1 9.375 (9.375)\tPrec@5 39.062 (39.062)\n",
      "\n",
      " Epoch: 5\tTraining Loss 2.2331 \tTraining Prec@1 14.857 \tTraining Prec@5 61.402 \tValidation Loss 2.2633 \tValidation Prec@1 10.110 \tValidation Prec@5 55.350 \n",
      "\n",
      "TRAINING - Epoch: [5][0/469]\tTime 0.072 (0.072)\tData 0.060 (0.060)\tLoss 2.2364 (2.2364)\tPrec@1 10.156 (10.156)\tPrec@5 64.844 (64.844)\n",
      "TRAINING - Epoch: [5][100/469]\tTime 0.012 (0.011)\tData 0.001 (0.002)\tLoss 2.1930 (2.2323)\tPrec@1 21.094 (14.782)\tPrec@5 64.844 (62.020)\n",
      "TRAINING - Epoch: [5][200/469]\tTime 0.008 (0.011)\tData 0.003 (0.002)\tLoss 2.1383 (2.2366)\tPrec@1 27.344 (14.708)\tPrec@5 72.656 (61.489)\n",
      "TRAINING - Epoch: [5][300/469]\tTime 0.013 (0.010)\tData 0.005 (0.002)\tLoss 2.3057 (2.2321)\tPrec@1 14.844 (14.935)\tPrec@5 62.500 (61.392)\n",
      "TRAINING - Epoch: [5][400/469]\tTime 0.006 (0.010)\tData 0.000 (0.002)\tLoss 2.2562 (2.2315)\tPrec@1 13.281 (15.052)\tPrec@5 62.500 (61.195)\n",
      "EVALUATING - Epoch: [5][0/79]\tTime 0.056 (0.056)\tData 0.051 (0.051)\tLoss 3.4410 (3.4410)\tPrec@1 10.156 (10.156)\tPrec@5 46.875 (46.875)\n",
      "\n",
      " Epoch: 6\tTraining Loss 2.2310 \tTraining Prec@1 15.038 \tTraining Prec@5 61.125 \tValidation Loss 3.4489 \tValidation Prec@1 8.430 \tValidation Prec@5 48.410 \n",
      "\n",
      "TRAINING - Epoch: [6][0/469]\tTime 0.078 (0.078)\tData 0.062 (0.062)\tLoss 2.1950 (2.1950)\tPrec@1 10.938 (10.938)\tPrec@5 57.031 (57.031)\n",
      "TRAINING - Epoch: [6][100/469]\tTime 0.007 (0.010)\tData 0.002 (0.003)\tLoss 2.2074 (2.2269)\tPrec@1 13.281 (14.782)\tPrec@5 64.844 (61.270)\n",
      "TRAINING - Epoch: [6][200/469]\tTime 0.014 (0.010)\tData 0.006 (0.004)\tLoss 2.2014 (2.2281)\tPrec@1 19.531 (14.921)\tPrec@5 57.812 (61.035)\n",
      "TRAINING - Epoch: [6][300/469]\tTime 0.018 (0.011)\tData 0.012 (0.004)\tLoss 2.3166 (2.2284)\tPrec@1 8.594 (14.979)\tPrec@5 62.500 (60.805)\n",
      "TRAINING - Epoch: [6][400/469]\tTime 0.016 (0.011)\tData 0.010 (0.005)\tLoss 2.1530 (2.2273)\tPrec@1 26.562 (15.089)\tPrec@5 57.812 (60.805)\n",
      "EVALUATING - Epoch: [6][0/79]\tTime 0.051 (0.051)\tData 0.045 (0.045)\tLoss 2.7632 (2.7632)\tPrec@1 14.062 (14.062)\tPrec@5 48.438 (48.438)\n",
      "\n",
      " Epoch: 7\tTraining Loss 2.2276 \tTraining Prec@1 15.045 \tTraining Prec@5 60.790 \tValidation Loss 2.8429 \tValidation Prec@1 9.950 \tValidation Prec@5 52.570 \n",
      "\n",
      "TRAINING - Epoch: [7][0/469]\tTime 0.071 (0.071)\tData 0.059 (0.059)\tLoss 2.1459 (2.1459)\tPrec@1 23.438 (23.438)\tPrec@5 60.938 (60.938)\n",
      "TRAINING - Epoch: [7][100/469]\tTime 0.008 (0.010)\tData 0.002 (0.003)\tLoss 2.2710 (2.2181)\tPrec@1 9.375 (15.633)\tPrec@5 56.250 (61.007)\n",
      "TRAINING - Epoch: [7][200/469]\tTime 0.011 (0.009)\tData 0.003 (0.003)\tLoss 2.2260 (2.2186)\tPrec@1 21.875 (15.683)\tPrec@5 54.688 (60.747)\n",
      "TRAINING - Epoch: [7][300/469]\tTime 0.010 (0.009)\tData 0.004 (0.003)\tLoss 2.1679 (2.2181)\tPrec@1 21.094 (15.711)\tPrec@5 60.938 (60.437)\n",
      "TRAINING - Epoch: [7][400/469]\tTime 0.008 (0.009)\tData 0.002 (0.003)\tLoss 2.2316 (2.2188)\tPrec@1 14.844 (15.911)\tPrec@5 60.938 (60.314)\n",
      "EVALUATING - Epoch: [7][0/79]\tTime 0.054 (0.054)\tData 0.049 (0.049)\tLoss 2.3629 (2.3629)\tPrec@1 11.719 (11.719)\tPrec@5 44.531 (44.531)\n",
      "\n",
      " Epoch: 8\tTraining Loss 2.2199 \tTraining Prec@1 15.937 \tTraining Prec@5 60.177 \tValidation Loss 2.3386 \tValidation Prec@1 11.590 \tValidation Prec@5 50.830 \n",
      "\n",
      "TRAINING - Epoch: [8][0/469]\tTime 0.073 (0.073)\tData 0.061 (0.061)\tLoss 2.2064 (2.2064)\tPrec@1 19.531 (19.531)\tPrec@5 53.906 (53.906)\n",
      "TRAINING - Epoch: [8][100/469]\tTime 0.014 (0.013)\tData 0.001 (0.002)\tLoss 2.2684 (2.2198)\tPrec@1 14.844 (16.662)\tPrec@5 59.375 (60.357)\n",
      "TRAINING - Epoch: [8][200/469]\tTime 0.011 (0.013)\tData 0.002 (0.002)\tLoss 2.2386 (2.2174)\tPrec@1 16.406 (16.126)\tPrec@5 57.812 (60.075)\n",
      "TRAINING - Epoch: [8][300/469]\tTime 0.008 (0.012)\tData 0.000 (0.002)\tLoss 2.2311 (2.2184)\tPrec@1 19.531 (16.147)\tPrec@5 54.688 (59.746)\n",
      "TRAINING - Epoch: [8][400/469]\tTime 0.014 (0.012)\tData 0.001 (0.002)\tLoss 2.2156 (2.2204)\tPrec@1 20.312 (16.124)\tPrec@5 56.250 (59.601)\n",
      "EVALUATING - Epoch: [8][0/79]\tTime 0.052 (0.052)\tData 0.046 (0.046)\tLoss 2.2554 (2.2554)\tPrec@1 9.375 (9.375)\tPrec@5 50.000 (50.000)\n",
      "\n",
      " Epoch: 9\tTraining Loss 2.2207 \tTraining Prec@1 16.088 \tTraining Prec@5 59.610 \tValidation Loss 2.1984 \tValidation Prec@1 10.130 \tValidation Prec@5 59.910 \n",
      "\n",
      "TRAINING - Epoch: [9][0/469]\tTime 0.071 (0.071)\tData 0.060 (0.060)\tLoss 2.2944 (2.2944)\tPrec@1 9.375 (9.375)\tPrec@5 55.469 (55.469)\n",
      "TRAINING - Epoch: [9][100/469]\tTime 0.011 (0.010)\tData 0.006 (0.004)\tLoss 2.2173 (2.2225)\tPrec@1 13.281 (16.429)\tPrec@5 60.938 (59.522)\n",
      "TRAINING - Epoch: [9][200/469]\tTime 0.012 (0.010)\tData 0.006 (0.004)\tLoss 2.2512 (2.2222)\tPrec@1 15.625 (16.329)\tPrec@5 50.000 (59.375)\n",
      "TRAINING - Epoch: [9][300/469]\tTime 0.011 (0.010)\tData 0.004 (0.004)\tLoss 2.1100 (2.2234)\tPrec@1 26.562 (16.162)\tPrec@5 64.062 (59.261)\n",
      "TRAINING - Epoch: [9][400/469]\tTime 0.010 (0.010)\tData 0.004 (0.004)\tLoss 2.2364 (2.2212)\tPrec@1 14.062 (16.176)\tPrec@5 62.500 (59.445)\n",
      "EVALUATING - Epoch: [9][0/79]\tTime 0.052 (0.052)\tData 0.046 (0.046)\tLoss 2.2211 (2.2211)\tPrec@1 21.094 (21.094)\tPrec@5 50.781 (50.781)\n",
      "\n",
      " Epoch: 10\tTraining Loss 2.2210 \tTraining Prec@1 16.160 \tTraining Prec@5 59.532 \tValidation Loss 2.2715 \tValidation Prec@1 18.950 \tValidation Prec@5 54.950 \n",
      "\n",
      "TRAINING - Epoch: [10][0/469]\tTime 0.073 (0.073)\tData 0.056 (0.056)\tLoss 2.2326 (2.2326)\tPrec@1 18.750 (18.750)\tPrec@5 61.719 (61.719)\n",
      "TRAINING - Epoch: [10][100/469]\tTime 0.014 (0.013)\tData 0.003 (0.002)\tLoss 2.1930 (2.2170)\tPrec@1 19.531 (16.530)\tPrec@5 57.812 (59.824)\n",
      "TRAINING - Epoch: [10][200/469]\tTime 0.014 (0.012)\tData 0.003 (0.002)\tLoss 2.2518 (2.2151)\tPrec@1 15.625 (16.601)\tPrec@5 53.125 (60.028)\n",
      "TRAINING - Epoch: [10][300/469]\tTime 0.014 (0.012)\tData 0.001 (0.002)\tLoss 2.2366 (2.2206)\tPrec@1 13.281 (16.064)\tPrec@5 57.812 (60.206)\n",
      "TRAINING - Epoch: [10][400/469]\tTime 0.009 (0.012)\tData 0.002 (0.002)\tLoss 2.2243 (2.2233)\tPrec@1 15.625 (15.888)\tPrec@5 60.156 (60.045)\n",
      "EVALUATING - Epoch: [10][0/79]\tTime 0.053 (0.053)\tData 0.046 (0.046)\tLoss 2.2304 (2.2304)\tPrec@1 17.969 (17.969)\tPrec@5 51.562 (51.562)\n",
      "\n",
      " Epoch: 11\tTraining Loss 2.2231 \tTraining Prec@1 15.860 \tTraining Prec@5 60.050 \tValidation Loss 2.2211 \tValidation Prec@1 18.770 \tValidation Prec@5 57.750 \n",
      "\n",
      "TRAINING - Epoch: [11][0/469]\tTime 0.079 (0.079)\tData 0.065 (0.065)\tLoss 2.1713 (2.1713)\tPrec@1 17.969 (17.969)\tPrec@5 53.906 (53.906)\n",
      "TRAINING - Epoch: [11][100/469]\tTime 0.008 (0.010)\tData 0.002 (0.002)\tLoss 2.2439 (2.2176)\tPrec@1 16.406 (16.112)\tPrec@5 53.906 (60.752)\n",
      "TRAINING - Epoch: [11][200/469]\tTime 0.011 (0.010)\tData 0.002 (0.002)\tLoss 2.2126 (2.2215)\tPrec@1 17.969 (15.788)\tPrec@5 60.938 (60.627)\n",
      "TRAINING - Epoch: [11][300/469]\tTime 0.012 (0.010)\tData 0.003 (0.002)\tLoss 2.3124 (2.2217)\tPrec@1 10.938 (15.869)\tPrec@5 56.250 (60.463)\n",
      "TRAINING - Epoch: [11][400/469]\tTime 0.008 (0.011)\tData 0.002 (0.002)\tLoss 2.1922 (2.2220)\tPrec@1 18.750 (15.900)\tPrec@5 53.906 (60.337)\n",
      "EVALUATING - Epoch: [11][0/79]\tTime 0.066 (0.066)\tData 0.060 (0.060)\tLoss 2.1794 (2.1794)\tPrec@1 12.500 (12.500)\tPrec@5 55.469 (55.469)\n",
      "\n",
      " Epoch: 12\tTraining Loss 2.2218 \tTraining Prec@1 15.910 \tTraining Prec@5 60.387 \tValidation Loss 2.1978 \tValidation Prec@1 11.010 \tValidation Prec@5 59.760 \n",
      "\n",
      "TRAINING - Epoch: [12][0/469]\tTime 0.092 (0.092)\tData 0.076 (0.076)\tLoss 2.2181 (2.2181)\tPrec@1 14.062 (14.062)\tPrec@5 59.375 (59.375)\n",
      "TRAINING - Epoch: [12][100/469]\tTime 0.019 (0.013)\tData 0.013 (0.006)\tLoss 2.1989 (2.2286)\tPrec@1 18.750 (15.022)\tPrec@5 54.688 (60.729)\n",
      "TRAINING - Epoch: [12][200/469]\tTime 0.024 (0.013)\tData 0.017 (0.005)\tLoss 2.1599 (2.2257)\tPrec@1 19.531 (15.225)\tPrec@5 71.094 (60.875)\n",
      "TRAINING - Epoch: [12][300/469]\tTime 0.019 (0.013)\tData 0.013 (0.005)\tLoss 2.2930 (2.2240)\tPrec@1 10.938 (15.425)\tPrec@5 63.281 (60.958)\n",
      "TRAINING - Epoch: [12][400/469]\tTime 0.018 (0.013)\tData 0.011 (0.005)\tLoss 2.1898 (2.2239)\tPrec@1 17.188 (15.594)\tPrec@5 59.375 (60.669)\n",
      "EVALUATING - Epoch: [12][0/79]\tTime 0.052 (0.052)\tData 0.047 (0.047)\tLoss 3.9054 (3.9054)\tPrec@1 15.625 (15.625)\tPrec@5 42.969 (42.969)\n",
      "\n",
      " Epoch: 13\tTraining Loss 2.2229 \tTraining Prec@1 15.653 \tTraining Prec@5 60.655 \tValidation Loss 4.0149 \tValidation Prec@1 10.710 \tValidation Prec@5 49.080 \n",
      "\n",
      "TRAINING - Epoch: [13][0/469]\tTime 0.082 (0.082)\tData 0.065 (0.065)\tLoss 2.2953 (2.2953)\tPrec@1 12.500 (12.500)\tPrec@5 58.594 (58.594)\n",
      "TRAINING - Epoch: [13][100/469]\tTime 0.014 (0.012)\tData 0.001 (0.003)\tLoss 2.2123 (2.2263)\tPrec@1 19.531 (15.780)\tPrec@5 57.031 (60.079)\n",
      "TRAINING - Epoch: [13][200/469]\tTime 0.011 (0.012)\tData 0.001 (0.002)\tLoss 2.2141 (2.2239)\tPrec@1 17.969 (15.745)\tPrec@5 53.906 (59.647)\n",
      "TRAINING - Epoch: [13][300/469]\tTime 0.008 (0.012)\tData 0.000 (0.002)\tLoss 2.2428 (2.2293)\tPrec@1 19.531 (15.508)\tPrec@5 64.062 (59.596)\n",
      "TRAINING - Epoch: [13][400/469]\tTime 0.011 (0.012)\tData 0.001 (0.002)\tLoss 2.2885 (2.2270)\tPrec@1 7.031 (15.602)\tPrec@5 53.125 (59.864)\n",
      "EVALUATING - Epoch: [13][0/79]\tTime 0.053 (0.053)\tData 0.047 (0.047)\tLoss 2.3450 (2.3450)\tPrec@1 9.375 (9.375)\tPrec@5 45.312 (45.312)\n",
      "\n",
      " Epoch: 14\tTraining Loss 2.2267 \tTraining Prec@1 15.565 \tTraining Prec@5 59.903 \tValidation Loss 2.3137 \tValidation Prec@1 10.270 \tValidation Prec@5 52.760 \n",
      "\n",
      "TRAINING - Epoch: [14][0/469]\tTime 0.065 (0.065)\tData 0.054 (0.054)\tLoss 2.2417 (2.2417)\tPrec@1 9.375 (9.375)\tPrec@5 57.812 (57.812)\n",
      "TRAINING - Epoch: [14][100/469]\tTime 0.015 (0.012)\tData 0.008 (0.004)\tLoss 2.2264 (2.2235)\tPrec@1 18.750 (15.254)\tPrec@5 60.938 (59.855)\n",
      "TRAINING - Epoch: [14][200/469]\tTime 0.014 (0.012)\tData 0.004 (0.004)\tLoss 2.1613 (2.2280)\tPrec@1 20.312 (15.337)\tPrec@5 53.906 (59.705)\n",
      "TRAINING - Epoch: [14][300/469]\tTime 0.017 (0.012)\tData 0.007 (0.004)\tLoss 2.1647 (2.2306)\tPrec@1 12.500 (15.269)\tPrec@5 64.062 (59.847)\n",
      "TRAINING - Epoch: [14][400/469]\tTime 0.013 (0.012)\tData 0.007 (0.004)\tLoss 2.3003 (2.2291)\tPrec@1 7.031 (15.311)\tPrec@5 57.812 (60.061)\n",
      "EVALUATING - Epoch: [14][0/79]\tTime 0.056 (0.056)\tData 0.050 (0.050)\tLoss 2.1971 (2.1971)\tPrec@1 21.094 (21.094)\tPrec@5 50.000 (50.000)\n",
      "\n",
      " Epoch: 15\tTraining Loss 2.2288 \tTraining Prec@1 15.255 \tTraining Prec@5 60.028 \tValidation Loss 2.1856 \tValidation Prec@1 20.870 \tValidation Prec@5 58.190 \n",
      "\n",
      "TRAINING - Epoch: [15][0/469]\tTime 0.078 (0.078)\tData 0.062 (0.062)\tLoss 2.2224 (2.2224)\tPrec@1 18.750 (18.750)\tPrec@5 58.594 (58.594)\n",
      "TRAINING - Epoch: [15][100/469]\tTime 0.012 (0.012)\tData 0.002 (0.003)\tLoss 2.1847 (2.2205)\tPrec@1 10.938 (14.991)\tPrec@5 61.719 (60.705)\n",
      "TRAINING - Epoch: [15][200/469]\tTime 0.011 (0.012)\tData 0.005 (0.003)\tLoss 2.2802 (2.2258)\tPrec@1 10.156 (15.135)\tPrec@5 63.281 (60.261)\n",
      "TRAINING - Epoch: [15][300/469]\tTime 0.013 (0.011)\tData 0.003 (0.003)\tLoss 2.2667 (2.2264)\tPrec@1 13.281 (15.417)\tPrec@5 59.375 (60.172)\n",
      "TRAINING - Epoch: [15][400/469]\tTime 0.011 (0.011)\tData 0.002 (0.003)\tLoss 2.2665 (2.2271)\tPrec@1 13.281 (15.483)\tPrec@5 58.594 (60.000)\n",
      "EVALUATING - Epoch: [15][0/79]\tTime 0.053 (0.053)\tData 0.048 (0.048)\tLoss 2.2616 (2.2616)\tPrec@1 10.156 (10.156)\tPrec@5 50.000 (50.000)\n",
      "\n",
      " Epoch: 16\tTraining Loss 2.2271 \tTraining Prec@1 15.463 \tTraining Prec@5 59.997 \tValidation Loss 2.2065 \tValidation Prec@1 10.420 \tValidation Prec@5 59.460 \n",
      "\n",
      "TRAINING - Epoch: [16][0/469]\tTime 0.066 (0.066)\tData 0.054 (0.054)\tLoss 2.2927 (2.2927)\tPrec@1 15.625 (15.625)\tPrec@5 50.781 (50.781)\n",
      "TRAINING - Epoch: [16][100/469]\tTime 0.017 (0.012)\tData 0.006 (0.005)\tLoss 2.1327 (2.2334)\tPrec@1 18.750 (15.138)\tPrec@5 67.188 (59.460)\n",
      "TRAINING - Epoch: [16][200/469]\tTime 0.019 (0.012)\tData 0.012 (0.005)\tLoss 2.2736 (2.2290)\tPrec@1 20.312 (15.543)\tPrec@5 57.812 (59.760)\n",
      "TRAINING - Epoch: [16][300/469]\tTime 0.009 (0.013)\tData 0.003 (0.005)\tLoss 2.2305 (2.2286)\tPrec@1 17.188 (15.542)\tPrec@5 61.719 (59.718)\n",
      "TRAINING - Epoch: [16][400/469]\tTime 0.015 (0.013)\tData 0.005 (0.005)\tLoss 2.2729 (2.2275)\tPrec@1 12.500 (15.510)\tPrec@5 66.406 (59.804)\n",
      "EVALUATING - Epoch: [16][0/79]\tTime 0.052 (0.052)\tData 0.046 (0.046)\tLoss 3.7089 (3.7089)\tPrec@1 15.625 (15.625)\tPrec@5 50.000 (50.000)\n",
      "\n",
      " Epoch: 17\tTraining Loss 2.2273 \tTraining Prec@1 15.557 \tTraining Prec@5 59.923 \tValidation Loss 3.9288 \tValidation Prec@1 11.390 \tValidation Prec@5 48.650 \n",
      "\n",
      "TRAINING - Epoch: [17][0/469]\tTime 0.071 (0.071)\tData 0.059 (0.059)\tLoss 2.2388 (2.2388)\tPrec@1 19.531 (19.531)\tPrec@5 60.156 (60.156)\n",
      "TRAINING - Epoch: [17][100/469]\tTime 0.015 (0.011)\tData 0.007 (0.004)\tLoss 2.1880 (2.2265)\tPrec@1 18.750 (15.494)\tPrec@5 60.938 (60.187)\n",
      "TRAINING - Epoch: [17][200/469]\tTime 0.011 (0.010)\tData 0.003 (0.004)\tLoss 2.3024 (2.2264)\tPrec@1 14.062 (15.714)\tPrec@5 50.000 (59.985)\n",
      "TRAINING - Epoch: [17][300/469]\tTime 0.014 (0.011)\tData 0.008 (0.004)\tLoss 2.2608 (2.2266)\tPrec@1 9.375 (15.654)\tPrec@5 55.469 (60.252)\n",
      "TRAINING - Epoch: [17][400/469]\tTime 0.014 (0.011)\tData 0.008 (0.004)\tLoss 2.2323 (2.2263)\tPrec@1 11.719 (15.417)\tPrec@5 60.938 (60.298)\n",
      "EVALUATING - Epoch: [17][0/79]\tTime 0.052 (0.052)\tData 0.046 (0.046)\tLoss 2.2660 (2.2660)\tPrec@1 20.312 (20.312)\tPrec@5 46.875 (46.875)\n",
      "\n",
      " Epoch: 18\tTraining Loss 2.2283 \tTraining Prec@1 15.270 \tTraining Prec@5 60.393 \tValidation Loss 2.2397 \tValidation Prec@1 20.270 \tValidation Prec@5 55.730 \n",
      "\n",
      "TRAINING - Epoch: [18][0/469]\tTime 0.067 (0.067)\tData 0.054 (0.054)\tLoss 2.1615 (2.1615)\tPrec@1 21.094 (21.094)\tPrec@5 59.375 (59.375)\n",
      "TRAINING - Epoch: [18][100/469]\tTime 0.019 (0.013)\tData 0.011 (0.005)\tLoss 2.3189 (2.2183)\tPrec@1 10.938 (15.679)\tPrec@5 52.344 (59.785)\n",
      "TRAINING - Epoch: [18][200/469]\tTime 0.012 (0.012)\tData 0.005 (0.005)\tLoss 2.1975 (2.2224)\tPrec@1 17.188 (15.578)\tPrec@5 61.719 (60.071)\n",
      "TRAINING - Epoch: [18][300/469]\tTime 0.025 (0.012)\tData 0.012 (0.004)\tLoss 2.2341 (2.2246)\tPrec@1 12.500 (15.630)\tPrec@5 63.281 (59.977)\n",
      "TRAINING - Epoch: [18][400/469]\tTime 0.015 (0.013)\tData 0.006 (0.005)\tLoss 2.2433 (2.2235)\tPrec@1 10.156 (15.785)\tPrec@5 62.500 (60.022)\n",
      "EVALUATING - Epoch: [18][0/79]\tTime 0.055 (0.055)\tData 0.050 (0.050)\tLoss 3.6283 (3.6283)\tPrec@1 10.156 (10.156)\tPrec@5 52.344 (52.344)\n",
      "\n",
      " Epoch: 19\tTraining Loss 2.2248 \tTraining Prec@1 15.768 \tTraining Prec@5 60.083 \tValidation Loss 3.9387 \tValidation Prec@1 10.480 \tValidation Prec@5 50.550 \n",
      "\n",
      "TRAINING - Epoch: [19][0/469]\tTime 0.069 (0.069)\tData 0.056 (0.056)\tLoss 2.1645 (2.1645)\tPrec@1 18.750 (18.750)\tPrec@5 62.500 (62.500)\n",
      "TRAINING - Epoch: [19][100/469]\tTime 0.006 (0.012)\tData 0.000 (0.006)\tLoss 2.2063 (2.2303)\tPrec@1 11.719 (15.176)\tPrec@5 58.594 (60.365)\n",
      "TRAINING - Epoch: [19][200/469]\tTime 0.007 (0.012)\tData 0.000 (0.005)\tLoss 2.2118 (2.2343)\tPrec@1 10.156 (14.964)\tPrec@5 60.938 (59.709)\n",
      "TRAINING - Epoch: [19][300/469]\tTime 0.005 (0.012)\tData 0.000 (0.005)\tLoss 2.2382 (2.2305)\tPrec@1 8.594 (15.412)\tPrec@5 58.594 (59.746)\n",
      "TRAINING - Epoch: [19][400/469]\tTime 0.007 (0.012)\tData 0.001 (0.005)\tLoss 2.2876 (2.2288)\tPrec@1 15.625 (15.321)\tPrec@5 53.906 (60.092)\n",
      "EVALUATING - Epoch: [19][0/79]\tTime 0.058 (0.058)\tData 0.046 (0.046)\tLoss 2.3619 (2.3619)\tPrec@1 16.406 (16.406)\tPrec@5 38.281 (38.281)\n",
      "\n",
      " Epoch: 20\tTraining Loss 2.2276 \tTraining Prec@1 15.360 \tTraining Prec@5 60.138 \tValidation Loss 2.3211 \tValidation Prec@1 17.170 \tValidation Prec@5 50.820 \n",
      "\n",
      "TRAINING - Epoch: [20][0/469]\tTime 0.068 (0.068)\tData 0.055 (0.055)\tLoss 2.1895 (2.1895)\tPrec@1 22.656 (22.656)\tPrec@5 64.844 (64.844)\n",
      "TRAINING - Epoch: [20][100/469]\tTime 0.010 (0.011)\tData 0.002 (0.003)\tLoss 2.1933 (2.2261)\tPrec@1 23.438 (15.200)\tPrec@5 60.938 (59.986)\n",
      "TRAINING - Epoch: [20][200/469]\tTime 0.021 (0.013)\tData 0.008 (0.003)\tLoss 2.2130 (2.2279)\tPrec@1 15.625 (15.508)\tPrec@5 58.594 (60.113)\n",
      "TRAINING - Epoch: [20][300/469]\tTime 0.008 (0.012)\tData 0.002 (0.003)\tLoss 2.2638 (2.2276)\tPrec@1 16.406 (15.602)\tPrec@5 57.812 (60.208)\n",
      "TRAINING - Epoch: [20][400/469]\tTime 0.013 (0.012)\tData 0.002 (0.003)\tLoss 2.2201 (2.2282)\tPrec@1 15.625 (15.522)\tPrec@5 57.031 (60.269)\n",
      "EVALUATING - Epoch: [20][0/79]\tTime 0.055 (0.055)\tData 0.049 (0.049)\tLoss 2.3311 (2.3311)\tPrec@1 10.938 (10.938)\tPrec@5 44.531 (44.531)\n",
      "\n",
      " Epoch: 21\tTraining Loss 2.2291 \tTraining Prec@1 15.378 \tTraining Prec@5 60.165 \tValidation Loss 2.2981 \tValidation Prec@1 12.630 \tValidation Prec@5 52.180 \n",
      "\n",
      "TRAINING - Epoch: [21][0/469]\tTime 0.072 (0.072)\tData 0.054 (0.054)\tLoss 2.1856 (2.1856)\tPrec@1 21.094 (21.094)\tPrec@5 69.531 (69.531)\n",
      "TRAINING - Epoch: [21][100/469]\tTime 0.015 (0.011)\tData 0.002 (0.002)\tLoss 2.2024 (2.2209)\tPrec@1 25.000 (15.316)\tPrec@5 64.062 (60.543)\n",
      "TRAINING - Epoch: [21][200/469]\tTime 0.010 (0.011)\tData 0.004 (0.002)\tLoss 2.2917 (2.2295)\tPrec@1 14.844 (15.139)\tPrec@5 57.031 (60.040)\n",
      "TRAINING - Epoch: [21][300/469]\tTime 0.010 (0.011)\tData 0.003 (0.003)\tLoss 2.2652 (2.2268)\tPrec@1 12.500 (15.539)\tPrec@5 57.812 (60.026)\n",
      "TRAINING - Epoch: [21][400/469]\tTime 0.012 (0.011)\tData 0.003 (0.003)\tLoss 2.1663 (2.2272)\tPrec@1 8.594 (15.520)\tPrec@5 64.062 (60.133)\n",
      "EVALUATING - Epoch: [21][0/79]\tTime 0.140 (0.140)\tData 0.125 (0.125)\tLoss 3.7325 (3.7325)\tPrec@1 16.406 (16.406)\tPrec@5 50.000 (50.000)\n",
      "\n",
      " Epoch: 22\tTraining Loss 2.2269 \tTraining Prec@1 15.475 \tTraining Prec@5 60.057 \tValidation Loss 4.0578 \tValidation Prec@1 9.610 \tValidation Prec@5 50.000 \n",
      "\n",
      "TRAINING - Epoch: [22][0/469]\tTime 0.092 (0.092)\tData 0.071 (0.071)\tLoss 2.3613 (2.3613)\tPrec@1 13.281 (13.281)\tPrec@5 56.250 (56.250)\n",
      "TRAINING - Epoch: [22][100/469]\tTime 0.010 (0.015)\tData 0.002 (0.003)\tLoss 2.2503 (2.2264)\tPrec@1 15.625 (16.012)\tPrec@5 65.625 (60.381)\n",
      "TRAINING - Epoch: [22][200/469]\tTime 0.013 (0.014)\tData 0.003 (0.003)\tLoss 2.2225 (2.2314)\tPrec@1 10.938 (15.683)\tPrec@5 64.844 (60.199)\n",
      "TRAINING - Epoch: [22][300/469]\tTime 0.011 (0.014)\tData 0.000 (0.003)\tLoss 2.2284 (2.2309)\tPrec@1 14.062 (15.581)\tPrec@5 63.281 (60.135)\n",
      "TRAINING - Epoch: [22][400/469]\tTime 0.013 (0.014)\tData 0.000 (0.003)\tLoss 2.2725 (2.2332)\tPrec@1 7.812 (15.255)\tPrec@5 59.375 (60.022)\n",
      "EVALUATING - Epoch: [22][0/79]\tTime 0.054 (0.054)\tData 0.048 (0.048)\tLoss 3.7929 (3.7929)\tPrec@1 14.062 (14.062)\tPrec@5 47.656 (47.656)\n",
      "\n",
      " Epoch: 23\tTraining Loss 2.2333 \tTraining Prec@1 15.193 \tTraining Prec@5 59.983 \tValidation Loss 3.9680 \tValidation Prec@1 10.690 \tValidation Prec@5 47.240 \n",
      "\n",
      "TRAINING - Epoch: [23][0/469]\tTime 0.064 (0.064)\tData 0.052 (0.052)\tLoss 2.2566 (2.2566)\tPrec@1 10.938 (10.938)\tPrec@5 59.375 (59.375)\n",
      "TRAINING - Epoch: [23][100/469]\tTime 0.011 (0.010)\tData 0.002 (0.003)\tLoss 2.2239 (2.2298)\tPrec@1 21.094 (14.952)\tPrec@5 56.250 (60.984)\n",
      "TRAINING - Epoch: [23][200/469]\tTime 0.010 (0.010)\tData 0.004 (0.003)\tLoss 2.1996 (2.2307)\tPrec@1 15.625 (14.867)\tPrec@5 64.844 (60.689)\n",
      "TRAINING - Epoch: [23][300/469]\tTime 0.011 (0.010)\tData 0.004 (0.003)\tLoss 2.2049 (2.2292)\tPrec@1 16.406 (15.272)\tPrec@5 54.688 (60.452)\n",
      "TRAINING - Epoch: [23][400/469]\tTime 0.016 (0.010)\tData 0.011 (0.003)\tLoss 2.3514 (2.2321)\tPrec@1 7.031 (15.107)\tPrec@5 61.719 (60.170)\n",
      "EVALUATING - Epoch: [23][0/79]\tTime 0.050 (0.050)\tData 0.045 (0.045)\tLoss 2.2915 (2.2915)\tPrec@1 13.281 (13.281)\tPrec@5 46.875 (46.875)\n",
      "\n",
      " Epoch: 24\tTraining Loss 2.2306 \tTraining Prec@1 15.255 \tTraining Prec@5 60.137 \tValidation Loss 2.2780 \tValidation Prec@1 13.830 \tValidation Prec@5 53.050 \n",
      "\n",
      "TRAINING - Epoch: [24][0/469]\tTime 0.066 (0.066)\tData 0.052 (0.052)\tLoss 2.1720 (2.1720)\tPrec@1 22.656 (22.656)\tPrec@5 64.844 (64.844)\n",
      "TRAINING - Epoch: [24][100/469]\tTime 0.009 (0.012)\tData 0.003 (0.003)\tLoss 2.3134 (2.2352)\tPrec@1 10.156 (15.045)\tPrec@5 60.156 (59.901)\n",
      "TRAINING - Epoch: [24][200/469]\tTime 0.016 (0.011)\tData 0.002 (0.003)\tLoss 2.2164 (2.2325)\tPrec@1 9.375 (15.116)\tPrec@5 61.719 (59.942)\n",
      "TRAINING - Epoch: [24][300/469]\tTime 0.009 (0.011)\tData 0.002 (0.003)\tLoss 2.2155 (2.2260)\tPrec@1 14.062 (15.467)\tPrec@5 63.281 (60.208)\n",
      "TRAINING - Epoch: [24][400/469]\tTime 0.018 (0.011)\tData 0.004 (0.003)\tLoss 2.1084 (2.2277)\tPrec@1 17.188 (15.481)\tPrec@5 69.531 (60.147)\n",
      "EVALUATING - Epoch: [24][0/79]\tTime 0.053 (0.053)\tData 0.046 (0.046)\tLoss 2.3438 (2.3438)\tPrec@1 11.719 (11.719)\tPrec@5 42.969 (42.969)\n",
      "\n",
      " Epoch: 25\tTraining Loss 2.2298 \tTraining Prec@1 15.362 \tTraining Prec@5 60.005 \tValidation Loss 2.4118 \tValidation Prec@1 10.550 \tValidation Prec@5 48.780 \n",
      "\n",
      "TRAINING - Epoch: [25][0/469]\tTime 0.091 (0.091)\tData 0.063 (0.063)\tLoss 2.2598 (2.2598)\tPrec@1 7.812 (7.812)\tPrec@5 63.281 (63.281)\n",
      "TRAINING - Epoch: [25][100/469]\tTime 0.015 (0.014)\tData 0.004 (0.004)\tLoss 2.2155 (2.2353)\tPrec@1 14.062 (15.207)\tPrec@5 59.375 (59.893)\n",
      "TRAINING - Epoch: [25][200/469]\tTime 0.010 (0.013)\tData 0.000 (0.003)\tLoss 2.3002 (2.2326)\tPrec@1 10.938 (15.419)\tPrec@5 63.281 (59.814)\n",
      "TRAINING - Epoch: [25][300/469]\tTime 0.011 (0.013)\tData 0.003 (0.003)\tLoss 2.2076 (2.2344)\tPrec@1 21.094 (15.355)\tPrec@5 57.031 (59.772)\n",
      "TRAINING - Epoch: [25][400/469]\tTime 0.007 (0.013)\tData 0.001 (0.003)\tLoss 2.2481 (2.2329)\tPrec@1 10.156 (15.237)\tPrec@5 56.250 (59.663)\n",
      "EVALUATING - Epoch: [25][0/79]\tTime 0.054 (0.054)\tData 0.048 (0.048)\tLoss 2.9378 (2.9378)\tPrec@1 14.844 (14.844)\tPrec@5 57.031 (57.031)\n",
      "\n",
      " Epoch: 26\tTraining Loss 2.2324 \tTraining Prec@1 15.303 \tTraining Prec@5 59.683 \tValidation Loss 3.1771 \tValidation Prec@1 11.100 \tValidation Prec@5 54.590 \n",
      "\n",
      "TRAINING - Epoch: [26][0/469]\tTime 0.075 (0.075)\tData 0.061 (0.061)\tLoss 2.1515 (2.1515)\tPrec@1 23.438 (23.438)\tPrec@5 63.281 (63.281)\n",
      "TRAINING - Epoch: [26][100/469]\tTime 0.008 (0.011)\tData 0.002 (0.003)\tLoss 2.2189 (2.2265)\tPrec@1 15.625 (16.066)\tPrec@5 56.250 (60.156)\n",
      "TRAINING - Epoch: [26][200/469]\tTime 0.016 (0.011)\tData 0.002 (0.003)\tLoss 2.2569 (2.2306)\tPrec@1 9.375 (15.598)\tPrec@5 64.062 (60.211)\n",
      "TRAINING - Epoch: [26][300/469]\tTime 0.015 (0.011)\tData 0.002 (0.003)\tLoss 2.2757 (2.2313)\tPrec@1 14.844 (15.573)\tPrec@5 53.906 (60.060)\n",
      "TRAINING - Epoch: [26][400/469]\tTime 0.009 (0.011)\tData 0.002 (0.003)\tLoss 2.2286 (2.2301)\tPrec@1 14.844 (15.598)\tPrec@5 66.406 (59.911)\n",
      "EVALUATING - Epoch: [26][0/79]\tTime 0.075 (0.075)\tData 0.065 (0.065)\tLoss 2.3961 (2.3961)\tPrec@1 13.281 (13.281)\tPrec@5 39.062 (39.062)\n",
      "\n",
      " Epoch: 27\tTraining Loss 2.2301 \tTraining Prec@1 15.487 \tTraining Prec@5 59.905 \tValidation Loss 2.3811 \tValidation Prec@1 13.100 \tValidation Prec@5 51.520 \n",
      "\n",
      "TRAINING - Epoch: [27][0/469]\tTime 0.069 (0.069)\tData 0.057 (0.057)\tLoss 2.2146 (2.2146)\tPrec@1 17.969 (17.969)\tPrec@5 53.906 (53.906)\n",
      "TRAINING - Epoch: [27][100/469]\tTime 0.007 (0.011)\tData 0.000 (0.004)\tLoss 2.1983 (2.2321)\tPrec@1 18.750 (15.076)\tPrec@5 53.906 (59.120)\n",
      "TRAINING - Epoch: [27][200/469]\tTime 0.006 (0.010)\tData 0.000 (0.004)\tLoss 2.1813 (2.2309)\tPrec@1 22.656 (15.217)\tPrec@5 51.562 (59.503)\n",
      "TRAINING - Epoch: [27][300/469]\tTime 0.005 (0.010)\tData 0.000 (0.004)\tLoss 2.2737 (2.2305)\tPrec@1 15.625 (15.303)\tPrec@5 53.906 (59.824)\n",
      "TRAINING - Epoch: [27][400/469]\tTime 0.010 (0.011)\tData 0.004 (0.004)\tLoss 2.3232 (2.2286)\tPrec@1 10.156 (15.520)\tPrec@5 51.562 (59.921)\n",
      "EVALUATING - Epoch: [27][0/79]\tTime 0.056 (0.056)\tData 0.051 (0.051)\tLoss 3.5365 (3.5365)\tPrec@1 21.094 (21.094)\tPrec@5 42.969 (42.969)\n",
      "\n",
      " Epoch: 28\tTraining Loss 2.2302 \tTraining Prec@1 15.410 \tTraining Prec@5 59.845 \tValidation Loss 3.8586 \tValidation Prec@1 14.960 \tValidation Prec@5 43.430 \n",
      "\n",
      "TRAINING - Epoch: [28][0/469]\tTime 0.072 (0.072)\tData 0.059 (0.059)\tLoss 2.2287 (2.2287)\tPrec@1 18.750 (18.750)\tPrec@5 51.562 (51.562)\n",
      "TRAINING - Epoch: [28][100/469]\tTime 0.016 (0.013)\tData 0.001 (0.004)\tLoss 2.3192 (2.2220)\tPrec@1 12.500 (15.548)\tPrec@5 54.688 (60.087)\n",
      "TRAINING - Epoch: [28][200/469]\tTime 0.009 (0.013)\tData 0.001 (0.004)\tLoss 2.1446 (2.2324)\tPrec@1 19.531 (15.330)\tPrec@5 53.125 (59.394)\n",
      "TRAINING - Epoch: [28][300/469]\tTime 0.005 (0.013)\tData 0.000 (0.004)\tLoss 2.3200 (2.2343)\tPrec@1 10.938 (15.420)\tPrec@5 46.094 (59.313)\n",
      "TRAINING - Epoch: [28][400/469]\tTime 0.011 (0.013)\tData 0.000 (0.004)\tLoss 2.1709 (2.2350)\tPrec@1 20.312 (15.323)\tPrec@5 62.500 (59.539)\n",
      "EVALUATING - Epoch: [28][0/79]\tTime 0.055 (0.055)\tData 0.049 (0.049)\tLoss 5.3233 (5.3233)\tPrec@1 15.625 (15.625)\tPrec@5 42.969 (42.969)\n",
      "\n",
      " Epoch: 29\tTraining Loss 2.2345 \tTraining Prec@1 15.280 \tTraining Prec@5 59.653 \tValidation Loss 5.5249 \tValidation Prec@1 9.820 \tValidation Prec@5 49.260 \n",
      "\n",
      "TRAINING - Epoch: [29][0/469]\tTime 0.083 (0.083)\tData 0.068 (0.068)\tLoss 2.3084 (2.3084)\tPrec@1 11.719 (11.719)\tPrec@5 51.562 (51.562)\n",
      "TRAINING - Epoch: [29][100/469]\tTime 0.006 (0.012)\tData 0.000 (0.005)\tLoss 2.2001 (2.2536)\tPrec@1 24.219 (14.527)\tPrec@5 50.781 (58.864)\n",
      "TRAINING - Epoch: [29][200/469]\tTime 0.006 (0.011)\tData 0.000 (0.005)\tLoss 2.2252 (2.2405)\tPrec@1 17.188 (15.104)\tPrec@5 61.719 (59.321)\n",
      "TRAINING - Epoch: [29][300/469]\tTime 0.006 (0.011)\tData 0.000 (0.004)\tLoss 2.2367 (2.2380)\tPrec@1 16.406 (15.134)\tPrec@5 51.562 (59.528)\n",
      "TRAINING - Epoch: [29][400/469]\tTime 0.007 (0.010)\tData 0.000 (0.004)\tLoss 2.2888 (2.2356)\tPrec@1 12.500 (15.233)\tPrec@5 68.750 (59.780)\n",
      "EVALUATING - Epoch: [29][0/79]\tTime 0.051 (0.051)\tData 0.046 (0.046)\tLoss 4.4007 (4.4007)\tPrec@1 15.625 (15.625)\tPrec@5 43.750 (43.750)\n",
      "\n",
      " Epoch: 30\tTraining Loss 2.2348 \tTraining Prec@1 15.198 \tTraining Prec@5 59.778 \tValidation Loss 4.5757 \tValidation Prec@1 9.750 \tValidation Prec@5 49.430 \n",
      "\n",
      "TRAINING - Epoch: [30][0/469]\tTime 0.066 (0.066)\tData 0.054 (0.054)\tLoss 2.1652 (2.1652)\tPrec@1 17.188 (17.188)\tPrec@5 64.062 (64.062)\n",
      "TRAINING - Epoch: [30][100/469]\tTime 0.009 (0.010)\tData 0.002 (0.003)\tLoss 2.2745 (2.2239)\tPrec@1 5.469 (15.161)\tPrec@5 57.812 (61.069)\n",
      "TRAINING - Epoch: [30][200/469]\tTime 0.008 (0.010)\tData 0.002 (0.003)\tLoss 2.1884 (2.2299)\tPrec@1 22.656 (15.120)\tPrec@5 55.469 (60.211)\n",
      "TRAINING - Epoch: [30][300/469]\tTime 0.008 (0.010)\tData 0.002 (0.003)\tLoss 2.2192 (2.2344)\tPrec@1 13.281 (15.028)\tPrec@5 64.844 (59.943)\n",
      "TRAINING - Epoch: [30][400/469]\tTime 0.007 (0.010)\tData 0.001 (0.003)\tLoss 2.3420 (2.2321)\tPrec@1 12.500 (15.163)\tPrec@5 56.250 (59.901)\n",
      "EVALUATING - Epoch: [30][0/79]\tTime 0.051 (0.051)\tData 0.046 (0.046)\tLoss 2.5150 (2.5150)\tPrec@1 10.156 (10.156)\tPrec@5 48.438 (48.438)\n",
      "\n",
      " Epoch: 31\tTraining Loss 2.2316 \tTraining Prec@1 15.213 \tTraining Prec@5 59.793 \tValidation Loss 2.5097 \tValidation Prec@1 10.230 \tValidation Prec@5 54.760 \n",
      "\n",
      "TRAINING - Epoch: [31][0/469]\tTime 0.069 (0.069)\tData 0.054 (0.054)\tLoss 2.2414 (2.2414)\tPrec@1 15.625 (15.625)\tPrec@5 62.500 (62.500)\n",
      "TRAINING - Epoch: [31][100/469]\tTime 0.006 (0.010)\tData 0.000 (0.003)\tLoss 2.2185 (2.2242)\tPrec@1 14.844 (15.617)\tPrec@5 58.594 (59.739)\n",
      "TRAINING - Epoch: [31][200/469]\tTime 0.008 (0.009)\tData 0.002 (0.003)\tLoss 2.2110 (2.2286)\tPrec@1 24.219 (15.299)\tPrec@5 56.250 (59.869)\n",
      "TRAINING - Epoch: [31][300/469]\tTime 0.008 (0.009)\tData 0.000 (0.002)\tLoss 2.2381 (2.2332)\tPrec@1 17.188 (15.160)\tPrec@5 60.156 (59.673)\n",
      "TRAINING - Epoch: [31][400/469]\tTime 0.009 (0.009)\tData 0.000 (0.002)\tLoss 2.1445 (2.2322)\tPrec@1 28.125 (15.132)\tPrec@5 64.844 (59.778)\n",
      "EVALUATING - Epoch: [31][0/79]\tTime 0.052 (0.052)\tData 0.046 (0.046)\tLoss 2.2997 (2.2997)\tPrec@1 13.281 (13.281)\tPrec@5 46.094 (46.094)\n",
      "\n",
      " Epoch: 32\tTraining Loss 2.2306 \tTraining Prec@1 15.180 \tTraining Prec@5 59.930 \tValidation Loss 2.2950 \tValidation Prec@1 12.840 \tValidation Prec@5 52.180 \n",
      "\n",
      "TRAINING - Epoch: [32][0/469]\tTime 0.072 (0.072)\tData 0.057 (0.057)\tLoss 2.2044 (2.2044)\tPrec@1 20.312 (20.312)\tPrec@5 63.281 (63.281)\n",
      "TRAINING - Epoch: [32][100/469]\tTime 0.006 (0.013)\tData 0.000 (0.006)\tLoss 2.2835 (2.2308)\tPrec@1 11.719 (15.408)\tPrec@5 61.719 (59.831)\n",
      "TRAINING - Epoch: [32][200/469]\tTime 0.006 (0.013)\tData 0.000 (0.006)\tLoss 2.1559 (2.2334)\tPrec@1 18.750 (14.945)\tPrec@5 61.719 (60.020)\n",
      "TRAINING - Epoch: [32][300/469]\tTime 0.008 (0.012)\tData 0.000 (0.006)\tLoss 2.2054 (2.2288)\tPrec@1 14.844 (15.210)\tPrec@5 65.625 (60.193)\n",
      "TRAINING - Epoch: [32][400/469]\tTime 0.006 (0.012)\tData 0.000 (0.006)\tLoss 2.2263 (2.2283)\tPrec@1 16.406 (15.354)\tPrec@5 60.156 (60.006)\n",
      "EVALUATING - Epoch: [32][0/79]\tTime 0.054 (0.054)\tData 0.048 (0.048)\tLoss 2.3482 (2.3482)\tPrec@1 15.625 (15.625)\tPrec@5 46.094 (46.094)\n",
      "\n",
      " Epoch: 33\tTraining Loss 2.2269 \tTraining Prec@1 15.478 \tTraining Prec@5 59.923 \tValidation Loss 2.4294 \tValidation Prec@1 11.140 \tValidation Prec@5 49.970 \n",
      "\n",
      "TRAINING - Epoch: [33][0/469]\tTime 0.140 (0.140)\tData 0.116 (0.116)\tLoss 2.2124 (2.2124)\tPrec@1 17.188 (17.188)\tPrec@5 58.594 (58.594)\n",
      "TRAINING - Epoch: [33][100/469]\tTime 0.018 (0.013)\tData 0.012 (0.006)\tLoss 2.1727 (2.2352)\tPrec@1 21.094 (14.209)\tPrec@5 63.281 (59.916)\n",
      "TRAINING - Epoch: [33][200/469]\tTime 0.019 (0.013)\tData 0.013 (0.006)\tLoss 2.1642 (2.2272)\tPrec@1 11.719 (14.758)\tPrec@5 67.188 (60.560)\n",
      "TRAINING - Epoch: [33][300/469]\tTime 0.018 (0.012)\tData 0.012 (0.006)\tLoss 2.1652 (2.2299)\tPrec@1 14.062 (14.768)\tPrec@5 65.625 (60.335)\n",
      "TRAINING - Epoch: [33][400/469]\tTime 0.018 (0.012)\tData 0.011 (0.006)\tLoss 2.2967 (2.2321)\tPrec@1 17.188 (14.717)\tPrec@5 60.156 (60.187)\n",
      "EVALUATING - Epoch: [33][0/79]\tTime 0.074 (0.074)\tData 0.067 (0.067)\tLoss 3.5799 (3.5799)\tPrec@1 7.812 (7.812)\tPrec@5 53.906 (53.906)\n",
      "\n",
      " Epoch: 34\tTraining Loss 2.2302 \tTraining Prec@1 14.905 \tTraining Prec@5 60.152 \tValidation Loss 3.6340 \tValidation Prec@1 10.290 \tValidation Prec@5 50.300 \n",
      "\n",
      "TRAINING - Epoch: [34][0/469]\tTime 0.069 (0.069)\tData 0.054 (0.054)\tLoss 2.2550 (2.2550)\tPrec@1 17.188 (17.188)\tPrec@5 57.812 (57.812)\n",
      "TRAINING - Epoch: [34][100/469]\tTime 0.009 (0.012)\tData 0.004 (0.005)\tLoss 2.2438 (2.2351)\tPrec@1 3.125 (14.635)\tPrec@5 56.250 (59.360)\n",
      "TRAINING - Epoch: [34][200/469]\tTime 0.011 (0.012)\tData 0.005 (0.005)\tLoss 2.1904 (2.2321)\tPrec@1 20.312 (15.236)\tPrec@5 67.969 (59.597)\n",
      "TRAINING - Epoch: [34][300/469]\tTime 0.019 (0.012)\tData 0.011 (0.005)\tLoss 2.2818 (2.2287)\tPrec@1 7.031 (15.324)\tPrec@5 60.156 (59.790)\n",
      "TRAINING - Epoch: [34][400/469]\tTime 0.012 (0.012)\tData 0.006 (0.005)\tLoss 2.3205 (2.2313)\tPrec@1 11.719 (15.175)\tPrec@5 54.688 (59.654)\n",
      "EVALUATING - Epoch: [34][0/79]\tTime 0.055 (0.055)\tData 0.050 (0.050)\tLoss 2.3144 (2.3144)\tPrec@1 10.156 (10.156)\tPrec@5 45.312 (45.312)\n",
      "\n",
      " Epoch: 35\tTraining Loss 2.2314 \tTraining Prec@1 15.145 \tTraining Prec@5 59.725 \tValidation Loss 2.2818 \tValidation Prec@1 10.260 \tValidation Prec@5 54.240 \n",
      "\n",
      "TRAINING - Epoch: [35][0/469]\tTime 0.086 (0.086)\tData 0.066 (0.066)\tLoss 2.1558 (2.1558)\tPrec@1 18.750 (18.750)\tPrec@5 67.969 (67.969)\n",
      "TRAINING - Epoch: [35][100/469]\tTime 0.013 (0.014)\tData 0.001 (0.003)\tLoss 2.2951 (2.2247)\tPrec@1 10.156 (15.223)\tPrec@5 58.594 (60.752)\n",
      "TRAINING - Epoch: [35][200/469]\tTime 0.011 (0.013)\tData 0.003 (0.003)\tLoss 2.1666 (2.2265)\tPrec@1 12.500 (15.155)\tPrec@5 64.844 (60.316)\n",
      "TRAINING - Epoch: [35][300/469]\tTime 0.010 (0.013)\tData 0.002 (0.002)\tLoss 2.2618 (2.2294)\tPrec@1 16.406 (15.153)\tPrec@5 52.344 (60.115)\n",
      "TRAINING - Epoch: [35][400/469]\tTime 0.008 (0.012)\tData 0.001 (0.002)\tLoss 2.1408 (2.2292)\tPrec@1 21.875 (15.062)\tPrec@5 62.500 (60.139)\n",
      "EVALUATING - Epoch: [35][0/79]\tTime 0.052 (0.052)\tData 0.047 (0.047)\tLoss 3.9470 (3.9470)\tPrec@1 15.625 (15.625)\tPrec@5 43.750 (43.750)\n",
      "\n",
      " Epoch: 36\tTraining Loss 2.2280 \tTraining Prec@1 15.138 \tTraining Prec@5 60.330 \tValidation Loss 3.9906 \tValidation Prec@1 9.680 \tValidation Prec@5 49.640 \n",
      "\n",
      "TRAINING - Epoch: [36][0/469]\tTime 0.069 (0.069)\tData 0.058 (0.058)\tLoss 2.3704 (2.3704)\tPrec@1 11.719 (11.719)\tPrec@5 57.812 (57.812)\n",
      "TRAINING - Epoch: [36][100/469]\tTime 0.011 (0.011)\tData 0.005 (0.004)\tLoss 2.1734 (2.2351)\tPrec@1 23.438 (15.261)\tPrec@5 60.156 (59.653)\n",
      "TRAINING - Epoch: [36][200/469]\tTime 0.014 (0.011)\tData 0.008 (0.004)\tLoss 2.1619 (2.2326)\tPrec@1 11.719 (15.283)\tPrec@5 69.531 (59.830)\n",
      "TRAINING - Epoch: [36][300/469]\tTime 0.011 (0.011)\tData 0.002 (0.004)\tLoss 2.1841 (2.2319)\tPrec@1 21.875 (14.987)\tPrec@5 65.625 (60.052)\n",
      "TRAINING - Epoch: [36][400/469]\tTime 0.013 (0.011)\tData 0.001 (0.003)\tLoss 2.3386 (2.2307)\tPrec@1 11.719 (15.025)\tPrec@5 55.469 (59.979)\n",
      "EVALUATING - Epoch: [36][0/79]\tTime 0.051 (0.051)\tData 0.046 (0.046)\tLoss 3.5716 (3.5716)\tPrec@1 7.031 (7.031)\tPrec@5 52.344 (52.344)\n",
      "\n",
      " Epoch: 37\tTraining Loss 2.2307 \tTraining Prec@1 15.098 \tTraining Prec@5 59.980 \tValidation Loss 3.6552 \tValidation Prec@1 9.260 \tValidation Prec@5 49.950 \n",
      "\n",
      "TRAINING - Epoch: [37][0/469]\tTime 0.067 (0.067)\tData 0.055 (0.055)\tLoss 2.2430 (2.2430)\tPrec@1 13.281 (13.281)\tPrec@5 69.531 (69.531)\n",
      "TRAINING - Epoch: [37][100/469]\tTime 0.011 (0.011)\tData 0.002 (0.003)\tLoss 2.1704 (2.2294)\tPrec@1 21.094 (15.238)\tPrec@5 54.688 (60.164)\n",
      "TRAINING - Epoch: [37][200/469]\tTime 0.008 (0.010)\tData 0.002 (0.003)\tLoss 2.2464 (2.2339)\tPrec@1 13.281 (14.731)\tPrec@5 57.031 (60.187)\n",
      "TRAINING - Epoch: [37][300/469]\tTime 0.012 (0.010)\tData 0.005 (0.003)\tLoss 2.2003 (2.2315)\tPrec@1 20.312 (15.360)\tPrec@5 59.375 (60.270)\n",
      "TRAINING - Epoch: [37][400/469]\tTime 0.009 (0.010)\tData 0.003 (0.003)\tLoss 2.1847 (2.2311)\tPrec@1 9.375 (15.167)\tPrec@5 64.844 (60.082)\n",
      "EVALUATING - Epoch: [37][0/79]\tTime 0.058 (0.058)\tData 0.053 (0.053)\tLoss 2.2871 (2.2871)\tPrec@1 9.375 (9.375)\tPrec@5 49.219 (49.219)\n",
      "\n",
      " Epoch: 38\tTraining Loss 2.2296 \tTraining Prec@1 15.217 \tTraining Prec@5 60.023 \tValidation Loss 2.2741 \tValidation Prec@1 10.650 \tValidation Prec@5 55.040 \n",
      "\n",
      "TRAINING - Epoch: [38][0/469]\tTime 0.069 (0.069)\tData 0.056 (0.056)\tLoss 2.1811 (2.1811)\tPrec@1 14.844 (14.844)\tPrec@5 68.750 (68.750)\n",
      "TRAINING - Epoch: [38][100/469]\tTime 0.010 (0.011)\tData 0.001 (0.003)\tLoss 2.1612 (2.2265)\tPrec@1 21.094 (14.612)\tPrec@5 55.469 (60.063)\n",
      "TRAINING - Epoch: [38][200/469]\tTime 0.010 (0.011)\tData 0.004 (0.003)\tLoss 2.1869 (2.2281)\tPrec@1 21.875 (14.956)\tPrec@5 51.562 (60.036)\n",
      "TRAINING - Epoch: [38][300/469]\tTime 0.010 (0.011)\tData 0.002 (0.003)\tLoss 2.2483 (2.2276)\tPrec@1 16.406 (14.955)\tPrec@5 56.250 (60.260)\n",
      "TRAINING - Epoch: [38][400/469]\tTime 0.012 (0.012)\tData 0.002 (0.003)\tLoss 2.2643 (2.2301)\tPrec@1 14.062 (14.898)\tPrec@5 63.281 (60.045)\n",
      "EVALUATING - Epoch: [38][0/79]\tTime 0.066 (0.066)\tData 0.057 (0.057)\tLoss 2.2659 (2.2659)\tPrec@1 9.375 (9.375)\tPrec@5 49.219 (49.219)\n",
      "\n",
      " Epoch: 39\tTraining Loss 2.2294 \tTraining Prec@1 14.990 \tTraining Prec@5 60.028 \tValidation Loss 2.2319 \tValidation Prec@1 10.140 \tValidation Prec@5 57.250 \n",
      "\n",
      "TRAINING - Epoch: [39][0/469]\tTime 0.081 (0.081)\tData 0.066 (0.066)\tLoss 2.2337 (2.2337)\tPrec@1 12.500 (12.500)\tPrec@5 67.188 (67.188)\n",
      "TRAINING - Epoch: [39][100/469]\tTime 0.011 (0.014)\tData 0.002 (0.003)\tLoss 2.1595 (2.2184)\tPrec@1 25.000 (15.671)\tPrec@5 56.250 (60.357)\n",
      "TRAINING - Epoch: [39][200/469]\tTime 0.016 (0.014)\tData 0.003 (0.003)\tLoss 2.2952 (2.2290)\tPrec@1 10.156 (15.170)\tPrec@5 57.812 (59.768)\n",
      "TRAINING - Epoch: [39][300/469]\tTime 0.012 (0.014)\tData 0.002 (0.003)\tLoss 2.2262 (2.2308)\tPrec@1 10.156 (14.958)\tPrec@5 57.812 (59.681)\n",
      "TRAINING - Epoch: [39][400/469]\tTime 0.008 (0.014)\tData 0.002 (0.002)\tLoss 2.2469 (2.2298)\tPrec@1 14.844 (15.118)\tPrec@5 51.562 (59.769)\n",
      "EVALUATING - Epoch: [39][0/79]\tTime 0.054 (0.054)\tData 0.048 (0.048)\tLoss 3.3722 (3.3722)\tPrec@1 15.625 (15.625)\tPrec@5 41.406 (41.406)\n",
      "\n",
      " Epoch: 40\tTraining Loss 2.2310 \tTraining Prec@1 14.962 \tTraining Prec@5 59.668 \tValidation Loss 3.3972 \tValidation Prec@1 9.750 \tValidation Prec@5 49.680 \n",
      "\n",
      "TRAINING - Epoch: [40][0/469]\tTime 0.070 (0.070)\tData 0.054 (0.054)\tLoss 2.2792 (2.2792)\tPrec@1 9.375 (9.375)\tPrec@5 54.688 (54.688)\n",
      "TRAINING - Epoch: [40][100/469]\tTime 0.015 (0.011)\tData 0.009 (0.004)\tLoss 2.2221 (2.2285)\tPrec@1 15.625 (15.091)\tPrec@5 65.625 (59.777)\n",
      "TRAINING - Epoch: [40][200/469]\tTime 0.015 (0.011)\tData 0.009 (0.004)\tLoss 2.1838 (2.2241)\tPrec@1 14.844 (15.516)\tPrec@5 60.938 (60.075)\n",
      "TRAINING - Epoch: [40][300/469]\tTime 0.015 (0.011)\tData 0.005 (0.004)\tLoss 2.2832 (2.2264)\tPrec@1 10.156 (15.534)\tPrec@5 65.625 (59.964)\n",
      "TRAINING - Epoch: [40][400/469]\tTime 0.010 (0.011)\tData 0.004 (0.004)\tLoss 2.2809 (2.2271)\tPrec@1 13.281 (15.438)\tPrec@5 57.812 (59.835)\n",
      "EVALUATING - Epoch: [40][0/79]\tTime 0.052 (0.052)\tData 0.046 (0.046)\tLoss 3.5423 (3.5423)\tPrec@1 8.594 (8.594)\tPrec@5 53.125 (53.125)\n",
      "\n",
      " Epoch: 41\tTraining Loss 2.2269 \tTraining Prec@1 15.467 \tTraining Prec@5 59.870 \tValidation Loss 3.6303 \tValidation Prec@1 10.100 \tValidation Prec@5 50.250 \n",
      "\n",
      "TRAINING - Epoch: [41][0/469]\tTime 0.068 (0.068)\tData 0.055 (0.055)\tLoss 2.3104 (2.3104)\tPrec@1 8.594 (8.594)\tPrec@5 59.375 (59.375)\n",
      "TRAINING - Epoch: [41][100/469]\tTime 0.006 (0.012)\tData 0.000 (0.005)\tLoss 2.2636 (2.2306)\tPrec@1 11.719 (14.890)\tPrec@5 57.031 (59.630)\n",
      "TRAINING - Epoch: [41][200/469]\tTime 0.006 (0.012)\tData 0.000 (0.005)\tLoss 2.1400 (2.2289)\tPrec@1 17.969 (15.120)\tPrec@5 69.531 (60.152)\n",
      "TRAINING - Epoch: [41][300/469]\tTime 0.006 (0.012)\tData 0.000 (0.005)\tLoss 2.2616 (2.2273)\tPrec@1 17.969 (15.303)\tPrec@5 54.688 (60.076)\n",
      "TRAINING - Epoch: [41][400/469]\tTime 0.009 (0.012)\tData 0.003 (0.005)\tLoss 2.2402 (2.2279)\tPrec@1 13.281 (15.159)\tPrec@5 56.250 (59.866)\n",
      "EVALUATING - Epoch: [41][0/79]\tTime 0.051 (0.051)\tData 0.046 (0.046)\tLoss 3.2467 (3.2467)\tPrec@1 13.281 (13.281)\tPrec@5 45.312 (45.312)\n",
      "\n",
      " Epoch: 42\tTraining Loss 2.2275 \tTraining Prec@1 15.355 \tTraining Prec@5 59.930 \tValidation Loss 3.4262 \tValidation Prec@1 10.680 \tValidation Prec@5 45.980 \n",
      "\n",
      "TRAINING - Epoch: [42][0/469]\tTime 0.064 (0.064)\tData 0.053 (0.053)\tLoss 2.2197 (2.2197)\tPrec@1 18.750 (18.750)\tPrec@5 59.375 (59.375)\n",
      "TRAINING - Epoch: [42][100/469]\tTime 0.019 (0.012)\tData 0.013 (0.005)\tLoss 2.1775 (2.2347)\tPrec@1 14.062 (15.200)\tPrec@5 71.875 (59.561)\n",
      "TRAINING - Epoch: [42][200/469]\tTime 0.017 (0.011)\tData 0.011 (0.005)\tLoss 2.1706 (2.2320)\tPrec@1 25.000 (15.120)\tPrec@5 59.375 (59.701)\n",
      "TRAINING - Epoch: [42][300/469]\tTime 0.016 (0.011)\tData 0.010 (0.005)\tLoss 2.2095 (2.2336)\tPrec@1 16.406 (15.233)\tPrec@5 57.031 (59.762)\n",
      "TRAINING - Epoch: [42][400/469]\tTime 0.022 (0.011)\tData 0.014 (0.005)\tLoss 2.2539 (2.2326)\tPrec@1 9.375 (15.245)\tPrec@5 54.688 (59.587)\n",
      "EVALUATING - Epoch: [42][0/79]\tTime 0.053 (0.053)\tData 0.046 (0.046)\tLoss 2.2068 (2.2068)\tPrec@1 21.875 (21.875)\tPrec@5 54.688 (54.688)\n",
      "\n",
      " Epoch: 43\tTraining Loss 2.2324 \tTraining Prec@1 15.292 \tTraining Prec@5 59.640 \tValidation Loss 2.2171 \tValidation Prec@1 20.510 \tValidation Prec@5 59.090 \n",
      "\n",
      "TRAINING - Epoch: [43][0/469]\tTime 0.074 (0.074)\tData 0.057 (0.057)\tLoss 2.1507 (2.1507)\tPrec@1 27.344 (27.344)\tPrec@5 62.500 (62.500)\n",
      "TRAINING - Epoch: [43][100/469]\tTime 0.014 (0.012)\tData 0.003 (0.003)\tLoss 2.1946 (2.2186)\tPrec@1 18.750 (15.927)\tPrec@5 71.094 (60.311)\n",
      "TRAINING - Epoch: [43][200/469]\tTime 0.007 (0.011)\tData 0.000 (0.003)\tLoss 2.3351 (2.2250)\tPrec@1 9.375 (15.749)\tPrec@5 53.125 (60.160)\n",
      "TRAINING - Epoch: [43][300/469]\tTime 0.010 (0.011)\tData 0.001 (0.003)\tLoss 2.1986 (2.2254)\tPrec@1 16.406 (15.436)\tPrec@5 60.156 (59.975)\n",
      "TRAINING - Epoch: [43][400/469]\tTime 0.008 (0.011)\tData 0.002 (0.003)\tLoss 2.1723 (2.2269)\tPrec@1 16.406 (15.467)\tPrec@5 60.938 (59.882)\n",
      "EVALUATING - Epoch: [43][0/79]\tTime 0.073 (0.073)\tData 0.066 (0.066)\tLoss 3.2721 (3.2721)\tPrec@1 14.062 (14.062)\tPrec@5 50.000 (50.000)\n",
      "\n",
      " Epoch: 44\tTraining Loss 2.2258 \tTraining Prec@1 15.470 \tTraining Prec@5 60.005 \tValidation Loss 3.3802 \tValidation Prec@1 11.030 \tValidation Prec@5 48.570 \n",
      "\n",
      "TRAINING - Epoch: [44][0/469]\tTime 0.090 (0.090)\tData 0.070 (0.070)\tLoss 2.2233 (2.2233)\tPrec@1 17.188 (17.188)\tPrec@5 58.594 (58.594)\n",
      "TRAINING - Epoch: [44][100/469]\tTime 0.009 (0.014)\tData 0.003 (0.003)\tLoss 2.3237 (2.2286)\tPrec@1 9.375 (15.014)\tPrec@5 62.500 (59.886)\n",
      "TRAINING - Epoch: [44][200/469]\tTime 0.009 (0.012)\tData 0.003 (0.003)\tLoss 2.1991 (2.2249)\tPrec@1 21.094 (15.407)\tPrec@5 58.594 (59.974)\n",
      "TRAINING - Epoch: [44][300/469]\tTime 0.012 (0.012)\tData 0.002 (0.003)\tLoss 2.2615 (2.2269)\tPrec@1 10.156 (15.096)\tPrec@5 55.469 (59.666)\n",
      "TRAINING - Epoch: [44][400/469]\tTime 0.012 (0.011)\tData 0.003 (0.003)\tLoss 2.2604 (2.2269)\tPrec@1 10.156 (15.128)\tPrec@5 58.594 (59.868)\n",
      "EVALUATING - Epoch: [44][0/79]\tTime 0.055 (0.055)\tData 0.046 (0.046)\tLoss 2.3519 (2.3519)\tPrec@1 10.156 (10.156)\tPrec@5 43.750 (43.750)\n",
      "\n",
      " Epoch: 45\tTraining Loss 2.2286 \tTraining Prec@1 14.978 \tTraining Prec@5 59.813 \tValidation Loss 2.3439 \tValidation Prec@1 11.740 \tValidation Prec@5 49.330 \n",
      "\n",
      "TRAINING - Epoch: [45][0/469]\tTime 0.070 (0.070)\tData 0.058 (0.058)\tLoss 2.2587 (2.2587)\tPrec@1 13.281 (13.281)\tPrec@5 56.250 (56.250)\n",
      "TRAINING - Epoch: [45][100/469]\tTime 0.015 (0.011)\tData 0.009 (0.004)\tLoss 2.3267 (2.2275)\tPrec@1 9.375 (14.929)\tPrec@5 56.250 (60.350)\n",
      "TRAINING - Epoch: [45][200/469]\tTime 0.014 (0.011)\tData 0.008 (0.004)\tLoss 2.1192 (2.2309)\tPrec@1 17.969 (14.657)\tPrec@5 67.188 (60.032)\n",
      "TRAINING - Epoch: [45][300/469]\tTime 0.013 (0.011)\tData 0.005 (0.004)\tLoss 2.1935 (2.2325)\tPrec@1 20.312 (14.901)\tPrec@5 57.812 (59.881)\n",
      "TRAINING - Epoch: [45][400/469]\tTime 0.019 (0.011)\tData 0.012 (0.004)\tLoss 2.2102 (2.2305)\tPrec@1 16.406 (15.009)\tPrec@5 60.938 (59.850)\n",
      "EVALUATING - Epoch: [45][0/79]\tTime 0.053 (0.053)\tData 0.047 (0.047)\tLoss 2.7803 (2.7803)\tPrec@1 14.062 (14.062)\tPrec@5 61.719 (61.719)\n",
      "\n",
      " Epoch: 46\tTraining Loss 2.2303 \tTraining Prec@1 14.955 \tTraining Prec@5 59.903 \tValidation Loss 3.0804 \tValidation Prec@1 8.350 \tValidation Prec@5 57.220 \n",
      "\n",
      "TRAINING - Epoch: [46][0/469]\tTime 0.086 (0.086)\tData 0.068 (0.068)\tLoss 2.2341 (2.2341)\tPrec@1 10.938 (10.938)\tPrec@5 59.375 (59.375)\n",
      "TRAINING - Epoch: [46][100/469]\tTime 0.009 (0.013)\tData 0.001 (0.005)\tLoss 2.2730 (2.2239)\tPrec@1 18.750 (15.316)\tPrec@5 58.594 (60.056)\n",
      "TRAINING - Epoch: [46][200/469]\tTime 0.006 (0.013)\tData 0.000 (0.005)\tLoss 2.2814 (2.2287)\tPrec@1 5.469 (15.236)\tPrec@5 64.062 (60.024)\n",
      "TRAINING - Epoch: [46][300/469]\tTime 0.008 (0.013)\tData 0.000 (0.005)\tLoss 2.2888 (2.2318)\tPrec@1 9.375 (15.090)\tPrec@5 58.594 (59.772)\n",
      "TRAINING - Epoch: [46][400/469]\tTime 0.007 (0.012)\tData 0.000 (0.005)\tLoss 2.2365 (2.2322)\tPrec@1 19.531 (15.171)\tPrec@5 57.031 (59.737)\n",
      "EVALUATING - Epoch: [46][0/79]\tTime 0.053 (0.053)\tData 0.048 (0.048)\tLoss 2.5210 (2.5210)\tPrec@1 9.375 (9.375)\tPrec@5 45.312 (45.312)\n",
      "\n",
      " Epoch: 47\tTraining Loss 2.2334 \tTraining Prec@1 15.093 \tTraining Prec@5 59.627 \tValidation Loss 2.5791 \tValidation Prec@1 9.760 \tValidation Prec@5 51.040 \n",
      "\n",
      "TRAINING - Epoch: [47][0/469]\tTime 0.074 (0.074)\tData 0.061 (0.061)\tLoss 2.1886 (2.1886)\tPrec@1 15.625 (15.625)\tPrec@5 67.188 (67.188)\n",
      "TRAINING - Epoch: [47][100/469]\tTime 0.007 (0.010)\tData 0.000 (0.002)\tLoss 2.2272 (2.2333)\tPrec@1 6.250 (15.053)\tPrec@5 68.750 (59.615)\n",
      "TRAINING - Epoch: [47][200/469]\tTime 0.008 (0.010)\tData 0.002 (0.002)\tLoss 2.1721 (2.2309)\tPrec@1 12.500 (15.100)\tPrec@5 69.531 (59.768)\n",
      "TRAINING - Epoch: [47][300/469]\tTime 0.012 (0.010)\tData 0.003 (0.002)\tLoss 2.2013 (2.2328)\tPrec@1 19.531 (15.085)\tPrec@5 60.938 (59.536)\n",
      "TRAINING - Epoch: [47][400/469]\tTime 0.012 (0.010)\tData 0.005 (0.002)\tLoss 2.2638 (2.2315)\tPrec@1 8.594 (15.140)\tPrec@5 58.594 (59.743)\n",
      "EVALUATING - Epoch: [47][0/79]\tTime 0.052 (0.052)\tData 0.047 (0.047)\tLoss 2.2685 (2.2685)\tPrec@1 7.812 (7.812)\tPrec@5 57.031 (57.031)\n",
      "\n",
      " Epoch: 48\tTraining Loss 2.2306 \tTraining Prec@1 15.213 \tTraining Prec@5 59.752 \tValidation Loss 2.2665 \tValidation Prec@1 11.130 \tValidation Prec@5 61.260 \n",
      "\n",
      "TRAINING - Epoch: [48][0/469]\tTime 0.081 (0.081)\tData 0.067 (0.067)\tLoss 2.2491 (2.2491)\tPrec@1 14.062 (14.062)\tPrec@5 64.844 (64.844)\n",
      "TRAINING - Epoch: [48][100/469]\tTime 0.013 (0.013)\tData 0.002 (0.003)\tLoss 2.3540 (2.2284)\tPrec@1 14.062 (15.184)\tPrec@5 57.031 (59.561)\n",
      "TRAINING - Epoch: [48][200/469]\tTime 0.015 (0.013)\tData 0.002 (0.003)\tLoss 2.2028 (2.2272)\tPrec@1 16.406 (15.287)\tPrec@5 64.062 (59.422)\n",
      "TRAINING - Epoch: [48][300/469]\tTime 0.013 (0.013)\tData 0.002 (0.002)\tLoss 2.3115 (2.2276)\tPrec@1 14.844 (15.150)\tPrec@5 57.812 (59.466)\n",
      "TRAINING - Epoch: [48][400/469]\tTime 0.009 (0.013)\tData 0.002 (0.002)\tLoss 2.1775 (2.2257)\tPrec@1 25.781 (15.403)\tPrec@5 62.500 (59.582)\n",
      "EVALUATING - Epoch: [48][0/79]\tTime 0.055 (0.055)\tData 0.050 (0.050)\tLoss 2.3323 (2.3323)\tPrec@1 10.938 (10.938)\tPrec@5 43.750 (43.750)\n",
      "\n",
      " Epoch: 49\tTraining Loss 2.2289 \tTraining Prec@1 15.200 \tTraining Prec@5 59.648 \tValidation Loss 2.3245 \tValidation Prec@1 10.940 \tValidation Prec@5 50.500 \n",
      "\n",
      "TRAINING - Epoch: [49][0/469]\tTime 0.077 (0.077)\tData 0.063 (0.063)\tLoss 2.2692 (2.2692)\tPrec@1 12.500 (12.500)\tPrec@5 57.031 (57.031)\n",
      "TRAINING - Epoch: [49][100/469]\tTime 0.009 (0.010)\tData 0.001 (0.002)\tLoss 2.2141 (2.2336)\tPrec@1 14.844 (15.354)\tPrec@5 57.812 (60.435)\n",
      "TRAINING - Epoch: [49][200/469]\tTime 0.010 (0.010)\tData 0.002 (0.002)\tLoss 2.2871 (2.2290)\tPrec@1 12.500 (15.225)\tPrec@5 57.031 (60.444)\n",
      "TRAINING - Epoch: [49][300/469]\tTime 0.011 (0.011)\tData 0.005 (0.002)\tLoss 2.2065 (2.2313)\tPrec@1 19.531 (15.067)\tPrec@5 57.812 (60.143)\n",
      "TRAINING - Epoch: [49][400/469]\tTime 0.013 (0.011)\tData 0.005 (0.002)\tLoss 2.1872 (2.2316)\tPrec@1 20.312 (15.044)\tPrec@5 64.062 (60.205)\n",
      "EVALUATING - Epoch: [49][0/79]\tTime 0.051 (0.051)\tData 0.046 (0.046)\tLoss 2.9615 (2.9615)\tPrec@1 14.844 (14.844)\tPrec@5 56.250 (56.250)\n",
      "\n",
      " Epoch: 50\tTraining Loss 2.2291 \tTraining Prec@1 15.120 \tTraining Prec@5 60.248 \tValidation Loss 3.2262 \tValidation Prec@1 10.030 \tValidation Prec@5 50.960 \n",
      "\n",
      "TRAINING - Epoch: [50][0/469]\tTime 0.070 (0.070)\tData 0.056 (0.056)\tLoss 2.2592 (2.2592)\tPrec@1 17.188 (17.188)\tPrec@5 51.562 (51.562)\n",
      "TRAINING - Epoch: [50][100/469]\tTime 0.008 (0.012)\tData 0.001 (0.003)\tLoss 2.1872 (2.2349)\tPrec@1 25.000 (15.555)\tPrec@5 57.812 (59.437)\n",
      "TRAINING - Epoch: [50][200/469]\tTime 0.008 (0.011)\tData 0.001 (0.002)\tLoss 2.2234 (2.2349)\tPrec@1 17.188 (15.337)\tPrec@5 57.031 (59.635)\n",
      "TRAINING - Epoch: [50][300/469]\tTime 0.014 (0.011)\tData 0.002 (0.002)\tLoss 2.1978 (2.2337)\tPrec@1 21.094 (15.132)\tPrec@5 54.688 (59.666)\n",
      "TRAINING - Epoch: [50][400/469]\tTime 0.014 (0.011)\tData 0.003 (0.002)\tLoss 2.1643 (2.2336)\tPrec@1 19.531 (15.023)\tPrec@5 57.812 (59.706)\n",
      "EVALUATING - Epoch: [50][0/79]\tTime 0.066 (0.066)\tData 0.059 (0.059)\tLoss 2.1614 (2.1614)\tPrec@1 14.844 (14.844)\tPrec@5 55.469 (55.469)\n",
      "\n",
      " Epoch: 51\tTraining Loss 2.2321 \tTraining Prec@1 14.992 \tTraining Prec@5 59.665 \tValidation Loss 2.1792 \tValidation Prec@1 13.980 \tValidation Prec@5 61.730 \n",
      "\n",
      "TRAINING - Epoch: [51][0/469]\tTime 0.102 (0.102)\tData 0.083 (0.083)\tLoss 2.2952 (2.2952)\tPrec@1 15.625 (15.625)\tPrec@5 57.812 (57.812)\n",
      "TRAINING - Epoch: [51][100/469]\tTime 0.020 (0.014)\tData 0.014 (0.006)\tLoss 2.2148 (2.2319)\tPrec@1 17.188 (14.991)\tPrec@5 50.000 (59.986)\n",
      "TRAINING - Epoch: [51][200/469]\tTime 0.010 (0.014)\tData 0.002 (0.004)\tLoss 2.2399 (2.2278)\tPrec@1 14.844 (15.244)\tPrec@5 60.938 (59.682)\n",
      "TRAINING - Epoch: [51][300/469]\tTime 0.012 (0.012)\tData 0.004 (0.004)\tLoss 2.2594 (2.2275)\tPrec@1 13.281 (15.443)\tPrec@5 63.281 (59.798)\n",
      "TRAINING - Epoch: [51][400/469]\tTime 0.011 (0.012)\tData 0.004 (0.004)\tLoss 2.2318 (2.2252)\tPrec@1 19.531 (15.522)\tPrec@5 52.344 (59.922)\n",
      "EVALUATING - Epoch: [51][0/79]\tTime 0.057 (0.057)\tData 0.050 (0.050)\tLoss 2.3246 (2.3246)\tPrec@1 13.281 (13.281)\tPrec@5 43.750 (43.750)\n",
      "\n",
      " Epoch: 52\tTraining Loss 2.2272 \tTraining Prec@1 15.367 \tTraining Prec@5 60.023 \tValidation Loss 2.3371 \tValidation Prec@1 11.500 \tValidation Prec@5 50.820 \n",
      "\n",
      "TRAINING - Epoch: [52][0/469]\tTime 0.072 (0.072)\tData 0.059 (0.059)\tLoss 2.2432 (2.2432)\tPrec@1 17.969 (17.969)\tPrec@5 57.031 (57.031)\n",
      "TRAINING - Epoch: [52][100/469]\tTime 0.009 (0.010)\tData 0.002 (0.003)\tLoss 2.2508 (2.2311)\tPrec@1 11.719 (14.998)\tPrec@5 58.594 (60.334)\n",
      "TRAINING - Epoch: [52][200/469]\tTime 0.019 (0.010)\tData 0.004 (0.002)\tLoss 2.2423 (2.2319)\tPrec@1 14.844 (15.112)\tPrec@5 63.281 (59.674)\n",
      "TRAINING - Epoch: [52][300/469]\tTime 0.010 (0.009)\tData 0.004 (0.002)\tLoss 2.0873 (2.2308)\tPrec@1 19.531 (15.015)\tPrec@5 74.219 (59.645)\n",
      "TRAINING - Epoch: [52][400/469]\tTime 0.015 (0.009)\tData 0.004 (0.002)\tLoss 2.2339 (2.2301)\tPrec@1 12.500 (15.101)\tPrec@5 52.344 (59.659)\n",
      "EVALUATING - Epoch: [52][0/79]\tTime 0.053 (0.053)\tData 0.048 (0.048)\tLoss 2.3514 (2.3514)\tPrec@1 16.406 (16.406)\tPrec@5 46.875 (46.875)\n",
      "\n",
      " Epoch: 53\tTraining Loss 2.2304 \tTraining Prec@1 15.113 \tTraining Prec@5 59.655 \tValidation Loss 2.4521 \tValidation Prec@1 10.980 \tValidation Prec@5 50.790 \n",
      "\n",
      "TRAINING - Epoch: [53][0/469]\tTime 0.080 (0.080)\tData 0.062 (0.062)\tLoss 2.2551 (2.2551)\tPrec@1 12.500 (12.500)\tPrec@5 64.844 (64.844)\n",
      "TRAINING - Epoch: [53][100/469]\tTime 0.010 (0.011)\tData 0.003 (0.003)\tLoss 2.2193 (2.2250)\tPrec@1 15.625 (15.316)\tPrec@5 64.844 (60.226)\n",
      "TRAINING - Epoch: [53][200/469]\tTime 0.008 (0.010)\tData 0.000 (0.003)\tLoss 2.1865 (2.2277)\tPrec@1 21.875 (15.124)\tPrec@5 64.844 (59.907)\n",
      "TRAINING - Epoch: [53][300/469]\tTime 0.013 (0.010)\tData 0.004 (0.003)\tLoss 2.4276 (2.2275)\tPrec@1 10.938 (15.259)\tPrec@5 43.750 (59.728)\n",
      "TRAINING - Epoch: [53][400/469]\tTime 0.015 (0.010)\tData 0.004 (0.003)\tLoss 2.2299 (2.2302)\tPrec@1 10.156 (15.272)\tPrec@5 60.938 (59.564)\n",
      "EVALUATING - Epoch: [53][0/79]\tTime 0.051 (0.051)\tData 0.045 (0.045)\tLoss 2.3653 (2.3653)\tPrec@1 10.156 (10.156)\tPrec@5 42.969 (42.969)\n",
      "\n",
      " Epoch: 54\tTraining Loss 2.2311 \tTraining Prec@1 15.323 \tTraining Prec@5 59.535 \tValidation Loss 2.3404 \tValidation Prec@1 9.990 \tValidation Prec@5 50.780 \n",
      "\n",
      "TRAINING - Epoch: [54][0/469]\tTime 0.072 (0.072)\tData 0.057 (0.057)\tLoss 2.2919 (2.2919)\tPrec@1 12.500 (12.500)\tPrec@5 53.125 (53.125)\n",
      "TRAINING - Epoch: [54][100/469]\tTime 0.014 (0.011)\tData 0.004 (0.003)\tLoss 2.2175 (2.2323)\tPrec@1 16.406 (14.465)\tPrec@5 53.125 (59.421)\n",
      "TRAINING - Epoch: [54][200/469]\tTime 0.010 (0.011)\tData 0.002 (0.002)\tLoss 2.1890 (2.2311)\tPrec@1 17.188 (14.984)\tPrec@5 63.281 (59.771)\n",
      "TRAINING - Epoch: [54][300/469]\tTime 0.013 (0.011)\tData 0.004 (0.002)\tLoss 2.2967 (2.2273)\tPrec@1 12.500 (15.202)\tPrec@5 59.375 (60.021)\n",
      "TRAINING - Epoch: [54][400/469]\tTime 0.009 (0.011)\tData 0.002 (0.002)\tLoss 2.2395 (2.2282)\tPrec@1 17.969 (15.202)\tPrec@5 53.125 (59.870)\n",
      "EVALUATING - Epoch: [54][0/79]\tTime 0.050 (0.050)\tData 0.045 (0.045)\tLoss 2.2781 (2.2781)\tPrec@1 10.156 (10.156)\tPrec@5 48.438 (48.438)\n",
      "\n",
      " Epoch: 55\tTraining Loss 2.2290 \tTraining Prec@1 15.057 \tTraining Prec@5 59.847 \tValidation Loss 2.2647 \tValidation Prec@1 10.130 \tValidation Prec@5 54.980 \n",
      "\n",
      "TRAINING - Epoch: [55][0/469]\tTime 0.146 (0.146)\tData 0.126 (0.126)\tLoss 2.2554 (2.2554)\tPrec@1 7.031 (7.031)\tPrec@5 64.062 (64.062)\n",
      "TRAINING - Epoch: [55][100/469]\tTime 0.009 (0.011)\tData 0.003 (0.004)\tLoss 2.2174 (2.2307)\tPrec@1 14.062 (14.821)\tPrec@5 57.031 (60.040)\n",
      "TRAINING - Epoch: [55][200/469]\tTime 0.015 (0.011)\tData 0.005 (0.003)\tLoss 2.2275 (2.2300)\tPrec@1 15.625 (15.302)\tPrec@5 60.938 (59.923)\n",
      "TRAINING - Epoch: [55][300/469]\tTime 0.012 (0.011)\tData 0.005 (0.003)\tLoss 2.1897 (2.2332)\tPrec@1 18.750 (15.210)\tPrec@5 63.281 (59.679)\n",
      "TRAINING - Epoch: [55][400/469]\tTime 0.011 (0.010)\tData 0.004 (0.003)\tLoss 2.2053 (2.2312)\tPrec@1 21.094 (15.325)\tPrec@5 61.719 (59.802)\n",
      "EVALUATING - Epoch: [55][0/79]\tTime 0.051 (0.051)\tData 0.046 (0.046)\tLoss 2.2162 (2.2162)\tPrec@1 12.500 (12.500)\tPrec@5 55.469 (55.469)\n",
      "\n",
      " Epoch: 56\tTraining Loss 2.2307 \tTraining Prec@1 15.265 \tTraining Prec@5 59.865 \tValidation Loss 2.1754 \tValidation Prec@1 14.380 \tValidation Prec@5 64.830 \n",
      "\n",
      "TRAINING - Epoch: [56][0/469]\tTime 0.072 (0.072)\tData 0.060 (0.060)\tLoss 2.2476 (2.2476)\tPrec@1 12.500 (12.500)\tPrec@5 72.656 (72.656)\n",
      "TRAINING - Epoch: [56][100/469]\tTime 0.008 (0.011)\tData 0.001 (0.002)\tLoss 2.1910 (2.2368)\tPrec@1 14.062 (14.403)\tPrec@5 69.531 (60.164)\n",
      "TRAINING - Epoch: [56][200/469]\tTime 0.010 (0.011)\tData 0.002 (0.002)\tLoss 2.2481 (2.2349)\tPrec@1 13.281 (14.649)\tPrec@5 55.469 (59.593)\n",
      "TRAINING - Epoch: [56][300/469]\tTime 0.009 (0.011)\tData 0.001 (0.002)\tLoss 2.2171 (2.2327)\tPrec@1 12.500 (14.971)\tPrec@5 61.719 (59.598)\n",
      "TRAINING - Epoch: [56][400/469]\tTime 0.012 (0.011)\tData 0.002 (0.002)\tLoss 2.2724 (2.2321)\tPrec@1 14.844 (15.013)\tPrec@5 60.156 (59.743)\n",
      "EVALUATING - Epoch: [56][0/79]\tTime 0.053 (0.053)\tData 0.048 (0.048)\tLoss 3.4784 (3.4784)\tPrec@1 14.062 (14.062)\tPrec@5 48.438 (48.438)\n",
      "\n",
      " Epoch: 57\tTraining Loss 2.2304 \tTraining Prec@1 15.165 \tTraining Prec@5 59.792 \tValidation Loss 3.6149 \tValidation Prec@1 9.150 \tValidation Prec@5 52.380 \n",
      "\n",
      "TRAINING - Epoch: [57][0/469]\tTime 0.079 (0.079)\tData 0.066 (0.066)\tLoss 2.2497 (2.2497)\tPrec@1 20.312 (20.312)\tPrec@5 58.594 (58.594)\n",
      "TRAINING - Epoch: [57][100/469]\tTime 0.014 (0.013)\tData 0.003 (0.006)\tLoss 2.1447 (2.2276)\tPrec@1 13.281 (15.702)\tPrec@5 62.500 (59.978)\n",
      "TRAINING - Epoch: [57][200/469]\tTime 0.019 (0.013)\tData 0.007 (0.005)\tLoss 2.3308 (2.2291)\tPrec@1 10.156 (15.493)\tPrec@5 53.125 (59.795)\n",
      "TRAINING - Epoch: [57][300/469]\tTime 0.021 (0.013)\tData 0.015 (0.005)\tLoss 2.2733 (2.2290)\tPrec@1 14.844 (15.350)\tPrec@5 48.438 (59.907)\n",
      "TRAINING - Epoch: [57][400/469]\tTime 0.014 (0.013)\tData 0.008 (0.005)\tLoss 2.2018 (2.2271)\tPrec@1 8.594 (15.473)\tPrec@5 57.031 (60.113)\n",
      "EVALUATING - Epoch: [57][0/79]\tTime 0.054 (0.054)\tData 0.048 (0.048)\tLoss 2.7014 (2.7014)\tPrec@1 7.031 (7.031)\tPrec@5 57.812 (57.812)\n",
      "\n",
      " Epoch: 58\tTraining Loss 2.2284 \tTraining Prec@1 15.430 \tTraining Prec@5 59.975 \tValidation Loss 2.9404 \tValidation Prec@1 7.670 \tValidation Prec@5 53.940 \n",
      "\n",
      "TRAINING - Epoch: [58][0/469]\tTime 0.084 (0.084)\tData 0.069 (0.069)\tLoss 2.1634 (2.1634)\tPrec@1 26.562 (26.562)\tPrec@5 64.844 (64.844)\n",
      "TRAINING - Epoch: [58][100/469]\tTime 0.012 (0.012)\tData 0.005 (0.003)\tLoss 2.3232 (2.2286)\tPrec@1 9.375 (15.486)\tPrec@5 53.125 (60.435)\n",
      "TRAINING - Epoch: [58][200/469]\tTime 0.016 (0.012)\tData 0.002 (0.003)\tLoss 2.1822 (2.2250)\tPrec@1 21.875 (15.718)\tPrec@5 69.531 (60.525)\n",
      "TRAINING - Epoch: [58][300/469]\tTime 0.014 (0.012)\tData 0.002 (0.002)\tLoss 2.2648 (2.2236)\tPrec@1 11.719 (15.690)\tPrec@5 59.375 (60.538)\n",
      "TRAINING - Epoch: [58][400/469]\tTime 0.009 (0.012)\tData 0.001 (0.003)\tLoss 2.1658 (2.2273)\tPrec@1 26.562 (15.498)\tPrec@5 60.156 (60.084)\n",
      "EVALUATING - Epoch: [58][0/79]\tTime 0.053 (0.053)\tData 0.046 (0.046)\tLoss 2.9136 (2.9136)\tPrec@1 10.938 (10.938)\tPrec@5 50.781 (50.781)\n",
      "\n",
      " Epoch: 59\tTraining Loss 2.2290 \tTraining Prec@1 15.362 \tTraining Prec@5 59.832 \tValidation Loss 3.0240 \tValidation Prec@1 10.210 \tValidation Prec@5 49.880 \n",
      "\n",
      "TRAINING - Epoch: [59][0/469]\tTime 0.105 (0.105)\tData 0.087 (0.087)\tLoss 2.3825 (2.3825)\tPrec@1 10.938 (10.938)\tPrec@5 43.750 (43.750)\n",
      "TRAINING - Epoch: [59][100/469]\tTime 0.006 (0.014)\tData 0.000 (0.005)\tLoss 2.3007 (2.2384)\tPrec@1 17.969 (14.534)\tPrec@5 55.469 (59.916)\n",
      "TRAINING - Epoch: [59][200/469]\tTime 0.006 (0.013)\tData 0.000 (0.005)\tLoss 2.2207 (2.2310)\tPrec@1 21.875 (15.248)\tPrec@5 67.969 (60.024)\n",
      "TRAINING - Epoch: [59][300/469]\tTime 0.006 (0.012)\tData 0.000 (0.005)\tLoss 2.2241 (2.2298)\tPrec@1 8.594 (15.285)\tPrec@5 62.500 (59.884)\n",
      "TRAINING - Epoch: [59][400/469]\tTime 0.006 (0.012)\tData 0.000 (0.005)\tLoss 2.2146 (2.2306)\tPrec@1 20.312 (15.280)\tPrec@5 61.719 (59.778)\n",
      "EVALUATING - Epoch: [59][0/79]\tTime 0.058 (0.058)\tData 0.050 (0.050)\tLoss 2.3325 (2.3325)\tPrec@1 9.375 (9.375)\tPrec@5 44.531 (44.531)\n",
      "\n",
      " Epoch: 60\tTraining Loss 2.2308 \tTraining Prec@1 15.262 \tTraining Prec@5 59.727 \tValidation Loss 2.2980 \tValidation Prec@1 10.130 \tValidation Prec@5 52.270 \n",
      "\n",
      "TRAINING - Epoch: [60][0/469]\tTime 0.064 (0.064)\tData 0.053 (0.053)\tLoss 2.2255 (2.2255)\tPrec@1 14.062 (14.062)\tPrec@5 67.188 (67.188)\n",
      "TRAINING - Epoch: [60][100/469]\tTime 0.006 (0.010)\tData 0.000 (0.004)\tLoss 2.2519 (2.2259)\tPrec@1 13.281 (15.006)\tPrec@5 61.719 (60.017)\n",
      "TRAINING - Epoch: [60][200/469]\tTime 0.006 (0.010)\tData 0.000 (0.004)\tLoss 2.2271 (2.2251)\tPrec@1 10.938 (15.353)\tPrec@5 60.156 (60.110)\n",
      "TRAINING - Epoch: [60][300/469]\tTime 0.007 (0.010)\tData 0.000 (0.004)\tLoss 2.2464 (2.2287)\tPrec@1 14.844 (15.243)\tPrec@5 58.594 (60.008)\n",
      "TRAINING - Epoch: [60][400/469]\tTime 0.006 (0.011)\tData 0.000 (0.004)\tLoss 2.2350 (2.2281)\tPrec@1 17.969 (15.185)\tPrec@5 50.781 (60.063)\n",
      "EVALUATING - Epoch: [60][0/79]\tTime 0.052 (0.052)\tData 0.045 (0.045)\tLoss 2.2584 (2.2584)\tPrec@1 10.938 (10.938)\tPrec@5 49.219 (49.219)\n",
      "\n",
      " Epoch: 61\tTraining Loss 2.2280 \tTraining Prec@1 15.267 \tTraining Prec@5 60.123 \tValidation Loss 2.2388 \tValidation Prec@1 10.560 \tValidation Prec@5 57.010 \n",
      "\n",
      "TRAINING - Epoch: [61][0/469]\tTime 0.073 (0.073)\tData 0.061 (0.061)\tLoss 2.2371 (2.2371)\tPrec@1 15.625 (15.625)\tPrec@5 65.625 (65.625)\n",
      "TRAINING - Epoch: [61][100/469]\tTime 0.007 (0.011)\tData 0.000 (0.005)\tLoss 2.2503 (2.2183)\tPrec@1 11.719 (15.826)\tPrec@5 62.500 (59.800)\n",
      "TRAINING - Epoch: [61][200/469]\tTime 0.006 (0.011)\tData 0.000 (0.004)\tLoss 2.3853 (2.2232)\tPrec@1 8.594 (15.333)\tPrec@5 42.188 (59.787)\n",
      "TRAINING - Epoch: [61][300/469]\tTime 0.007 (0.011)\tData 0.000 (0.004)\tLoss 2.2602 (2.2209)\tPrec@1 11.719 (15.643)\tPrec@5 55.469 (59.819)\n",
      "TRAINING - Epoch: [61][400/469]\tTime 0.007 (0.011)\tData 0.001 (0.004)\tLoss 2.2434 (2.2253)\tPrec@1 13.281 (15.286)\tPrec@5 59.375 (59.782)\n",
      "EVALUATING - Epoch: [61][0/79]\tTime 0.052 (0.052)\tData 0.047 (0.047)\tLoss 2.2735 (2.2735)\tPrec@1 14.844 (14.844)\tPrec@5 48.438 (48.438)\n",
      "\n",
      " Epoch: 62\tTraining Loss 2.2265 \tTraining Prec@1 15.303 \tTraining Prec@5 59.817 \tValidation Loss 2.3375 \tValidation Prec@1 12.870 \tValidation Prec@5 54.020 \n",
      "\n",
      "TRAINING - Epoch: [62][0/469]\tTime 0.066 (0.066)\tData 0.054 (0.054)\tLoss 2.3170 (2.3170)\tPrec@1 12.500 (12.500)\tPrec@5 52.344 (52.344)\n",
      "TRAINING - Epoch: [62][100/469]\tTime 0.006 (0.011)\tData 0.000 (0.004)\tLoss 2.2442 (2.2362)\tPrec@1 18.750 (14.759)\tPrec@5 52.344 (59.104)\n",
      "TRAINING - Epoch: [62][200/469]\tTime 0.005 (0.010)\tData 0.000 (0.004)\tLoss 2.2755 (2.2315)\tPrec@1 12.500 (15.229)\tPrec@5 53.125 (59.317)\n",
      "TRAINING - Epoch: [62][300/469]\tTime 0.006 (0.010)\tData 0.000 (0.004)\tLoss 2.1836 (2.2300)\tPrec@1 24.219 (15.329)\tPrec@5 60.938 (59.505)\n",
      "TRAINING - Epoch: [62][400/469]\tTime 0.006 (0.011)\tData 0.000 (0.004)\tLoss 2.2579 (2.2341)\tPrec@1 17.969 (15.152)\tPrec@5 59.375 (59.340)\n",
      "EVALUATING - Epoch: [62][0/79]\tTime 0.051 (0.051)\tData 0.046 (0.046)\tLoss 2.1857 (2.1857)\tPrec@1 19.531 (19.531)\tPrec@5 52.344 (52.344)\n",
      "\n",
      " Epoch: 63\tTraining Loss 2.2329 \tTraining Prec@1 15.212 \tTraining Prec@5 59.368 \tValidation Loss 2.1918 \tValidation Prec@1 18.500 \tValidation Prec@5 58.110 \n",
      "\n",
      "TRAINING - Epoch: [63][0/469]\tTime 0.073 (0.073)\tData 0.059 (0.059)\tLoss 2.2459 (2.2459)\tPrec@1 15.625 (15.625)\tPrec@5 60.156 (60.156)\n",
      "TRAINING - Epoch: [63][100/469]\tTime 0.011 (0.011)\tData 0.003 (0.003)\tLoss 2.2550 (2.2208)\tPrec@1 14.844 (15.710)\tPrec@5 51.562 (60.056)\n",
      "TRAINING - Epoch: [63][200/469]\tTime 0.009 (0.010)\tData 0.003 (0.003)\tLoss 2.2402 (2.2272)\tPrec@1 14.062 (15.229)\tPrec@5 60.156 (60.001)\n",
      "TRAINING - Epoch: [63][300/469]\tTime 0.010 (0.010)\tData 0.003 (0.003)\tLoss 2.2407 (2.2281)\tPrec@1 15.625 (15.290)\tPrec@5 58.594 (59.666)\n",
      "TRAINING - Epoch: [63][400/469]\tTime 0.014 (0.010)\tData 0.004 (0.003)\tLoss 2.2372 (2.2282)\tPrec@1 16.406 (15.323)\tPrec@5 56.250 (59.792)\n",
      "EVALUATING - Epoch: [63][0/79]\tTime 0.059 (0.059)\tData 0.053 (0.053)\tLoss 2.3238 (2.3238)\tPrec@1 10.938 (10.938)\tPrec@5 49.219 (49.219)\n",
      "\n",
      " Epoch: 64\tTraining Loss 2.2297 \tTraining Prec@1 15.312 \tTraining Prec@5 59.825 \tValidation Loss 2.3950 \tValidation Prec@1 12.350 \tValidation Prec@5 52.570 \n",
      "\n",
      "TRAINING - Epoch: [64][0/469]\tTime 0.073 (0.073)\tData 0.052 (0.052)\tLoss 2.2670 (2.2670)\tPrec@1 10.938 (10.938)\tPrec@5 59.375 (59.375)\n",
      "TRAINING - Epoch: [64][100/469]\tTime 0.016 (0.013)\tData 0.007 (0.004)\tLoss 2.1867 (2.2330)\tPrec@1 11.719 (14.944)\tPrec@5 67.969 (60.226)\n",
      "TRAINING - Epoch: [64][200/469]\tTime 0.008 (0.011)\tData 0.002 (0.003)\tLoss 2.1831 (2.2262)\tPrec@1 14.062 (15.333)\tPrec@5 59.375 (59.993)\n",
      "TRAINING - Epoch: [64][300/469]\tTime 0.011 (0.011)\tData 0.002 (0.003)\tLoss 2.2612 (2.2273)\tPrec@1 17.969 (15.205)\tPrec@5 55.469 (59.943)\n",
      "TRAINING - Epoch: [64][400/469]\tTime 0.010 (0.011)\tData 0.002 (0.003)\tLoss 2.1779 (2.2299)\tPrec@1 16.406 (14.978)\tPrec@5 64.062 (59.907)\n",
      "EVALUATING - Epoch: [64][0/79]\tTime 0.068 (0.068)\tData 0.057 (0.057)\tLoss 2.2672 (2.2672)\tPrec@1 16.406 (16.406)\tPrec@5 55.469 (55.469)\n",
      "\n",
      " Epoch: 65\tTraining Loss 2.2305 \tTraining Prec@1 14.918 \tTraining Prec@5 59.960 \tValidation Loss 2.2848 \tValidation Prec@1 15.330 \tValidation Prec@5 57.440 \n",
      "\n",
      "TRAINING - Epoch: [65][0/469]\tTime 0.073 (0.073)\tData 0.058 (0.058)\tLoss 2.1955 (2.1955)\tPrec@1 10.156 (10.156)\tPrec@5 60.156 (60.156)\n",
      "TRAINING - Epoch: [65][100/469]\tTime 0.008 (0.013)\tData 0.000 (0.003)\tLoss 2.2417 (2.2306)\tPrec@1 13.281 (15.261)\tPrec@5 63.281 (60.002)\n",
      "TRAINING - Epoch: [65][200/469]\tTime 0.010 (0.013)\tData 0.000 (0.003)\tLoss 2.2361 (2.2299)\tPrec@1 13.281 (15.368)\tPrec@5 60.156 (59.639)\n",
      "TRAINING - Epoch: [65][300/469]\tTime 0.009 (0.013)\tData 0.003 (0.003)\tLoss 2.2694 (2.2284)\tPrec@1 14.844 (15.498)\tPrec@5 57.031 (60.013)\n",
      "TRAINING - Epoch: [65][400/469]\tTime 0.019 (0.012)\tData 0.003 (0.003)\tLoss 2.2071 (2.2282)\tPrec@1 12.500 (15.395)\tPrec@5 60.156 (59.804)\n",
      "EVALUATING - Epoch: [65][0/79]\tTime 0.053 (0.053)\tData 0.047 (0.047)\tLoss 2.2125 (2.2125)\tPrec@1 10.938 (10.938)\tPrec@5 56.250 (56.250)\n",
      "\n",
      " Epoch: 66\tTraining Loss 2.2286 \tTraining Prec@1 15.347 \tTraining Prec@5 59.815 \tValidation Loss 2.2043 \tValidation Prec@1 10.850 \tValidation Prec@5 60.260 \n",
      "\n",
      "TRAINING - Epoch: [66][0/469]\tTime 0.078 (0.078)\tData 0.067 (0.067)\tLoss 2.2340 (2.2340)\tPrec@1 13.281 (13.281)\tPrec@5 66.406 (66.406)\n",
      "TRAINING - Epoch: [66][100/469]\tTime 0.017 (0.013)\tData 0.009 (0.004)\tLoss 2.2481 (2.2265)\tPrec@1 18.750 (15.269)\tPrec@5 60.938 (60.133)\n",
      "TRAINING - Epoch: [66][200/469]\tTime 0.015 (0.012)\tData 0.009 (0.004)\tLoss 2.3066 (2.2316)\tPrec@1 13.281 (15.077)\tPrec@5 60.156 (59.900)\n",
      "TRAINING - Epoch: [66][300/469]\tTime 0.011 (0.011)\tData 0.004 (0.004)\tLoss 2.2430 (2.2271)\tPrec@1 12.500 (15.378)\tPrec@5 60.156 (60.206)\n",
      "TRAINING - Epoch: [66][400/469]\tTime 0.016 (0.011)\tData 0.008 (0.004)\tLoss 2.3100 (2.2283)\tPrec@1 12.500 (15.372)\tPrec@5 50.781 (60.026)\n",
      "EVALUATING - Epoch: [66][0/79]\tTime 0.050 (0.050)\tData 0.045 (0.045)\tLoss 2.4417 (2.4417)\tPrec@1 14.844 (14.844)\tPrec@5 46.094 (46.094)\n",
      "\n",
      " Epoch: 67\tTraining Loss 2.2267 \tTraining Prec@1 15.510 \tTraining Prec@5 60.037 \tValidation Loss 2.5893 \tValidation Prec@1 11.580 \tValidation Prec@5 49.800 \n",
      "\n",
      "TRAINING - Epoch: [67][0/469]\tTime 0.076 (0.076)\tData 0.061 (0.061)\tLoss 2.1857 (2.1857)\tPrec@1 21.875 (21.875)\tPrec@5 61.719 (61.719)\n",
      "TRAINING - Epoch: [67][100/469]\tTime 0.013 (0.011)\tData 0.004 (0.003)\tLoss 2.2565 (2.2279)\tPrec@1 14.844 (16.120)\tPrec@5 40.625 (60.497)\n",
      "TRAINING - Epoch: [67][200/469]\tTime 0.009 (0.011)\tData 0.004 (0.003)\tLoss 2.2203 (2.2298)\tPrec@1 20.312 (15.501)\tPrec@5 57.031 (60.176)\n",
      "TRAINING - Epoch: [67][300/469]\tTime 0.012 (0.010)\tData 0.006 (0.003)\tLoss 2.2496 (2.2258)\tPrec@1 13.281 (15.565)\tPrec@5 60.938 (60.234)\n",
      "TRAINING - Epoch: [67][400/469]\tTime 0.012 (0.010)\tData 0.005 (0.003)\tLoss 2.1709 (2.2275)\tPrec@1 21.875 (15.401)\tPrec@5 64.062 (60.104)\n",
      "EVALUATING - Epoch: [67][0/79]\tTime 0.051 (0.051)\tData 0.045 (0.045)\tLoss 2.9328 (2.9328)\tPrec@1 10.938 (10.938)\tPrec@5 42.188 (42.188)\n",
      "\n",
      " Epoch: 68\tTraining Loss 2.2271 \tTraining Prec@1 15.362 \tTraining Prec@5 60.052 \tValidation Loss 2.9243 \tValidation Prec@1 10.110 \tValidation Prec@5 51.020 \n",
      "\n",
      "TRAINING - Epoch: [68][0/469]\tTime 0.158 (0.158)\tData 0.080 (0.080)\tLoss 2.2818 (2.2818)\tPrec@1 14.844 (14.844)\tPrec@5 48.438 (48.438)\n",
      "TRAINING - Epoch: [68][100/469]\tTime 0.006 (0.014)\tData 0.000 (0.004)\tLoss 2.1829 (2.2323)\tPrec@1 17.969 (14.929)\tPrec@5 61.719 (59.445)\n",
      "TRAINING - Epoch: [68][200/469]\tTime 0.009 (0.012)\tData 0.002 (0.003)\tLoss 2.1781 (2.2346)\tPrec@1 13.281 (14.863)\tPrec@5 73.438 (59.892)\n",
      "TRAINING - Epoch: [68][300/469]\tTime 0.014 (0.012)\tData 0.002 (0.003)\tLoss 2.2623 (2.2333)\tPrec@1 10.938 (14.937)\tPrec@5 57.812 (59.866)\n",
      "TRAINING - Epoch: [68][400/469]\tTime 0.009 (0.012)\tData 0.001 (0.003)\tLoss 2.1713 (2.2318)\tPrec@1 18.750 (15.078)\tPrec@5 63.281 (59.878)\n",
      "EVALUATING - Epoch: [68][0/79]\tTime 0.063 (0.063)\tData 0.057 (0.057)\tLoss 2.6039 (2.6039)\tPrec@1 12.500 (12.500)\tPrec@5 48.438 (48.438)\n",
      "\n",
      " Epoch: 69\tTraining Loss 2.2309 \tTraining Prec@1 15.082 \tTraining Prec@5 59.860 \tValidation Loss 2.7471 \tValidation Prec@1 10.180 \tValidation Prec@5 48.200 \n",
      "\n",
      "TRAINING - Epoch: [69][0/469]\tTime 0.077 (0.077)\tData 0.062 (0.062)\tLoss 2.1552 (2.1552)\tPrec@1 17.969 (17.969)\tPrec@5 66.406 (66.406)\n",
      "TRAINING - Epoch: [69][100/469]\tTime 0.015 (0.014)\tData 0.002 (0.003)\tLoss 2.1796 (2.2200)\tPrec@1 21.875 (16.019)\tPrec@5 60.156 (59.762)\n",
      "TRAINING - Epoch: [69][200/469]\tTime 0.009 (0.014)\tData 0.002 (0.002)\tLoss 2.1204 (2.2259)\tPrec@1 29.688 (15.617)\tPrec@5 67.188 (59.950)\n",
      "TRAINING - Epoch: [69][300/469]\tTime 0.012 (0.013)\tData 0.002 (0.002)\tLoss 2.2955 (2.2261)\tPrec@1 17.188 (15.436)\tPrec@5 64.062 (60.001)\n",
      "TRAINING - Epoch: [69][400/469]\tTime 0.014 (0.013)\tData 0.001 (0.002)\tLoss 2.2261 (2.2260)\tPrec@1 10.938 (15.395)\tPrec@5 60.938 (59.804)\n",
      "EVALUATING - Epoch: [69][0/79]\tTime 0.053 (0.053)\tData 0.047 (0.047)\tLoss 2.3254 (2.3254)\tPrec@1 10.938 (10.938)\tPrec@5 44.531 (44.531)\n",
      "\n",
      " Epoch: 70\tTraining Loss 2.2271 \tTraining Prec@1 15.373 \tTraining Prec@5 59.792 \tValidation Loss 2.2918 \tValidation Prec@1 12.370 \tValidation Prec@5 52.050 \n",
      "\n",
      "TRAINING - Epoch: [70][0/469]\tTime 0.072 (0.072)\tData 0.054 (0.054)\tLoss 2.2048 (2.2048)\tPrec@1 19.531 (19.531)\tPrec@5 56.250 (56.250)\n",
      "TRAINING - Epoch: [70][100/469]\tTime 0.010 (0.011)\tData 0.002 (0.002)\tLoss 2.1911 (2.2291)\tPrec@1 23.438 (15.122)\tPrec@5 57.812 (59.421)\n",
      "TRAINING - Epoch: [70][200/469]\tTime 0.009 (0.010)\tData 0.002 (0.002)\tLoss 2.1442 (2.2332)\tPrec@1 21.875 (15.143)\tPrec@5 57.812 (59.748)\n",
      "TRAINING - Epoch: [70][300/469]\tTime 0.015 (0.010)\tData 0.003 (0.002)\tLoss 2.1998 (2.2333)\tPrec@1 17.969 (15.054)\tPrec@5 57.812 (59.666)\n",
      "TRAINING - Epoch: [70][400/469]\tTime 0.007 (0.011)\tData 0.000 (0.002)\tLoss 2.2833 (2.2316)\tPrec@1 11.719 (15.117)\tPrec@5 51.562 (59.730)\n",
      "EVALUATING - Epoch: [70][0/79]\tTime 0.054 (0.054)\tData 0.049 (0.049)\tLoss 2.3141 (2.3141)\tPrec@1 9.375 (9.375)\tPrec@5 46.094 (46.094)\n",
      "\n",
      " Epoch: 71\tTraining Loss 2.2324 \tTraining Prec@1 15.182 \tTraining Prec@5 59.692 \tValidation Loss 2.2751 \tValidation Prec@1 10.130 \tValidation Prec@5 54.160 \n",
      "\n",
      "TRAINING - Epoch: [71][0/469]\tTime 0.078 (0.078)\tData 0.065 (0.065)\tLoss 2.3284 (2.3284)\tPrec@1 9.375 (9.375)\tPrec@5 60.938 (60.938)\n",
      "TRAINING - Epoch: [71][100/469]\tTime 0.008 (0.012)\tData 0.002 (0.004)\tLoss 2.1757 (2.2311)\tPrec@1 21.875 (14.952)\tPrec@5 57.031 (60.002)\n",
      "TRAINING - Epoch: [71][200/469]\tTime 0.008 (0.012)\tData 0.002 (0.003)\tLoss 2.1850 (2.2286)\tPrec@1 12.500 (14.984)\tPrec@5 60.938 (60.032)\n",
      "TRAINING - Epoch: [71][300/469]\tTime 0.012 (0.012)\tData 0.002 (0.003)\tLoss 2.1740 (2.2326)\tPrec@1 18.750 (14.940)\tPrec@5 66.406 (59.868)\n",
      "TRAINING - Epoch: [71][400/469]\tTime 0.007 (0.012)\tData 0.000 (0.003)\tLoss 2.2078 (2.2322)\tPrec@1 18.750 (14.852)\tPrec@5 65.625 (59.883)\n",
      "EVALUATING - Epoch: [71][0/79]\tTime 0.058 (0.058)\tData 0.052 (0.052)\tLoss 2.2468 (2.2468)\tPrec@1 19.531 (19.531)\tPrec@5 50.000 (50.000)\n",
      "\n",
      " Epoch: 72\tTraining Loss 2.2320 \tTraining Prec@1 14.943 \tTraining Prec@5 59.853 \tValidation Loss 2.2197 \tValidation Prec@1 20.170 \tValidation Prec@5 57.190 \n",
      "\n",
      "TRAINING - Epoch: [72][0/469]\tTime 0.109 (0.109)\tData 0.081 (0.081)\tLoss 2.2169 (2.2169)\tPrec@1 18.750 (18.750)\tPrec@5 56.250 (56.250)\n",
      "TRAINING - Epoch: [72][100/469]\tTime 0.010 (0.014)\tData 0.002 (0.003)\tLoss 2.1837 (2.2314)\tPrec@1 24.219 (15.029)\tPrec@5 53.906 (59.653)\n",
      "TRAINING - Epoch: [72][200/469]\tTime 0.011 (0.012)\tData 0.003 (0.003)\tLoss 2.2016 (2.2331)\tPrec@1 12.500 (14.933)\tPrec@5 63.281 (59.771)\n",
      "TRAINING - Epoch: [72][300/469]\tTime 0.010 (0.012)\tData 0.000 (0.003)\tLoss 2.1487 (2.2337)\tPrec@1 15.625 (14.935)\tPrec@5 64.062 (59.725)\n",
      "TRAINING - Epoch: [72][400/469]\tTime 0.008 (0.012)\tData 0.002 (0.003)\tLoss 2.2201 (2.2324)\tPrec@1 23.438 (15.089)\tPrec@5 54.688 (59.656)\n",
      "EVALUATING - Epoch: [72][0/79]\tTime 0.051 (0.051)\tData 0.046 (0.046)\tLoss 2.3124 (2.3124)\tPrec@1 11.719 (11.719)\tPrec@5 43.750 (43.750)\n",
      "\n",
      " Epoch: 73\tTraining Loss 2.2313 \tTraining Prec@1 15.067 \tTraining Prec@5 59.680 \tValidation Loss 2.3124 \tValidation Prec@1 11.870 \tValidation Prec@5 51.580 \n",
      "\n",
      "TRAINING - Epoch: [73][0/469]\tTime 0.067 (0.067)\tData 0.053 (0.053)\tLoss 2.2508 (2.2508)\tPrec@1 17.188 (17.188)\tPrec@5 60.938 (60.938)\n",
      "TRAINING - Epoch: [73][100/469]\tTime 0.012 (0.011)\tData 0.005 (0.003)\tLoss 2.2187 (2.2356)\tPrec@1 17.969 (15.091)\tPrec@5 55.469 (59.615)\n",
      "TRAINING - Epoch: [73][200/469]\tTime 0.010 (0.010)\tData 0.003 (0.003)\tLoss 2.1919 (2.2279)\tPrec@1 23.438 (15.435)\tPrec@5 61.719 (59.935)\n",
      "TRAINING - Epoch: [73][300/469]\tTime 0.014 (0.010)\tData 0.002 (0.003)\tLoss 2.2345 (2.2281)\tPrec@1 10.156 (15.500)\tPrec@5 53.906 (59.845)\n",
      "TRAINING - Epoch: [73][400/469]\tTime 0.008 (0.010)\tData 0.001 (0.002)\tLoss 2.2569 (2.2265)\tPrec@1 10.938 (15.448)\tPrec@5 53.906 (60.020)\n",
      "EVALUATING - Epoch: [73][0/79]\tTime 0.052 (0.052)\tData 0.046 (0.046)\tLoss 2.2067 (2.2067)\tPrec@1 20.312 (20.312)\tPrec@5 53.906 (53.906)\n",
      "\n",
      " Epoch: 74\tTraining Loss 2.2264 \tTraining Prec@1 15.365 \tTraining Prec@5 60.010 \tValidation Loss 2.1820 \tValidation Prec@1 20.920 \tValidation Prec@5 60.000 \n",
      "\n",
      "TRAINING - Epoch: [74][0/469]\tTime 0.070 (0.070)\tData 0.058 (0.058)\tLoss 2.1257 (2.1257)\tPrec@1 26.562 (26.562)\tPrec@5 60.938 (60.938)\n",
      "TRAINING - Epoch: [74][100/469]\tTime 0.014 (0.012)\tData 0.007 (0.004)\tLoss 2.2928 (2.2346)\tPrec@1 9.375 (14.906)\tPrec@5 56.250 (59.460)\n",
      "TRAINING - Epoch: [74][200/469]\tTime 0.011 (0.011)\tData 0.004 (0.004)\tLoss 2.2068 (2.2296)\tPrec@1 20.312 (14.906)\tPrec@5 55.469 (59.993)\n",
      "TRAINING - Epoch: [74][300/469]\tTime 0.016 (0.011)\tData 0.010 (0.004)\tLoss 2.1477 (2.2295)\tPrec@1 24.219 (14.994)\tPrec@5 67.969 (59.930)\n",
      "TRAINING - Epoch: [74][400/469]\tTime 0.021 (0.011)\tData 0.013 (0.004)\tLoss 2.1969 (2.2292)\tPrec@1 19.531 (15.159)\tPrec@5 57.031 (59.921)\n",
      "EVALUATING - Epoch: [74][0/79]\tTime 0.056 (0.056)\tData 0.049 (0.049)\tLoss 2.4459 (2.4459)\tPrec@1 8.594 (8.594)\tPrec@5 43.750 (43.750)\n",
      "\n",
      " Epoch: 75\tTraining Loss 2.2287 \tTraining Prec@1 15.237 \tTraining Prec@5 59.842 \tValidation Loss 2.4347 \tValidation Prec@1 9.980 \tValidation Prec@5 48.990 \n",
      "\n",
      "TRAINING - Epoch: [75][0/469]\tTime 0.071 (0.071)\tData 0.057 (0.057)\tLoss 2.1519 (2.1519)\tPrec@1 17.969 (17.969)\tPrec@5 64.844 (64.844)\n",
      "TRAINING - Epoch: [75][100/469]\tTime 0.011 (0.011)\tData 0.005 (0.003)\tLoss 2.1982 (2.2341)\tPrec@1 11.719 (14.813)\tPrec@5 60.938 (59.940)\n",
      "TRAINING - Epoch: [75][200/469]\tTime 0.010 (0.011)\tData 0.003 (0.003)\tLoss 2.2423 (2.2335)\tPrec@1 13.281 (15.058)\tPrec@5 52.344 (59.694)\n",
      "TRAINING - Epoch: [75][300/469]\tTime 0.013 (0.011)\tData 0.006 (0.004)\tLoss 2.2434 (2.2326)\tPrec@1 15.625 (15.033)\tPrec@5 60.156 (59.609)\n",
      "TRAINING - Epoch: [75][400/469]\tTime 0.010 (0.011)\tData 0.004 (0.004)\tLoss 2.2890 (2.2306)\tPrec@1 7.031 (15.099)\tPrec@5 57.031 (59.790)\n",
      "EVALUATING - Epoch: [75][0/79]\tTime 0.056 (0.056)\tData 0.050 (0.050)\tLoss 2.9721 (2.9721)\tPrec@1 15.625 (15.625)\tPrec@5 55.469 (55.469)\n",
      "\n",
      " Epoch: 76\tTraining Loss 2.2296 \tTraining Prec@1 15.100 \tTraining Prec@5 59.710 \tValidation Loss 3.2462 \tValidation Prec@1 9.830 \tValidation Prec@5 49.800 \n",
      "\n",
      "TRAINING - Epoch: [76][0/469]\tTime 0.072 (0.072)\tData 0.059 (0.059)\tLoss 2.2493 (2.2493)\tPrec@1 17.969 (17.969)\tPrec@5 59.375 (59.375)\n",
      "TRAINING - Epoch: [76][100/469]\tTime 0.010 (0.013)\tData 0.002 (0.003)\tLoss 2.2608 (2.2301)\tPrec@1 14.844 (15.176)\tPrec@5 53.906 (60.334)\n",
      "TRAINING - Epoch: [76][200/469]\tTime 0.013 (0.012)\tData 0.002 (0.003)\tLoss 2.1443 (2.2284)\tPrec@1 21.094 (14.964)\tPrec@5 54.688 (60.047)\n",
      "TRAINING - Epoch: [76][300/469]\tTime 0.009 (0.011)\tData 0.001 (0.002)\tLoss 2.2523 (2.2270)\tPrec@1 16.406 (15.044)\tPrec@5 53.906 (60.091)\n",
      "TRAINING - Epoch: [76][400/469]\tTime 0.013 (0.011)\tData 0.002 (0.002)\tLoss 2.2263 (2.2264)\tPrec@1 10.938 (15.081)\tPrec@5 58.594 (60.185)\n",
      "EVALUATING - Epoch: [76][0/79]\tTime 0.057 (0.057)\tData 0.050 (0.050)\tLoss 2.4459 (2.4459)\tPrec@1 10.156 (10.156)\tPrec@5 40.625 (40.625)\n",
      "\n",
      " Epoch: 77\tTraining Loss 2.2267 \tTraining Prec@1 15.103 \tTraining Prec@5 60.167 \tValidation Loss 2.4138 \tValidation Prec@1 9.940 \tValidation Prec@5 47.340 \n",
      "\n",
      "TRAINING - Epoch: [77][0/469]\tTime 0.072 (0.072)\tData 0.060 (0.060)\tLoss 2.2253 (2.2253)\tPrec@1 14.844 (14.844)\tPrec@5 62.500 (62.500)\n",
      "TRAINING - Epoch: [77][100/469]\tTime 0.012 (0.012)\tData 0.004 (0.003)\tLoss 2.1832 (2.2255)\tPrec@1 10.938 (15.261)\tPrec@5 67.188 (59.491)\n",
      "TRAINING - Epoch: [77][200/469]\tTime 0.010 (0.012)\tData 0.001 (0.002)\tLoss 2.2180 (2.2320)\tPrec@1 10.938 (14.840)\tPrec@5 58.594 (59.247)\n",
      "TRAINING - Epoch: [77][300/469]\tTime 0.010 (0.011)\tData 0.002 (0.002)\tLoss 2.2420 (2.2303)\tPrec@1 17.969 (15.046)\tPrec@5 61.719 (59.598)\n",
      "TRAINING - Epoch: [77][400/469]\tTime 0.021 (0.011)\tData 0.003 (0.002)\tLoss 2.2600 (2.2320)\tPrec@1 16.406 (14.963)\tPrec@5 59.375 (59.722)\n",
      "EVALUATING - Epoch: [77][0/79]\tTime 0.057 (0.057)\tData 0.048 (0.048)\tLoss 4.5613 (4.5613)\tPrec@1 15.625 (15.625)\tPrec@5 55.469 (55.469)\n",
      "\n",
      " Epoch: 78\tTraining Loss 2.2304 \tTraining Prec@1 15.123 \tTraining Prec@5 59.773 \tValidation Loss 4.8056 \tValidation Prec@1 9.820 \tValidation Prec@5 49.860 \n",
      "\n",
      "TRAINING - Epoch: [78][0/469]\tTime 0.076 (0.076)\tData 0.062 (0.062)\tLoss 2.1367 (2.1367)\tPrec@1 25.781 (25.781)\tPrec@5 60.938 (60.938)\n",
      "TRAINING - Epoch: [78][100/469]\tTime 0.015 (0.013)\tData 0.003 (0.004)\tLoss 2.1935 (2.2419)\tPrec@1 21.094 (14.728)\tPrec@5 63.281 (59.545)\n",
      "TRAINING - Epoch: [78][200/469]\tTime 0.007 (0.012)\tData 0.000 (0.003)\tLoss 2.2109 (2.2331)\tPrec@1 19.531 (14.758)\tPrec@5 60.938 (59.884)\n",
      "TRAINING - Epoch: [78][300/469]\tTime 0.013 (0.012)\tData 0.003 (0.003)\tLoss 2.2042 (2.2310)\tPrec@1 13.281 (15.046)\tPrec@5 63.281 (59.884)\n",
      "TRAINING - Epoch: [78][400/469]\tTime 0.014 (0.013)\tData 0.003 (0.003)\tLoss 2.2186 (2.2304)\tPrec@1 14.844 (15.004)\tPrec@5 59.375 (59.846)\n",
      "EVALUATING - Epoch: [78][0/79]\tTime 0.058 (0.058)\tData 0.052 (0.052)\tLoss 2.2895 (2.2895)\tPrec@1 14.062 (14.062)\tPrec@5 46.875 (46.875)\n",
      "\n",
      " Epoch: 79\tTraining Loss 2.2322 \tTraining Prec@1 15.028 \tTraining Prec@5 59.667 \tValidation Loss 2.2982 \tValidation Prec@1 12.580 \tValidation Prec@5 52.120 \n",
      "\n",
      "TRAINING - Epoch: [79][0/469]\tTime 0.070 (0.070)\tData 0.057 (0.057)\tLoss 2.2119 (2.2119)\tPrec@1 20.312 (20.312)\tPrec@5 57.812 (57.812)\n",
      "TRAINING - Epoch: [79][100/469]\tTime 0.011 (0.012)\tData 0.002 (0.003)\tLoss 2.2710 (2.2357)\tPrec@1 12.500 (15.354)\tPrec@5 53.906 (59.329)\n",
      "TRAINING - Epoch: [79][200/469]\tTime 0.013 (0.011)\tData 0.001 (0.003)\tLoss 2.1474 (2.2340)\tPrec@1 26.562 (15.135)\tPrec@5 64.062 (59.713)\n",
      "TRAINING - Epoch: [79][300/469]\tTime 0.013 (0.011)\tData 0.004 (0.003)\tLoss 2.1789 (2.2275)\tPrec@1 20.312 (15.459)\tPrec@5 55.469 (59.884)\n",
      "TRAINING - Epoch: [79][400/469]\tTime 0.008 (0.011)\tData 0.000 (0.003)\tLoss 2.2424 (2.2255)\tPrec@1 11.719 (15.479)\tPrec@5 58.594 (60.115)\n",
      "EVALUATING - Epoch: [79][0/79]\tTime 0.056 (0.056)\tData 0.051 (0.051)\tLoss 2.2710 (2.2710)\tPrec@1 10.156 (10.156)\tPrec@5 49.219 (49.219)\n",
      "\n",
      " Epoch: 80\tTraining Loss 2.2277 \tTraining Prec@1 15.337 \tTraining Prec@5 59.970 \tValidation Loss 2.2423 \tValidation Prec@1 10.140 \tValidation Prec@5 56.470 \n",
      "\n",
      "TRAINING - Epoch: [80][0/469]\tTime 0.077 (0.077)\tData 0.065 (0.065)\tLoss 2.2172 (2.2172)\tPrec@1 10.938 (10.938)\tPrec@5 61.719 (61.719)\n",
      "TRAINING - Epoch: [80][100/469]\tTime 0.019 (0.013)\tData 0.012 (0.006)\tLoss 2.1532 (2.2302)\tPrec@1 20.312 (15.393)\tPrec@5 54.688 (60.110)\n",
      "TRAINING - Epoch: [80][200/469]\tTime 0.009 (0.012)\tData 0.002 (0.006)\tLoss 2.2678 (2.2283)\tPrec@1 15.625 (15.489)\tPrec@5 63.281 (60.382)\n",
      "TRAINING - Epoch: [80][300/469]\tTime 0.011 (0.012)\tData 0.005 (0.006)\tLoss 2.2615 (2.2257)\tPrec@1 10.938 (15.464)\tPrec@5 65.625 (60.455)\n",
      "TRAINING - Epoch: [80][400/469]\tTime 0.018 (0.012)\tData 0.012 (0.006)\tLoss 2.2770 (2.2258)\tPrec@1 11.719 (15.475)\tPrec@5 57.812 (60.115)\n",
      "EVALUATING - Epoch: [80][0/79]\tTime 0.052 (0.052)\tData 0.046 (0.046)\tLoss 2.8255 (2.8255)\tPrec@1 9.375 (9.375)\tPrec@5 51.562 (51.562)\n",
      "\n",
      " Epoch: 81\tTraining Loss 2.2251 \tTraining Prec@1 15.517 \tTraining Prec@5 60.033 \tValidation Loss 2.9974 \tValidation Prec@1 7.110 \tValidation Prec@5 52.250 \n",
      "\n",
      "TRAINING - Epoch: [81][0/469]\tTime 0.073 (0.073)\tData 0.059 (0.059)\tLoss 2.2159 (2.2159)\tPrec@1 16.406 (16.406)\tPrec@5 58.594 (58.594)\n",
      "TRAINING - Epoch: [81][100/469]\tTime 0.006 (0.012)\tData 0.000 (0.005)\tLoss 2.2456 (2.2285)\tPrec@1 12.500 (14.759)\tPrec@5 55.469 (59.429)\n",
      "TRAINING - Epoch: [81][200/469]\tTime 0.010 (0.013)\tData 0.001 (0.006)\tLoss 2.2323 (2.2310)\tPrec@1 14.844 (15.093)\tPrec@5 65.625 (59.480)\n",
      "TRAINING - Epoch: [81][300/469]\tTime 0.008 (0.013)\tData 0.000 (0.005)\tLoss 2.1384 (2.2287)\tPrec@1 12.500 (15.179)\tPrec@5 64.844 (59.884)\n",
      "TRAINING - Epoch: [81][400/469]\tTime 0.008 (0.013)\tData 0.000 (0.005)\tLoss 2.2851 (2.2271)\tPrec@1 11.719 (15.228)\tPrec@5 51.562 (59.870)\n",
      "EVALUATING - Epoch: [81][0/79]\tTime 0.071 (0.071)\tData 0.062 (0.062)\tLoss 2.2501 (2.2501)\tPrec@1 10.938 (10.938)\tPrec@5 50.781 (50.781)\n",
      "\n",
      " Epoch: 82\tTraining Loss 2.2256 \tTraining Prec@1 15.325 \tTraining Prec@5 59.997 \tValidation Loss 2.2924 \tValidation Prec@1 10.950 \tValidation Prec@5 54.700 \n",
      "\n",
      "TRAINING - Epoch: [82][0/469]\tTime 0.099 (0.099)\tData 0.075 (0.075)\tLoss 2.2388 (2.2388)\tPrec@1 15.625 (15.625)\tPrec@5 62.500 (62.500)\n",
      "TRAINING - Epoch: [82][100/469]\tTime 0.010 (0.019)\tData 0.002 (0.004)\tLoss 2.3328 (2.2276)\tPrec@1 6.250 (15.246)\tPrec@5 59.375 (60.265)\n",
      "TRAINING - Epoch: [82][200/469]\tTime 0.008 (0.015)\tData 0.002 (0.003)\tLoss 2.1840 (2.2241)\tPrec@1 20.312 (15.415)\tPrec@5 62.500 (60.125)\n",
      "TRAINING - Epoch: [82][300/469]\tTime 0.008 (0.013)\tData 0.002 (0.003)\tLoss 2.3184 (2.2244)\tPrec@1 13.281 (15.262)\tPrec@5 64.844 (60.421)\n",
      "TRAINING - Epoch: [82][400/469]\tTime 0.011 (0.012)\tData 0.005 (0.003)\tLoss 2.2406 (2.2254)\tPrec@1 21.875 (15.424)\tPrec@5 55.469 (60.373)\n",
      "EVALUATING - Epoch: [82][0/79]\tTime 0.058 (0.058)\tData 0.052 (0.052)\tLoss 2.3465 (2.3465)\tPrec@1 14.844 (14.844)\tPrec@5 54.688 (54.688)\n",
      "\n",
      " Epoch: 83\tTraining Loss 2.2256 \tTraining Prec@1 15.365 \tTraining Prec@5 60.197 \tValidation Loss 2.3858 \tValidation Prec@1 12.190 \tValidation Prec@5 54.930 \n",
      "\n",
      "TRAINING - Epoch: [83][0/469]\tTime 0.102 (0.102)\tData 0.081 (0.081)\tLoss 2.2611 (2.2611)\tPrec@1 9.375 (9.375)\tPrec@5 59.375 (59.375)\n",
      "TRAINING - Epoch: [83][100/469]\tTime 0.010 (0.014)\tData 0.003 (0.004)\tLoss 2.2125 (2.2237)\tPrec@1 19.531 (15.741)\tPrec@5 59.375 (60.466)\n",
      "TRAINING - Epoch: [83][200/469]\tTime 0.009 (0.013)\tData 0.003 (0.003)\tLoss 2.2149 (2.2294)\tPrec@1 12.500 (15.279)\tPrec@5 56.250 (60.110)\n",
      "TRAINING - Epoch: [83][300/469]\tTime 0.015 (0.012)\tData 0.002 (0.003)\tLoss 2.2795 (2.2317)\tPrec@1 7.812 (15.075)\tPrec@5 58.594 (59.936)\n",
      "TRAINING - Epoch: [83][400/469]\tTime 0.023 (0.012)\tData 0.005 (0.003)\tLoss 2.1892 (2.2289)\tPrec@1 9.375 (15.068)\tPrec@5 62.500 (59.993)\n",
      "EVALUATING - Epoch: [83][0/79]\tTime 0.065 (0.065)\tData 0.058 (0.058)\tLoss 2.2201 (2.2201)\tPrec@1 15.625 (15.625)\tPrec@5 57.031 (57.031)\n",
      "\n",
      " Epoch: 84\tTraining Loss 2.2285 \tTraining Prec@1 15.187 \tTraining Prec@5 60.087 \tValidation Loss 2.2343 \tValidation Prec@1 14.950 \tValidation Prec@5 59.970 \n",
      "\n",
      "TRAINING - Epoch: [84][0/469]\tTime 0.079 (0.079)\tData 0.061 (0.061)\tLoss 2.2272 (2.2272)\tPrec@1 12.500 (12.500)\tPrec@5 60.938 (60.938)\n",
      "TRAINING - Epoch: [84][100/469]\tTime 0.012 (0.015)\tData 0.003 (0.004)\tLoss 2.3492 (2.2356)\tPrec@1 8.594 (15.362)\tPrec@5 52.344 (59.607)\n",
      "TRAINING - Epoch: [84][200/469]\tTime 0.018 (0.015)\tData 0.002 (0.003)\tLoss 2.2771 (2.2335)\tPrec@1 16.406 (15.232)\tPrec@5 60.938 (59.624)\n",
      "TRAINING - Epoch: [84][300/469]\tTime 0.014 (0.014)\tData 0.003 (0.003)\tLoss 2.2211 (2.2307)\tPrec@1 10.938 (14.914)\tPrec@5 55.469 (59.697)\n",
      "TRAINING - Epoch: [84][400/469]\tTime 0.010 (0.014)\tData 0.002 (0.003)\tLoss 2.2705 (2.2311)\tPrec@1 19.531 (14.881)\tPrec@5 55.469 (59.661)\n",
      "EVALUATING - Epoch: [84][0/79]\tTime 0.077 (0.077)\tData 0.069 (0.069)\tLoss 2.3277 (2.3277)\tPrec@1 10.938 (10.938)\tPrec@5 42.969 (42.969)\n",
      "\n",
      " Epoch: 85\tTraining Loss 2.2301 \tTraining Prec@1 14.967 \tTraining Prec@5 59.758 \tValidation Loss 2.3324 \tValidation Prec@1 10.820 \tValidation Prec@5 50.120 \n",
      "\n",
      "TRAINING - Epoch: [85][0/469]\tTime 0.079 (0.079)\tData 0.066 (0.066)\tLoss 2.1239 (2.1239)\tPrec@1 25.781 (25.781)\tPrec@5 66.406 (66.406)\n",
      "TRAINING - Epoch: [85][100/469]\tTime 0.010 (0.013)\tData 0.000 (0.003)\tLoss 2.1718 (2.2196)\tPrec@1 27.344 (16.143)\tPrec@5 60.156 (59.916)\n",
      "TRAINING - Epoch: [85][200/469]\tTime 0.008 (0.012)\tData 0.002 (0.003)\tLoss 2.1998 (2.2251)\tPrec@1 10.938 (15.609)\tPrec@5 63.281 (60.316)\n",
      "TRAINING - Epoch: [85][300/469]\tTime 0.013 (0.012)\tData 0.002 (0.003)\tLoss 2.2230 (2.2241)\tPrec@1 10.938 (15.394)\tPrec@5 65.625 (60.530)\n",
      "TRAINING - Epoch: [85][400/469]\tTime 0.014 (0.012)\tData 0.003 (0.003)\tLoss 2.2884 (2.2265)\tPrec@1 12.500 (15.280)\tPrec@5 55.469 (60.349)\n",
      "EVALUATING - Epoch: [85][0/79]\tTime 0.065 (0.065)\tData 0.057 (0.057)\tLoss 2.3189 (2.3189)\tPrec@1 14.844 (14.844)\tPrec@5 46.875 (46.875)\n",
      "\n",
      " Epoch: 86\tTraining Loss 2.2276 \tTraining Prec@1 15.143 \tTraining Prec@5 60.248 \tValidation Loss 2.3846 \tValidation Prec@1 13.610 \tValidation Prec@5 53.230 \n",
      "\n",
      "TRAINING - Epoch: [86][0/469]\tTime 0.071 (0.071)\tData 0.055 (0.055)\tLoss 2.1844 (2.1844)\tPrec@1 11.719 (11.719)\tPrec@5 57.031 (57.031)\n",
      "TRAINING - Epoch: [86][100/469]\tTime 0.009 (0.013)\tData 0.000 (0.002)\tLoss 2.2353 (2.2263)\tPrec@1 10.938 (14.488)\tPrec@5 59.375 (59.870)\n",
      "TRAINING - Epoch: [86][200/469]\tTime 0.012 (0.012)\tData 0.001 (0.002)\tLoss 2.3174 (2.2277)\tPrec@1 7.812 (14.805)\tPrec@5 58.594 (59.787)\n",
      "TRAINING - Epoch: [86][300/469]\tTime 0.013 (0.012)\tData 0.001 (0.002)\tLoss 2.3206 (2.2264)\tPrec@1 12.500 (15.155)\tPrec@5 49.219 (59.881)\n",
      "TRAINING - Epoch: [86][400/469]\tTime 0.009 (0.012)\tData 0.001 (0.002)\tLoss 2.1633 (2.2282)\tPrec@1 21.875 (15.243)\tPrec@5 63.281 (59.802)\n",
      "EVALUATING - Epoch: [86][0/79]\tTime 0.056 (0.056)\tData 0.048 (0.048)\tLoss 2.9829 (2.9829)\tPrec@1 14.062 (14.062)\tPrec@5 55.469 (55.469)\n",
      "\n",
      " Epoch: 87\tTraining Loss 2.2295 \tTraining Prec@1 15.130 \tTraining Prec@5 59.842 \tValidation Loss 3.2410 \tValidation Prec@1 9.020 \tValidation Prec@5 49.410 \n",
      "\n",
      "TRAINING - Epoch: [87][0/469]\tTime 0.077 (0.077)\tData 0.063 (0.063)\tLoss 2.2048 (2.2048)\tPrec@1 19.531 (19.531)\tPrec@5 57.031 (57.031)\n",
      "TRAINING - Epoch: [87][100/469]\tTime 0.010 (0.011)\tData 0.002 (0.003)\tLoss 2.2670 (2.2271)\tPrec@1 15.625 (15.145)\tPrec@5 54.688 (59.916)\n",
      "TRAINING - Epoch: [87][200/469]\tTime 0.008 (0.010)\tData 0.001 (0.002)\tLoss 2.2278 (2.2278)\tPrec@1 14.844 (15.302)\tPrec@5 57.812 (59.760)\n",
      "TRAINING - Epoch: [87][300/469]\tTime 0.007 (0.010)\tData 0.001 (0.002)\tLoss 2.1270 (2.2278)\tPrec@1 21.875 (15.301)\tPrec@5 64.844 (59.801)\n",
      "TRAINING - Epoch: [87][400/469]\tTime 0.011 (0.010)\tData 0.003 (0.002)\tLoss 2.2270 (2.2268)\tPrec@1 19.531 (15.424)\tPrec@5 56.250 (59.769)\n",
      "EVALUATING - Epoch: [87][0/79]\tTime 0.053 (0.053)\tData 0.047 (0.047)\tLoss 2.2282 (2.2282)\tPrec@1 9.375 (9.375)\tPrec@5 54.688 (54.688)\n",
      "\n",
      " Epoch: 88\tTraining Loss 2.2276 \tTraining Prec@1 15.418 \tTraining Prec@5 59.817 \tValidation Loss 2.2282 \tValidation Prec@1 10.120 \tValidation Prec@5 60.540 \n",
      "\n",
      "TRAINING - Epoch: [88][0/469]\tTime 0.084 (0.084)\tData 0.069 (0.069)\tLoss 2.1801 (2.1801)\tPrec@1 14.062 (14.062)\tPrec@5 67.188 (67.188)\n",
      "TRAINING - Epoch: [88][100/469]\tTime 0.010 (0.013)\tData 0.002 (0.003)\tLoss 2.1804 (2.2290)\tPrec@1 22.656 (14.952)\tPrec@5 64.844 (59.669)\n",
      "TRAINING - Epoch: [88][200/469]\tTime 0.011 (0.013)\tData 0.002 (0.003)\tLoss 2.1903 (2.2341)\tPrec@1 12.500 (14.914)\tPrec@5 53.125 (59.406)\n",
      "TRAINING - Epoch: [88][300/469]\tTime 0.014 (0.013)\tData 0.003 (0.002)\tLoss 2.2695 (2.2311)\tPrec@1 8.594 (15.018)\tPrec@5 57.031 (59.699)\n",
      "TRAINING - Epoch: [88][400/469]\tTime 0.011 (0.013)\tData 0.001 (0.002)\tLoss 2.2055 (2.2299)\tPrec@1 17.188 (15.091)\tPrec@5 60.938 (59.677)\n",
      "EVALUATING - Epoch: [88][0/79]\tTime 0.053 (0.053)\tData 0.047 (0.047)\tLoss 2.2420 (2.2420)\tPrec@1 9.375 (9.375)\tPrec@5 51.562 (51.562)\n",
      "\n",
      " Epoch: 89\tTraining Loss 2.2288 \tTraining Prec@1 15.200 \tTraining Prec@5 59.748 \tValidation Loss 2.1957 \tValidation Prec@1 10.540 \tValidation Prec@5 60.030 \n",
      "\n",
      "TRAINING - Epoch: [89][0/469]\tTime 0.091 (0.091)\tData 0.074 (0.074)\tLoss 2.2336 (2.2336)\tPrec@1 13.281 (13.281)\tPrec@5 58.594 (58.594)\n",
      "TRAINING - Epoch: [89][100/469]\tTime 0.012 (0.013)\tData 0.002 (0.003)\tLoss 2.2027 (2.2255)\tPrec@1 11.719 (15.223)\tPrec@5 69.531 (59.715)\n",
      "TRAINING - Epoch: [89][200/469]\tTime 0.014 (0.013)\tData 0.002 (0.002)\tLoss 2.2085 (2.2308)\tPrec@1 16.406 (14.852)\tPrec@5 59.375 (59.461)\n",
      "TRAINING - Epoch: [89][300/469]\tTime 0.011 (0.013)\tData 0.001 (0.002)\tLoss 2.1852 (2.2312)\tPrec@1 12.500 (14.945)\tPrec@5 57.031 (59.401)\n",
      "TRAINING - Epoch: [89][400/469]\tTime 0.014 (0.012)\tData 0.002 (0.002)\tLoss 2.2492 (2.2290)\tPrec@1 13.281 (15.235)\tPrec@5 53.906 (59.661)\n",
      "EVALUATING - Epoch: [89][0/79]\tTime 0.059 (0.059)\tData 0.049 (0.049)\tLoss 2.2577 (2.2577)\tPrec@1 9.375 (9.375)\tPrec@5 53.125 (53.125)\n",
      "\n",
      " Epoch: 90\tTraining Loss 2.2327 \tTraining Prec@1 15.035 \tTraining Prec@5 59.588 \tValidation Loss 2.2204 \tValidation Prec@1 10.540 \tValidation Prec@5 61.060 \n",
      "\n",
      "TRAINING - Epoch: [90][0/469]\tTime 0.087 (0.087)\tData 0.072 (0.072)\tLoss 2.2584 (2.2584)\tPrec@1 12.500 (12.500)\tPrec@5 63.281 (63.281)\n",
      "TRAINING - Epoch: [90][100/469]\tTime 0.024 (0.017)\tData 0.004 (0.004)\tLoss 2.2299 (2.2377)\tPrec@1 11.719 (14.287)\tPrec@5 60.156 (59.986)\n",
      "TRAINING - Epoch: [90][200/469]\tTime 0.019 (0.016)\tData 0.004 (0.003)\tLoss 2.1579 (2.2303)\tPrec@1 11.719 (15.085)\tPrec@5 67.188 (60.308)\n",
      "TRAINING - Epoch: [90][300/469]\tTime 0.021 (0.015)\tData 0.003 (0.003)\tLoss 2.2741 (2.2329)\tPrec@1 13.281 (14.865)\tPrec@5 67.188 (59.933)\n",
      "TRAINING - Epoch: [90][400/469]\tTime 0.015 (0.015)\tData 0.003 (0.003)\tLoss 2.1338 (2.2314)\tPrec@1 15.625 (14.988)\tPrec@5 66.406 (60.221)\n",
      "EVALUATING - Epoch: [90][0/79]\tTime 0.066 (0.066)\tData 0.056 (0.056)\tLoss 2.3547 (2.3547)\tPrec@1 9.375 (9.375)\tPrec@5 42.969 (42.969)\n",
      "\n",
      " Epoch: 91\tTraining Loss 2.2294 \tTraining Prec@1 15.002 \tTraining Prec@5 60.040 \tValidation Loss 2.3274 \tValidation Prec@1 10.120 \tValidation Prec@5 50.030 \n",
      "\n",
      "TRAINING - Epoch: [91][0/469]\tTime 0.083 (0.083)\tData 0.068 (0.068)\tLoss 2.2479 (2.2479)\tPrec@1 8.594 (8.594)\tPrec@5 60.938 (60.938)\n",
      "TRAINING - Epoch: [91][100/469]\tTime 0.012 (0.016)\tData 0.004 (0.006)\tLoss 2.2228 (2.2278)\tPrec@1 16.406 (15.215)\tPrec@5 60.156 (59.754)\n",
      "TRAINING - Epoch: [91][200/469]\tTime 0.017 (0.014)\tData 0.011 (0.006)\tLoss 2.2001 (2.2313)\tPrec@1 10.938 (14.898)\tPrec@5 58.594 (59.538)\n",
      "TRAINING - Epoch: [91][300/469]\tTime 0.020 (0.014)\tData 0.013 (0.005)\tLoss 2.2713 (2.2295)\tPrec@1 8.594 (14.961)\tPrec@5 53.906 (59.884)\n",
      "TRAINING - Epoch: [91][400/469]\tTime 0.019 (0.014)\tData 0.007 (0.005)\tLoss 2.1961 (2.2277)\tPrec@1 12.500 (14.966)\tPrec@5 67.188 (60.010)\n",
      "EVALUATING - Epoch: [91][0/79]\tTime 0.059 (0.059)\tData 0.053 (0.053)\tLoss 4.0765 (4.0765)\tPrec@1 10.156 (10.156)\tPrec@5 53.906 (53.906)\n",
      "\n",
      " Epoch: 92\tTraining Loss 2.2272 \tTraining Prec@1 15.047 \tTraining Prec@5 60.128 \tValidation Loss 4.3064 \tValidation Prec@1 7.380 \tValidation Prec@5 50.280 \n",
      "\n",
      "TRAINING - Epoch: [92][0/469]\tTime 0.089 (0.089)\tData 0.069 (0.069)\tLoss 2.2447 (2.2447)\tPrec@1 20.312 (20.312)\tPrec@5 52.344 (52.344)\n",
      "TRAINING - Epoch: [92][100/469]\tTime 0.007 (0.014)\tData 0.000 (0.004)\tLoss 2.2796 (2.2329)\tPrec@1 15.625 (14.944)\tPrec@5 60.156 (60.651)\n",
      "TRAINING - Epoch: [92][200/469]\tTime 0.015 (0.014)\tData 0.002 (0.003)\tLoss 2.1817 (2.2340)\tPrec@1 21.094 (14.642)\tPrec@5 62.500 (60.424)\n",
      "TRAINING - Epoch: [92][300/469]\tTime 0.012 (0.014)\tData 0.002 (0.003)\tLoss 2.3145 (2.2314)\tPrec@1 7.031 (14.836)\tPrec@5 48.438 (60.273)\n",
      "TRAINING - Epoch: [92][400/469]\tTime 0.015 (0.014)\tData 0.005 (0.003)\tLoss 2.2174 (2.2324)\tPrec@1 17.969 (14.896)\tPrec@5 54.688 (60.082)\n",
      "EVALUATING - Epoch: [92][0/79]\tTime 0.067 (0.067)\tData 0.058 (0.058)\tLoss 2.2376 (2.2376)\tPrec@1 14.062 (14.062)\tPrec@5 55.469 (55.469)\n",
      "\n",
      " Epoch: 93\tTraining Loss 2.2306 \tTraining Prec@1 15.087 \tTraining Prec@5 60.105 \tValidation Loss 2.2377 \tValidation Prec@1 13.950 \tValidation Prec@5 60.220 \n",
      "\n",
      "TRAINING - Epoch: [93][0/469]\tTime 0.096 (0.096)\tData 0.078 (0.078)\tLoss 2.1750 (2.1750)\tPrec@1 17.188 (17.188)\tPrec@5 58.594 (58.594)\n",
      "TRAINING - Epoch: [93][100/469]\tTime 0.013 (0.016)\tData 0.005 (0.006)\tLoss 2.2035 (2.2149)\tPrec@1 12.500 (15.408)\tPrec@5 64.844 (59.862)\n",
      "TRAINING - Epoch: [93][200/469]\tTime 0.023 (0.016)\tData 0.007 (0.006)\tLoss 2.2356 (2.2235)\tPrec@1 13.281 (15.081)\tPrec@5 59.375 (60.335)\n",
      "TRAINING - Epoch: [93][300/469]\tTime 0.017 (0.017)\tData 0.010 (0.006)\tLoss 2.2305 (2.2227)\tPrec@1 13.281 (15.145)\tPrec@5 58.594 (60.247)\n",
      "TRAINING - Epoch: [93][400/469]\tTime 0.015 (0.015)\tData 0.009 (0.005)\tLoss 2.2087 (2.2256)\tPrec@1 14.062 (15.105)\tPrec@5 61.719 (60.170)\n",
      "EVALUATING - Epoch: [93][0/79]\tTime 0.062 (0.062)\tData 0.055 (0.055)\tLoss 2.2430 (2.2430)\tPrec@1 10.938 (10.938)\tPrec@5 53.125 (53.125)\n",
      "\n",
      " Epoch: 94\tTraining Loss 2.2262 \tTraining Prec@1 15.087 \tTraining Prec@5 60.220 \tValidation Loss 2.2164 \tValidation Prec@1 10.810 \tValidation Prec@5 60.340 \n",
      "\n",
      "TRAINING - Epoch: [94][0/469]\tTime 0.091 (0.091)\tData 0.071 (0.071)\tLoss 2.1819 (2.1819)\tPrec@1 15.625 (15.625)\tPrec@5 66.406 (66.406)\n",
      "TRAINING - Epoch: [94][100/469]\tTime 0.013 (0.013)\tData 0.001 (0.003)\tLoss 2.2526 (2.2222)\tPrec@1 14.844 (15.285)\tPrec@5 57.031 (60.466)\n",
      "TRAINING - Epoch: [94][200/469]\tTime 0.012 (0.013)\tData 0.002 (0.003)\tLoss 2.2333 (2.2248)\tPrec@1 18.750 (15.333)\tPrec@5 58.594 (59.931)\n",
      "TRAINING - Epoch: [94][300/469]\tTime 0.013 (0.013)\tData 0.001 (0.002)\tLoss 2.2348 (2.2278)\tPrec@1 10.938 (15.212)\tPrec@5 62.500 (59.967)\n",
      "TRAINING - Epoch: [94][400/469]\tTime 0.013 (0.013)\tData 0.003 (0.002)\tLoss 2.1550 (2.2295)\tPrec@1 25.781 (15.270)\tPrec@5 64.062 (59.963)\n",
      "EVALUATING - Epoch: [94][0/79]\tTime 0.067 (0.067)\tData 0.057 (0.057)\tLoss 2.1817 (2.1817)\tPrec@1 20.312 (20.312)\tPrec@5 53.125 (53.125)\n",
      "\n",
      " Epoch: 95\tTraining Loss 2.2305 \tTraining Prec@1 15.030 \tTraining Prec@5 59.857 \tValidation Loss 2.1694 \tValidation Prec@1 20.040 \tValidation Prec@5 59.480 \n",
      "\n",
      "TRAINING - Epoch: [95][0/469]\tTime 0.083 (0.083)\tData 0.070 (0.070)\tLoss 2.1492 (2.1492)\tPrec@1 27.344 (27.344)\tPrec@5 61.719 (61.719)\n",
      "TRAINING - Epoch: [95][100/469]\tTime 0.008 (0.013)\tData 0.000 (0.005)\tLoss 2.3018 (2.2255)\tPrec@1 9.375 (15.540)\tPrec@5 57.812 (59.437)\n",
      "TRAINING - Epoch: [95][200/469]\tTime 0.008 (0.013)\tData 0.000 (0.005)\tLoss 2.1705 (2.2307)\tPrec@1 25.000 (15.186)\tPrec@5 61.719 (59.200)\n",
      "TRAINING - Epoch: [95][300/469]\tTime 0.007 (0.013)\tData 0.000 (0.005)\tLoss 2.2028 (2.2288)\tPrec@1 12.500 (15.173)\tPrec@5 66.406 (59.629)\n",
      "TRAINING - Epoch: [95][400/469]\tTime 0.006 (0.013)\tData 0.000 (0.005)\tLoss 2.2015 (2.2314)\tPrec@1 13.281 (15.011)\tPrec@5 54.688 (59.443)\n",
      "EVALUATING - Epoch: [95][0/79]\tTime 0.057 (0.057)\tData 0.051 (0.051)\tLoss 2.2656 (2.2656)\tPrec@1 10.156 (10.156)\tPrec@5 50.781 (50.781)\n",
      "\n",
      " Epoch: 96\tTraining Loss 2.2308 \tTraining Prec@1 14.983 \tTraining Prec@5 59.548 \tValidation Loss 2.2276 \tValidation Prec@1 10.420 \tValidation Prec@5 59.310 \n",
      "\n",
      "TRAINING - Epoch: [96][0/469]\tTime 0.084 (0.084)\tData 0.072 (0.072)\tLoss 2.2308 (2.2308)\tPrec@1 8.594 (8.594)\tPrec@5 57.812 (57.812)\n",
      "TRAINING - Epoch: [96][100/469]\tTime 0.023 (0.015)\tData 0.016 (0.006)\tLoss 2.2385 (2.2281)\tPrec@1 10.156 (15.640)\tPrec@5 53.125 (59.367)\n",
      "TRAINING - Epoch: [96][200/469]\tTime 0.022 (0.014)\tData 0.008 (0.005)\tLoss 2.2195 (2.2230)\tPrec@1 9.375 (15.889)\tPrec@5 64.844 (59.958)\n",
      "TRAINING - Epoch: [96][300/469]\tTime 0.010 (0.013)\tData 0.003 (0.005)\tLoss 2.2008 (2.2240)\tPrec@1 14.062 (15.609)\tPrec@5 65.625 (60.182)\n",
      "TRAINING - Epoch: [96][400/469]\tTime 0.012 (0.013)\tData 0.005 (0.005)\tLoss 2.1373 (2.2279)\tPrec@1 13.281 (15.452)\tPrec@5 64.062 (59.876)\n",
      "EVALUATING - Epoch: [96][0/79]\tTime 0.055 (0.055)\tData 0.049 (0.049)\tLoss 2.6186 (2.6186)\tPrec@1 9.375 (9.375)\tPrec@5 50.781 (50.781)\n",
      "\n",
      " Epoch: 97\tTraining Loss 2.2272 \tTraining Prec@1 15.388 \tTraining Prec@5 59.935 \tValidation Loss 2.6565 \tValidation Prec@1 9.540 \tValidation Prec@5 51.410 \n",
      "\n",
      "TRAINING - Epoch: [97][0/469]\tTime 0.073 (0.073)\tData 0.060 (0.060)\tLoss 2.3920 (2.3920)\tPrec@1 10.938 (10.938)\tPrec@5 56.250 (56.250)\n",
      "TRAINING - Epoch: [97][100/469]\tTime 0.007 (0.013)\tData 0.000 (0.005)\tLoss 2.1537 (2.2291)\tPrec@1 21.875 (14.836)\tPrec@5 57.031 (59.290)\n",
      "TRAINING - Epoch: [97][200/469]\tTime 0.008 (0.013)\tData 0.000 (0.005)\tLoss 2.2537 (2.2310)\tPrec@1 9.375 (14.785)\tPrec@5 58.594 (59.942)\n",
      "TRAINING - Epoch: [97][300/469]\tTime 0.007 (0.012)\tData 0.000 (0.005)\tLoss 2.2685 (2.2286)\tPrec@1 10.938 (15.015)\tPrec@5 45.312 (59.964)\n",
      "TRAINING - Epoch: [97][400/469]\tTime 0.008 (0.012)\tData 0.000 (0.004)\tLoss 2.2193 (2.2299)\tPrec@1 21.094 (14.990)\tPrec@5 52.344 (59.899)\n",
      "EVALUATING - Epoch: [97][0/79]\tTime 0.062 (0.062)\tData 0.053 (0.053)\tLoss 2.2837 (2.2837)\tPrec@1 9.375 (9.375)\tPrec@5 48.438 (48.438)\n",
      "\n",
      " Epoch: 98\tTraining Loss 2.2293 \tTraining Prec@1 14.950 \tTraining Prec@5 59.767 \tValidation Loss 2.2573 \tValidation Prec@1 10.100 \tValidation Prec@5 55.540 \n",
      "\n",
      "TRAINING - Epoch: [98][0/469]\tTime 0.102 (0.102)\tData 0.086 (0.086)\tLoss 2.1983 (2.1983)\tPrec@1 12.500 (12.500)\tPrec@5 64.062 (64.062)\n",
      "TRAINING - Epoch: [98][100/469]\tTime 0.009 (0.012)\tData 0.002 (0.003)\tLoss 2.3159 (2.2288)\tPrec@1 9.375 (15.316)\tPrec@5 61.719 (59.460)\n",
      "TRAINING - Epoch: [98][200/469]\tTime 0.009 (0.012)\tData 0.001 (0.003)\tLoss 2.1761 (2.2259)\tPrec@1 21.875 (15.714)\tPrec@5 59.375 (59.328)\n",
      "TRAINING - Epoch: [98][300/469]\tTime 0.012 (0.011)\tData 0.002 (0.002)\tLoss 2.2128 (2.2268)\tPrec@1 12.500 (15.319)\tPrec@5 67.969 (59.814)\n",
      "TRAINING - Epoch: [98][400/469]\tTime 0.014 (0.012)\tData 0.002 (0.002)\tLoss 2.2406 (2.2274)\tPrec@1 17.969 (15.276)\tPrec@5 55.469 (59.950)\n",
      "EVALUATING - Epoch: [98][0/79]\tTime 0.066 (0.066)\tData 0.054 (0.054)\tLoss 2.3464 (2.3464)\tPrec@1 9.375 (9.375)\tPrec@5 44.531 (44.531)\n",
      "\n",
      " Epoch: 99\tTraining Loss 2.2274 \tTraining Prec@1 15.237 \tTraining Prec@5 60.003 \tValidation Loss 2.3707 \tValidation Prec@1 10.410 \tValidation Prec@5 48.580 \n",
      "\n",
      "TRAINING - Epoch: [99][0/469]\tTime 0.084 (0.084)\tData 0.071 (0.071)\tLoss 2.1562 (2.1562)\tPrec@1 10.938 (10.938)\tPrec@5 65.625 (65.625)\n",
      "TRAINING - Epoch: [99][100/469]\tTime 0.008 (0.013)\tData 0.002 (0.003)\tLoss 2.2001 (2.2258)\tPrec@1 12.500 (14.944)\tPrec@5 57.812 (60.295)\n",
      "TRAINING - Epoch: [99][200/469]\tTime 0.013 (0.014)\tData 0.002 (0.003)\tLoss 2.2189 (2.2216)\tPrec@1 15.625 (15.162)\tPrec@5 59.375 (60.479)\n",
      "TRAINING - Epoch: [99][300/469]\tTime 0.013 (0.014)\tData 0.004 (0.003)\tLoss 2.2025 (2.2233)\tPrec@1 12.500 (14.903)\tPrec@5 62.500 (60.058)\n",
      "TRAINING - Epoch: [99][400/469]\tTime 0.016 (0.014)\tData 0.002 (0.003)\tLoss 2.2707 (2.2255)\tPrec@1 17.188 (14.988)\tPrec@5 60.938 (60.014)\n",
      "EVALUATING - Epoch: [99][0/79]\tTime 0.057 (0.057)\tData 0.051 (0.051)\tLoss 2.6227 (2.6227)\tPrec@1 6.250 (6.250)\tPrec@5 63.281 (63.281)\n",
      "\n",
      " Epoch: 100\tTraining Loss 2.2260 \tTraining Prec@1 14.997 \tTraining Prec@5 60.152 \tValidation Loss 2.8005 \tValidation Prec@1 8.090 \tValidation Prec@5 53.720 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "global best_prec1\n",
    "best_prec1 = 0\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.float().topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.contiguous().view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].contiguous().view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "    \n",
    "def save_checkpoint(state, is_best, path='.', filename='checkpoint.pth.tar', save_all=False):\n",
    "    filename = os.path.join(path, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, os.path.join(path, 'model_best.pth.tar'))\n",
    "    if save_all:\n",
    "        shutil.copyfile(filename, os.path.join(\n",
    "            path, 'checkpoint_epoch_%s.pth.tar' % state['epoch']))\n",
    "        \n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def forward(data_loader, model, criterion, epoch=0, training=True, optimizer=None):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (inputs, target) in enumerate(data_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        target = target.cuda()\n",
    "\n",
    "        if not training:\n",
    "            with torch.no_grad():\n",
    "                input_var = Variable(inputs.type(torch.cuda.FloatTensor))\n",
    "                target_var = Variable(target)\n",
    "                # compute output\n",
    "                output = model(input_var)\n",
    "        else:\n",
    "                input_var = Variable(inputs.type(torch.cuda.FloatTensor))\n",
    "                target_var = Variable(target)\n",
    "                # compute output\n",
    "                output = model(input_var)\n",
    "\n",
    "\n",
    "        loss = criterion(output, target_var)\n",
    "        if type(output) is list:\n",
    "            output = output[0]\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "        if training:\n",
    "            # compute gradient and do SGD step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for p in list(model.parameters()):\n",
    "                if hasattr(p,'org'):\n",
    "                    p.data.copy_(p.org)\n",
    "            optimizer.step()\n",
    "            for p in list(model.parameters()):\n",
    "                if hasattr(p,'org'):\n",
    "                    p.org.copy_(p.data.clamp_(-1,1))\n",
    "\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('{phase} - Epoch: [{0}][{1}/{2}]\\t'\n",
    "                         'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                         'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                         'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                         'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                         'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                             epoch, i, len(data_loader),\n",
    "                             phase='TRAINING' if training else 'EVALUATING',\n",
    "                             batch_time=batch_time,\n",
    "                             data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "\n",
    "    return losses.avg, top1.avg, top5.avg\n",
    "    \n",
    "def train(data_loader, model, criterion, epoch, optimizer):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    return forward(data_loader, model, criterion, epoch, training=True, optimizer=optimizer)\n",
    "\n",
    "\n",
    "def validate(data_loader, model, criterion, epoch):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    return forward(data_loader, model, criterion, epoch, training=False, optimizer=None)\n",
    "    \n",
    "for epoch in range(1, 100):\n",
    "\n",
    "        # train for one epoch\n",
    "        train_loss, train_prec1, train_prec5 = train(train_loader, model, criterion, epoch, optimizer)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        val_loss, val_prec1, val_prec5 = validate(test_loader, model, criterion, epoch)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = val_prec1 > best_prec1\n",
    "        best_prec1 = max(val_prec1, best_prec1)\n",
    "\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'model': 'BCNN',\n",
    "            'config': 'None',\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "        }, is_best, path='weight/')\n",
    "        print('\\n Epoch: {0}\\t'\n",
    "                     'Training Loss {train_loss:.4f} \\t'\n",
    "                     'Training Prec@1 {train_prec1:.3f} \\t'\n",
    "                     'Training Prec@5 {train_prec5:.3f} \\t'\n",
    "                     'Validation Loss {val_loss:.4f} \\t'\n",
    "                     'Validation Prec@1 {val_prec1:.3f} \\t'\n",
    "                     'Validation Prec@5 {val_prec5:.3f} \\n'\n",
    "                     .format(epoch + 1, train_loss=train_loss, val_loss=val_loss,\n",
    "                             train_prec1=train_prec1, val_prec1=val_prec1,\n",
    "                             train_prec5=train_prec5, val_prec5=val_prec5))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df51a72b-c52c-4bcf-8449-cd0017a739cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mnist_og = test_dataset[0]\n",
    "plt.imshow(mnist_og, cmap=cm.Greys_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0d062263-b449-46fb-941c-1f80abc9f526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.8005, Accuracy: 809/10000 (8.09%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "075178b4-684e-4162-a8b3-e67b8edfb958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-1., -1., -1.],\n",
      "          [-1.,  1.,  1.],\n",
      "          [ 1.,  1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1.,  1.],\n",
      "          [-1.,  1.,  1.],\n",
      "          [ 1.,  1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.],\n",
      "          [-1.,  1.,  1.],\n",
      "          [ 1.,  1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1., -1.]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3150,  0.8517, -0.2110,  0.0531], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([38.4891, 43.4622, 41.7565, 39.5340], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-5.9068, 16.8850, -3.2084, -8.7954], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-1.,  1.,  1.],\n",
      "          [ 1., -1., -1.],\n",
      "          [ 1., -1., -1.]],\n",
      "\n",
      "         [[-1., -1.,  1.],\n",
      "          [-1.,  1., -1.],\n",
      "          [ 1., -1., -1.]],\n",
      "\n",
      "         [[-1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1., -1., -1.]],\n",
      "\n",
      "         [[-1., -1.,  1.],\n",
      "          [ 1.,  1., -1.],\n",
      "          [-1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.],\n",
      "          [-1., -1., -1.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [-1., -1., -1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1., -1., -1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [-1., -1., -1.],\n",
      "          [-1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [-1., -1., -1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [-1., -1., -1.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [-1., -1., -1.],\n",
      "          [-1., -1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1., -1., -1.]],\n",
      "\n",
      "         [[-1., -1.,  1.],\n",
      "          [-1.,  1.,  1.],\n",
      "          [ 1.,  1., -1.]],\n",
      "\n",
      "         [[-1., -1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1., -1., -1.]],\n",
      "\n",
      "         [[-1., -1.,  1.],\n",
      "          [-1.,  1., -1.],\n",
      "          [ 1., -1., -1.]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3585, -0.7131, -0.8313, -0.6897], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([40.9033, 40.3064, 44.2862, 38.0511], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-2.7301,  0.8446,  1.9293, -3.0497], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 1.,  1.,  1.],\n",
      "          [-1., -1.,  1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [-1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[-1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1., -1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1.,  1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[ 1.,  1., -1.],\n",
      "          [-1.,  1.,  1.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[-1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1.,  1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1., -1.,  1.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1.,  1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.],\n",
      "          [ 1., -1.,  1.],\n",
      "          [ 1., -1., -1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[ 1., -1., -1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1., -1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1., -1., -1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1., -1., -1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1., -1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1., -1.,  1.]],\n",
      "\n",
      "         [[ 1., -1., -1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1., -1., -1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2603, -1.0000, -0.9896, -0.9968, -0.9904, -0.9802, -0.8152, -0.9988],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([34.3094, 39.5586, 34.9179, 35.3032, 38.4952, 32.0920, 35.6568, 36.7261],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 7.3917, -1.9792, -4.3483,  5.0716, -4.2739, -0.1071, -0.1098, -2.2732],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-1., -1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1., -1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[-1.,  1., -1.],\n",
      "          [-1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1., -1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [ 1.,  1., -1.],\n",
      "          [ 1.,  1., -1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.],\n",
      "          [ 1., -1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1., -1., -1.],\n",
      "          [ 1.,  1., -1.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-1.,  1.,  1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1., -1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-1., -1.,  1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[ 1., -1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1., -1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1., -1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [-1., -1.,  1.],\n",
      "          [ 1., -1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1.,  1., -1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1., -1.,  1.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1., -1., -1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [-1., -1.,  1.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[ 1., -1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [-1.,  1., -1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [-1., -1.,  1.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [-1.,  1., -1.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [-1., -1., -1.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [-1., -1.,  1.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[-1.,  1.,  1.],\n",
      "          [-1., -1., -1.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[-1.,  1.,  1.],\n",
      "          [ 1., -1., -1.],\n",
      "          [-1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-1.,  1., -1.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-1., -1.,  1.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-1., -1.,  1.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1.,  1.],\n",
      "          [-1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[-1.,  1.,  1.],\n",
      "          [ 1., -1.,  1.],\n",
      "          [ 1.,  1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.,  1.,  1.],\n",
      "          [-1., -1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [ 1., -1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [-1.,  1., -1.]],\n",
      "\n",
      "         [[ 1., -1.,  1.],\n",
      "          [-1., -1., -1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[-1., -1.,  1.],\n",
      "          [-1., -1., -1.],\n",
      "          [ 1., -1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [ 1., -1., -1.],\n",
      "          [ 1.,  1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1.,  1.],\n",
      "          [-1.,  1.,  1.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [ 1., -1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1., -1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1., -1., -1.],\n",
      "          [ 1.,  1., -1.],\n",
      "          [ 1.,  1., -1.]],\n",
      "\n",
      "         [[ 1.,  1., -1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1., -1.,  1.]],\n",
      "\n",
      "         [[ 1., -1., -1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1., -1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[-1.,  1., -1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1., -1., -1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1., -1.]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-1.0000, -0.0111, -0.9833, -0.6950, -0.0313, -0.1777, -0.9612, -0.9858],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([35.2271,  0.7479, 40.0963, 33.8319,  0.4756,  0.7167, 37.3537, 30.5234],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0576, -2.0747,  7.3489, -3.2535, -2.2181, -2.2608,  5.5654, -4.8604],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-1.,  1., -1.],\n",
      "          [ 1.,  1., -1.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[ 1., -1.,  1.],\n",
      "          [-1., -1., -1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-1.,  1., -1.],\n",
      "          [-1.,  1., -1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1., -1.,  1.],\n",
      "          [-1., -1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [ 1.,  1., -1.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[-1.,  1., -1.],\n",
      "          [ 1.,  1., -1.],\n",
      "          [-1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1., -1.],\n",
      "          [ 1., -1.,  1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[-1.,  1.,  1.],\n",
      "          [-1.,  1., -1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[ 1., -1.,  1.],\n",
      "          [-1.,  1., -1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.,  1., -1.],\n",
      "          [-1.,  1.,  1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[-1., -1.,  1.],\n",
      "          [ 1., -1., -1.],\n",
      "          [ 1., -1.,  1.]],\n",
      "\n",
      "         [[ 1., -1.,  1.],\n",
      "          [-1., -1.,  1.],\n",
      "          [ 1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1., -1.,  1.],\n",
      "          [-1.,  1.,  1.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1., -1.],\n",
      "          [-1., -1.,  1.],\n",
      "          [ 1.,  1., -1.]],\n",
      "\n",
      "         [[-1.,  1.,  1.],\n",
      "          [ 1., -1.,  1.],\n",
      "          [ 1., -1.,  1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.,  1.,  1.],\n",
      "          [ 1.,  1., -1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [-1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1., -1.],\n",
      "          [ 1., -1.,  1.],\n",
      "          [ 1.,  1.,  1.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1., -1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[-1., -1.,  1.],\n",
      "          [-1.,  1.,  1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[ 1., -1.,  1.],\n",
      "          [-1., -1.,  1.],\n",
      "          [ 1.,  1., -1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1., -1.,  1.],\n",
      "          [ 1., -1., -1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1., -1., -1.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [ 1., -1.,  1.],\n",
      "          [ 1.,  1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1., -1.,  1.],\n",
      "          [-1., -1.,  1.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[-1.,  1., -1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1., -1., -1.]],\n",
      "\n",
      "         [[-1.,  1.,  1.],\n",
      "          [-1.,  1.,  1.],\n",
      "          [ 1., -1.,  1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.,  1., -1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1., -1.],\n",
      "          [-1., -1.,  1.],\n",
      "          [ 1., -1., -1.]],\n",
      "\n",
      "         [[-1., -1.,  1.],\n",
      "          [ 1.,  1., -1.],\n",
      "          [ 1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1., -1.,  1.]],\n",
      "\n",
      "         [[ 1.,  1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [ 1., -1.,  1.]],\n",
      "\n",
      "         [[-1., -1.,  1.],\n",
      "          [ 1.,  1., -1.],\n",
      "          [ 1., -1.,  1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.,  1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [ 1., -1.,  1.]],\n",
      "\n",
      "         [[-1.,  1., -1.],\n",
      "          [-1.,  1., -1.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1., -1.,  1.],\n",
      "          [ 1., -1., -1.],\n",
      "          [-1., -1.,  1.]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1663, -0.0503,  0.0841,  0.1097, -0.0072, -0.0596, -0.1023,  0.0367,\n",
      "        -0.0435,  0.0056, -0.0965,  0.0584,  0.1128, -0.0398, -0.0679, -0.0483],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([36.9729,  0.1207,  0.1446,  0.1181,  0.1108,  0.1241,  0.1509,  0.1664,\n",
      "         0.1138,  0.1282,  0.1242,  0.1525,  0.1278,  0.1356,  0.1157,  0.1249],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-13.7168,  -0.6345,  -0.7579,  -0.6996,  -0.6387,  -0.6302,  -0.6970,\n",
      "         -0.7680,  -0.6783,  -0.6279,  -0.6409,  -0.6180,  -0.6792,  -0.6255,\n",
      "         -0.6511,  -0.6832], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1., -1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1., -1., -1., -1.,  1.,\n",
      "          1., -1.],\n",
      "        [ 1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "         -1., -1.],\n",
      "        [-1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
      "         -1.,  1.],\n",
      "        [-1.,  1., -1., -1.,  1., -1., -1.,  1.,  1., -1., -1., -1., -1.,  1.,\n",
      "         -1.,  1.],\n",
      "        [-1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1.,  1.,\n",
      "          1., -1.],\n",
      "        [-1.,  1.,  1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1., -1.,\n",
      "          1., -1.],\n",
      "        [-1., -1., -1., -1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
      "         -1.,  1.],\n",
      "        [ 1., -1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,  1.,\n",
      "         -1.,  1.],\n",
      "        [-1., -1.,  1., -1., -1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1.,\n",
      "         -1.,  1.],\n",
      "        [-1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1.,  1.,\n",
      "          1.,  1.]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0874, -0.6092,  0.0867,  0.1206,  0.0202, -0.0100, -0.0028, -0.2024,\n",
      "         0.0589, -0.2281], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5f9dc-68a0-4d51-a290-3a9c8fdd001b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
